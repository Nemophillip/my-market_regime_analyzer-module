# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üî•üî•üî• MARKET REGIME ANALYZER 10.0 - PART 1/5 üî•üî•üî•
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Part 1: Imports, Base Classes, OnChainDataManager, MacroDataManager
#
# v10.0 PRODUCTION LEVEL: Î™®Îì† Í∏∞Îä• ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® Í≥†ÎèÑÌôî
# - v9.0Ïùò Î™®Îì† Í∏∞Îä• 100% Ïú†ÏßÄ + ÎåÄÌè≠ Í∞ïÌôî
# - Ïã§ÏãúÍ∞Ñ Ï†ÅÏùëÌòï Í∞ÄÏ§ëÏπò ÏãúÏä§ÌÖú
# - Regime Ï†ÑÌôò ÏïàÏ†ïÏÑ± Í∞ïÌôî
# - Ïò®Ï≤¥Ïù∏/Îß§ÌÅ¨Î°ú Îç∞Ïù¥ÌÑ∞ ÏúµÌï© Í≥†ÎèÑÌôî
# - ÌÜµÍ≥ÑÏ†Å Ïã†Î¢∞ÎèÑ Ïä§ÏΩîÏñ¥ÎßÅ
# - ÌîÑÎ°úÎçïÏÖò Ïù∏ÌîÑÎùº (Î™®ÎãàÌÑ∞ÎßÅ, ÏïåÎ¶º, Î∞±ÌÖåÏä§ÌåÖ)
#
# Î≥ëÌï© Î∞©Î≤ï:
# 1. Î™®Îì† ÌååÌä∏(1~5)Î•º Îã§Ïö¥Î°úÎìú
# 2. Part 1Ïùò ÎÇ¥Ïö©ÏùÑ market_regime_analyzer10.pyÎ°ú Î≥µÏÇ¨
# 3. Part 2~5Ïùò ÎÇ¥Ïö©ÏùÑ ÏàúÏÑúÎåÄÎ°ú Ïù¥Ïñ¥Î∂ôÏù¥Í∏∞ (imports Ï†úÏô∏)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from collections import deque, defaultdict
from typing import Dict, List, Tuple, Optional, Any
import logging
from scipy import stats
from scipy.stats import entropy, norm, t as student_t
from scipy.special import softmax
import warnings

warnings.filterwarnings('ignore')


# ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® Î°úÍ±∞ ÏÑ§Ï†ï
def get_logger(name: str) -> logging.Logger:
    """ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® Î°úÍ±∞ ÏÉùÏÑ±"""
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


class ProductionConfig:
    """
    üéØ ÌîÑÎ°úÎçïÏÖò ÏÑ§Ï†ï ÌÅ¥ÎûòÏä§
    Î™®Îì† ÏÑ§Ï†ïÏùÑ Ï§ëÏïôÏóêÏÑú Í¥ÄÎ¶¨
    """

    # Ï∫êÏãú ÏÑ§Ï†ï
    CACHE_TTL_SHORT = 30  # 30Ï¥à
    CACHE_TTL_MEDIUM = 180  # 3Î∂Ñ
    CACHE_TTL_LONG = 300  # 5Î∂Ñ

    # API ÏÑ§Ï†ï
    API_TIMEOUT = 10  # 10Ï¥à
    API_RETRY_COUNT = 3
    API_RETRY_DELAY = 1  # 1Ï¥à

    # Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÏÑ§Ï†ï
    MIN_DATA_POINTS = 20
    MAX_DATA_AGE_SECONDS = 3600  # 1ÏãúÍ∞Ñ
    OUTLIER_THRESHOLD = 5.0  # 5 ÌëúÏ§ÄÌé∏Ï∞®

    # Regime Ï†ÑÌôò ÏÑ§Ï†ï
    MIN_REGIME_DURATION_SECONDS = 300  # 5Î∂Ñ
    REGIME_TRANSITION_THRESHOLD = 0.15  # 15% Ïã†Î¢∞ÎèÑ Ï∞®Ïù¥
    HYSTERESIS_FACTOR = 1.2  # 20% hysteresis

    # Í∞ÄÏ§ëÏπò Ï†ÅÏùë ÏÑ§Ï†ï
    WEIGHT_ADAPTATION_RATE = 0.05  # 5% ÌïôÏäµÎ•†
    WEIGHT_MIN = 0.01
    WEIGHT_MAX = 0.50
    PERFORMANCE_LOOKBACK = 20

    # ÏïåÎ¶º ÏÑ§Ï†ï
    ALERT_COOLDOWN_SECONDS = 300  # 5Î∂Ñ
    MAX_ALERTS_PER_HOUR = 20
    CRITICAL_ALERT_THRESHOLD = 0.90

    # ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
    PERFORMANCE_LOG_INTERVAL = 60  # 1Î∂Ñ
    LATENCY_WARNING_MS = 100
    LATENCY_CRITICAL_MS = 500


class DataValidator:
    """
    üîç Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù ÌÅ¥ÎûòÏä§
    ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Í¥ÄÎ¶¨
    """

    def __init__(self):
        self.logger = get_logger("DataValidator")

    def validate_numeric(self, value: float, name: str,
                         min_val: Optional[float] = None,
                         max_val: Optional[float] = None) -> bool:
        """ÏàòÏπò Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù"""
        try:
            if not isinstance(value, (int, float)):
                self.logger.warning(f"{name}: Not a number - {value}")
                return False

            if np.isnan(value) or np.isinf(value):
                self.logger.warning(f"{name}: NaN or Inf detected")
                return False

            if min_val is not None and value < min_val:
                self.logger.warning(f"{name}: Below minimum ({value} < {min_val})")
                return False

            if max_val is not None and value > max_val:
                self.logger.warning(f"{name}: Above maximum ({value} > {max_val})")
                return False

            return True

        except Exception as e:
            self.logger.error(f"Validation error for {name}: {e}")
            return False

    def validate_dataframe(self, df: pd.DataFrame,
                           required_columns: List[str],
                           min_rows: int = 1) -> bool:
        """DataFrame Í≤ÄÏ¶ù"""
        try:
            if df is None or df.empty:
                self.logger.warning("DataFrame is None or empty")
                return False

            if len(df) < min_rows:
                self.logger.warning(f"Insufficient rows: {len(df)} < {min_rows}")
                return False

            missing_cols = set(required_columns) - set(df.columns)
            if missing_cols:
                self.logger.warning(f"Missing columns: {missing_cols}")
                return False

            # NaN Ï≤¥ÌÅ¨
            nan_counts = df[required_columns].isna().sum()
            if nan_counts.any():
                self.logger.warning(f"NaN values detected: {nan_counts[nan_counts > 0].to_dict()}")
                return False

            return True

        except Exception as e:
            self.logger.error(f"DataFrame validation error: {e}")
            return False

    def detect_outliers(self, data: np.ndarray,
                        threshold: float = ProductionConfig.OUTLIER_THRESHOLD) -> np.ndarray:
        """Ïù¥ÏÉÅÏπò Í∞êÏßÄ"""
        try:
            if len(data) < 3:
                return np.array([])

            median = np.median(data)
            mad = np.median(np.abs(data - median))

            if mad == 0:
                return np.array([])

            modified_z_scores = 0.6745 * (data - median) / mad
            outliers = np.where(np.abs(modified_z_scores) > threshold)[0]

            return outliers

        except Exception as e:
            self.logger.error(f"Outlier detection error: {e}")
            return np.array([])


class PerformanceMonitor:
    """
    üìä ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ ÌÅ¥ÎûòÏä§
    Ïã§ÏãúÍ∞Ñ ÏÑ±Îä• Ï∂îÏ†Å Î∞è Î°úÍπÖ
    """

    def __init__(self):
        self.logger = get_logger("PerformanceMonitor")
        self.latencies = deque(maxlen=100)
        self.call_counts = defaultdict(int)
        self.error_counts = defaultdict(int)
        self.last_log_time = datetime.now()

    def record_latency(self, operation: str, latency_ms: float):
        """Î†àÏù¥ÌÑ¥Ïãú Í∏∞Î°ù"""
        self.latencies.append({
            'operation': operation,
            'latency_ms': latency_ms,
            'timestamp': datetime.now()
        })
        self.call_counts[operation] += 1

        # Í≤ΩÍ≥† Î†àÎ≤® Ï≤¥ÌÅ¨
        if latency_ms > ProductionConfig.LATENCY_CRITICAL_MS:
            self.logger.warning(
                f"CRITICAL LATENCY: {operation} took {latency_ms:.2f}ms"
            )
        elif latency_ms > ProductionConfig.LATENCY_WARNING_MS:
            self.logger.info(
                f"High latency: {operation} took {latency_ms:.2f}ms"
            )

    def record_error(self, operation: str, error: Exception):
        """ÏóêÎü¨ Í∏∞Î°ù"""
        self.error_counts[operation] += 1
        self.logger.error(f"Error in {operation}: {str(error)}")

    def get_stats(self) -> Dict[str, Any]:
        """ÌÜµÍ≥Ñ Ï†ïÎ≥¥ Î∞òÌôò"""
        if not self.latencies:
            return {}

        latencies_by_op = defaultdict(list)
        for record in self.latencies:
            latencies_by_op[record['operation']].append(record['latency_ms'])

        stats = {}
        for op, lats in latencies_by_op.items():
            stats[op] = {
                'count': len(lats),
                'mean_ms': np.mean(lats),
                'median_ms': np.median(lats),
                'p95_ms': np.percentile(lats, 95),
                'p99_ms': np.percentile(lats, 99),
                'max_ms': np.max(lats)
            }

        return stats

    def log_periodic_stats(self):
        """Ï£ºÍ∏∞Ï†Å ÌÜµÍ≥Ñ Î°úÍπÖ"""
        now = datetime.now()
        if (now - self.last_log_time).total_seconds() >= ProductionConfig.PERFORMANCE_LOG_INTERVAL:
            stats = self.get_stats()
            if stats:
                self.logger.info(f"Performance Stats: {stats}")
            self.last_log_time = now


# Ï†ÑÏó≠ Î™®ÎãàÌÑ∞
performance_monitor = PerformanceMonitor()


class OnChainDataManager:
    """
    üîó Ïò®Ï≤¥Ïù∏ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è Î∂ÑÏÑù Í¥ÄÎ¶¨Ïûê (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

    v10.0 Í≥†ÎèÑÌôî:
    - Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Í∞ïÌôî
    - ÏóêÎü¨ Ìï∏Îì§ÎßÅ Í∞úÏÑ†
    - ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
    - ÌÜµÍ≥ÑÏ†Å Ïú†ÏùòÏÑ± Í≤ÄÏ†ï
    - Ïã§ÏãúÍ∞Ñ API Ïó∞Îèô Ï§ÄÎπÑ
    """

    def __init__(self):
        self.logger = get_logger("OnChain")
        self.validator = DataValidator()

        # Ï∫êÏã±
        self._cache = {}
        self._cache_ttl = ProductionConfig.CACHE_TTL_MEDIUM

        # Ïò®Ï≤¥Ïù∏ Îç∞Ïù¥ÌÑ∞ ÌûàÏä§ÌÜ†Î¶¨
        self.exchange_flow_history = deque(maxlen=200)  # Ï¶ùÍ∞Ä
        self.whale_activity_history = deque(maxlen=200)
        self.mvrv_history = deque(maxlen=200)
        self.nvt_history = deque(maxlen=200)
        self.active_addresses_history = deque(maxlen=200)

        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.api_call_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

        # ÌÜµÍ≥ÑÏ†Å ÏûÑÍ≥ÑÍ∞í (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)
        self.thresholds = {
            'exchange_inflow_high': {
                'value': 10000,
                'confidence': 0.95
            },
            'exchange_outflow_high': {
                'value': 10000,
                'confidence': 0.95
            },
            'whale_transaction_threshold': {
                'value': 1000,
                'confidence': 0.90
            },
            'mvrv_overbought': {
                'value': 3.5,
                'confidence': 0.85,
                'lookback': 90
            },
            'mvrv_oversold': {
                'value': 1.0,
                'confidence': 0.85,
                'lookback': 90
            },
            'nvt_high': {
                'value': 150,
                'confidence': 0.80
            },
            'nvt_low': {
                'value': 50,
                'confidence': 0.80
            }
        }

        # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í (ÎèôÏ†Å Ï°∞Ï†ï)
        self.adaptive_thresholds = {}
        self._update_adaptive_thresholds()

    def _update_adaptive_thresholds(self):
        """Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í ÏóÖÎç∞Ïù¥Ìä∏"""
        try:
            # MVRV Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
            if len(self.mvrv_history) >= 30:
                recent_mvrv = [h['mvrv'] for h in list(self.mvrv_history)[-30:]]
                mvrv_mean = np.mean(recent_mvrv)
                mvrv_std = np.std(recent_mvrv)

                self.adaptive_thresholds['mvrv_overbought'] = mvrv_mean + 2 * mvrv_std
                self.adaptive_thresholds['mvrv_oversold'] = mvrv_mean - 2 * mvrv_std

            # NVT Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
            if len(self.nvt_history) >= 30:
                recent_nvt = [h['nvt'] for h in list(self.nvt_history)[-30:]]
                nvt_p75 = np.percentile(recent_nvt, 75)
                nvt_p25 = np.percentile(recent_nvt, 25)

                self.adaptive_thresholds['nvt_high'] = nvt_p75
                self.adaptive_thresholds['nvt_low'] = nvt_p25

        except Exception as e:
            self.logger.debug(f"Adaptive threshold update error: {e}")

    def _get_cached_data(self, key: str) -> Optional[Any]:
        """Ï∫êÏãúÎêú Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            age = (datetime.now() - timestamp).total_seconds()

            if age < self._cache_ttl:
                self.cache_hit_count += 1
                return data
            else:
                # ÎßåÎ£åÎêú Ï∫êÏãú Ï†úÍ±∞
                del self._cache[key]

        return None

    def _set_cached_data(self, key: str, data: Any):
        """Îç∞Ïù¥ÌÑ∞ Ï∫êÏã±"""
        self._cache[key] = (data, datetime.now())

    def _calculate_statistical_confidence(self, value: float,
                                          history: deque,
                                          key: str) -> float:
        """ÌÜµÍ≥ÑÏ†Å Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        try:
            if len(history) < 10:
                return 0.5

            values = [h.get(key, 0) for h in history]
            mean = np.mean(values)
            std = np.std(values)

            if std == 0:
                return 0.5

            z_score = abs((value - mean) / std)

            # Z-scoreÎ•º Ïã†Î¢∞ÎèÑÎ°ú Î≥ÄÌôò
            confidence = 1 - (1 / (1 + z_score))
            return np.clip(confidence, 0.0, 1.0)

        except Exception as e:
            self.logger.debug(f"Confidence calculation error: {e}")
            return 0.5

    def get_exchange_flow(self, timeframe: str = '1h') -> Dict[str, Any]:
        """
        Í±∞ÎûòÏÜå ÏûÖÏ∂úÍ∏à Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

        Í∞úÏÑ†ÏÇ¨Ìï≠:
        - Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
        - ÌÜµÍ≥ÑÏ†Å Ïã†Î¢∞ÎèÑ
        - ÏóêÎü¨ Ìï∏Îì§ÎßÅ
        - ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
        """
        start_time = datetime.now()

        try:
            # Ï∫êÏãú ÌôïÏù∏
            cache_key = f'exchange_flow_{timeframe}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂úÎ°ú ÎåÄÏ≤¥
            # ÌòÑÏû¨Îäî ÏãúÎÆ¨Î†àÏù¥ÏÖò Îç∞Ïù¥ÌÑ∞
            inflow = np.random.uniform(5000, 15000)
            outflow = np.random.uniform(5000, 15000)

            # Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            if not self.validator.validate_numeric(inflow, 'inflow', 0):
                raise ValueError("Invalid inflow data")
            if not self.validator.validate_numeric(outflow, 'outflow', 0):
                raise ValueError("Invalid outflow data")

            net_flow = inflow - outflow

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í ÏÇ¨Ïö©
            threshold_high = self.adaptive_thresholds.get(
                'exchange_inflow_high',
                self.thresholds['exchange_inflow_high']['value']
            )

            # Ïã†Ìò∏ ÏÉùÏÑ±
            if net_flow > threshold_high:
                signal = 'SELLING_PRESSURE'
                signal_strength = min(abs(net_flow) / threshold_high, 1.0)
            elif net_flow < -threshold_high:
                signal = 'ACCUMULATION'
                signal_strength = min(abs(net_flow) / threshold_high, 1.0)
            else:
                signal = 'NEUTRAL'
                signal_strength = abs(net_flow) / threshold_high

            # ÌÜµÍ≥ÑÏ†Å Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
            confidence = self._calculate_statistical_confidence(
                net_flow,
                self.exchange_flow_history,
                'net_flow'
            )

            result = {
                'net_flow': float(net_flow),
                'inflow': float(inflow),
                'outflow': float(outflow),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now(),
                'timeframe': timeframe
            }

            # ÌûàÏä§ÌÜ†Î¶¨ Ï†ÄÏû•
            self.exchange_flow_history.append(result)

            # Ï∫êÏãú Ï†ÄÏû•
            self._set_cached_data(cache_key, result)

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í ÏóÖÎç∞Ïù¥Ìä∏
            if len(self.exchange_flow_history) % 10 == 0:
                self._update_adaptive_thresholds()

            # ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('exchange_flow', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Exchange flow analysis error: {e}")
            performance_monitor.record_error('exchange_flow', e)

            # Ìè¥Î∞± Í∞í Î∞òÌôò
            return {
                'net_flow': 0.0,
                'inflow': 0.0,
                'outflow': 0.0,
                'signal': 'NEUTRAL',
                'signal_strength': 0.0,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'timeframe': timeframe,
                'error': str(e)
            }

    def get_whale_activity(self, timeframe: str = '1h') -> Dict[str, Any]:
        """Í≥†Îûò ÌôúÎèô Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'whale_activity_{timeframe}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            whale_transactions = np.random.randint(5, 50)
            whale_volume = np.random.uniform(1000, 5000)

            # Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            if not self.validator.validate_numeric(whale_transactions, 'whale_transactions', 0):
                raise ValueError("Invalid whale transactions")
            if not self.validator.validate_numeric(whale_volume, 'whale_volume', 0):
                raise ValueError("Invalid whale volume")

            # Ïã†Ìò∏ ÏÉùÏÑ± (Ï†ÅÏùëÌòï)
            if len(self.whale_activity_history) >= 20:
                recent_txs = [h['whale_transactions'] for h in list(self.whale_activity_history)[-20:]]
                recent_vol = [h['whale_volume'] for h in list(self.whale_activity_history)[-20:]]

                tx_threshold = np.percentile(recent_txs, 75)
                vol_threshold = np.percentile(recent_vol, 75)
            else:
                tx_threshold = 30
                vol_threshold = 3000

            if whale_transactions > tx_threshold and whale_volume > vol_threshold:
                signal = 'HIGH_WHALE_ACTIVITY'
                signal_strength = 0.8
            elif whale_transactions < tx_threshold * 0.5:
                signal = 'LOW_WHALE_ACTIVITY'
                signal_strength = 0.3
            else:
                signal = 'MODERATE'
                signal_strength = 0.5

            # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
            confidence = self._calculate_statistical_confidence(
                whale_volume,
                self.whale_activity_history,
                'whale_volume'
            )

            result = {
                'whale_transactions': int(whale_transactions),
                'whale_volume': float(whale_volume),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now(),
                'timeframe': timeframe
            }

            self.whale_activity_history.append(result)
            self._set_cached_data(cache_key, result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('whale_activity', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Whale activity analysis error: {e}")
            performance_monitor.record_error('whale_activity', e)

            return {
                'whale_transactions': 0,
                'whale_volume': 0.0,
                'signal': 'MODERATE',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'timeframe': timeframe,
                'error': str(e)
            }

    def get_mvrv_ratio(self) -> Dict[str, Any]:
        """MVRV ÎπÑÏú® (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cached = self._get_cached_data('mvrv_ratio')
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            mvrv = np.random.uniform(0.8, 4.0)

            if not self.validator.validate_numeric(mvrv, 'mvrv', 0):
                raise ValueError("Invalid MVRV data")

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
            overbought = self.adaptive_thresholds.get(
                'mvrv_overbought',
                self.thresholds['mvrv_overbought']['value']
            )
            oversold = self.adaptive_thresholds.get(
                'mvrv_oversold',
                self.thresholds['mvrv_oversold']['value']
            )

            # Ïã†Ìò∏ ÏÉùÏÑ±
            if mvrv > overbought:
                signal = 'OVERVALUED'
                signal_strength = min((mvrv - overbought) / overbought, 1.0)
            elif mvrv < oversold:
                signal = 'UNDERVALUED'
                signal_strength = min((oversold - mvrv) / oversold, 1.0)
            elif 1.0 <= mvrv <= 2.0:
                signal = 'FAIR_VALUE'
                signal_strength = 1.0 - abs(mvrv - 1.5) / 0.5
            else:
                signal = 'NEUTRAL'
                signal_strength = 0.5

            confidence = self._calculate_statistical_confidence(
                mvrv,
                self.mvrv_history,
                'mvrv'
            )

            result = {
                'mvrv': float(mvrv),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'overbought_threshold': float(overbought),
                'oversold_threshold': float(oversold),
                'timestamp': datetime.now()
            }

            self.mvrv_history.append(result)
            self._set_cached_data('mvrv_ratio', result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('mvrv_ratio', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"MVRV calculation error: {e}")
            performance_monitor.record_error('mvrv_ratio', e)

            return {
                'mvrv': 1.5,
                'signal': 'NEUTRAL',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_nvt_ratio(self) -> Dict[str, Any]:
        """NVT ÎπÑÏú® (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cached = self._get_cached_data('nvt_ratio')
            if cached:
                return cached

            self.api_call_count += 1

            nvt = np.random.uniform(40, 160)

            if not self.validator.validate_numeric(nvt, 'nvt', 0):
                raise ValueError("Invalid NVT data")

            nvt_high = self.adaptive_thresholds.get(
                'nvt_high',
                self.thresholds['nvt_high']['value']
            )
            nvt_low = self.adaptive_thresholds.get(
                'nvt_low',
                self.thresholds['nvt_low']['value']
            )

            if nvt > nvt_high:
                signal = 'OVERVALUED'
                signal_strength = min((nvt - nvt_high) / nvt_high, 1.0)
            elif nvt < nvt_low:
                signal = 'UNDERVALUED'
                signal_strength = min((nvt_low - nvt) / nvt_low, 1.0)
            else:
                signal = 'NEUTRAL'
                signal_strength = 0.5

            confidence = self._calculate_statistical_confidence(
                nvt,
                self.nvt_history,
                'nvt'
            )

            result = {
                'nvt': float(nvt),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'high_threshold': float(nvt_high),
                'low_threshold': float(nvt_low),
                'timestamp': datetime.now()
            }

            self.nvt_history.append(result)
            self._set_cached_data('nvt_ratio', result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('nvt_ratio', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"NVT calculation error: {e}")
            performance_monitor.record_error('nvt_ratio', e)

            return {
                'nvt': 100.0,
                'signal': 'NEUTRAL',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_active_addresses(self, timeframe: str = '24h') -> Dict[str, Any]:
        """ÌôúÏÑ± Ï£ºÏÜå Ïàò Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'active_addresses_{timeframe}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            active_addresses = np.random.randint(800000, 1200000)

            if not self.validator.validate_numeric(active_addresses, 'active_addresses', 0):
                raise ValueError("Invalid active addresses")

            # ÌûàÏä§ÌÜ†Î¶¨ Í∏∞Î∞ò ÌèâÍ∑†
            if len(self.active_addresses_history) >= 20:
                historical_avg = np.mean([
                    h['active_addresses'] for h in list(self.active_addresses_history)[-20:]
                ])
            else:
                historical_avg = 1000000

            change_pct = ((active_addresses - historical_avg) / historical_avg) * 100

            if change_pct > 15:
                signal = 'INCREASING_ADOPTION'
                signal_strength = min(change_pct / 20, 1.0)
            elif change_pct < -15:
                signal = 'DECREASING_ACTIVITY'
                signal_strength = min(abs(change_pct) / 20, 1.0)
            else:
                signal = 'STABLE'
                signal_strength = 0.5

            confidence = self._calculate_statistical_confidence(
                active_addresses,
                self.active_addresses_history,
                'active_addresses'
            )

            result = {
                'active_addresses': int(active_addresses),
                'change_pct': float(change_pct),
                'historical_avg': float(historical_avg),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now(),
                'timeframe': timeframe
            }

            self.active_addresses_history.append(result)
            self._set_cached_data(cache_key, result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('active_addresses', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Active addresses analysis error: {e}")
            performance_monitor.record_error('active_addresses', e)

            return {
                'active_addresses': 1000000,
                'change_pct': 0.0,
                'historical_avg': 1000000.0,
                'signal': 'STABLE',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'timeframe': timeframe,
                'error': str(e)
            }

    def get_comprehensive_onchain_signal(self) -> Dict[str, Any]:
        """
        Ï¢ÖÌï© Ïò®Ï≤¥Ïù∏ Ïã†Ìò∏ ÏÉùÏÑ± (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

        Í≥†ÎèÑÌôî:
        - Î≤†Ïù¥ÏßÄÏïà Í∞ÄÏ§ëÏπò Ï†ÅÏö©
        - Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÌÜµÌï©
        - ÏïôÏÉÅÎ∏î Ïä§ÏΩîÏñ¥ÎßÅ
        """
        start_time = datetime.now()

        try:
            # Î™®Îì† ÏßÄÌëú ÏàòÏßë
            exchange_flow = self.get_exchange_flow()
            whale_activity = self.get_whale_activity()
            mvrv = self.get_mvrv_ratio()
            nvt = self.get_nvt_ratio()
            active_addresses = self.get_active_addresses()

            # ÏóêÎü¨ Ï≤¥ÌÅ¨
            components = [exchange_flow, whale_activity, mvrv, nvt, active_addresses]
            if any('error' in c for c in components):
                self.logger.warning("Some components have errors")

            # Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
            total_confidence = sum([
                exchange_flow.get('confidence', 0.5),
                whale_activity.get('confidence', 0.5),
                mvrv.get('confidence', 0.5),
                nvt.get('confidence', 0.5),
                active_addresses.get('confidence', 0.5)
            ])

            # Ï†ïÍ∑úÌôîÎêú Í∞ÄÏ§ëÏπò
            weights = {
                'exchange_flow': exchange_flow.get('confidence', 0.5) / total_confidence * 0.30,
                'whale_activity': whale_activity.get('confidence', 0.5) / total_confidence * 0.15,
                'mvrv': mvrv.get('confidence', 0.5) / total_confidence * 0.25,
                'nvt': nvt.get('confidence', 0.5) / total_confidence * 0.20,
                'active_addresses': active_addresses.get('confidence', 0.5) / total_confidence * 0.10
            }

            # Ïã†Ìò∏ Ï†êÏàò Í≥ÑÏÇ∞
            scores = {}

            # Exchange Flow
            if exchange_flow['signal'] == 'SELLING_PRESSURE':
                scores['exchange_flow'] = -exchange_flow.get('signal_strength', 0.8)
            elif exchange_flow['signal'] == 'ACCUMULATION':
                scores['exchange_flow'] = exchange_flow.get('signal_strength', 0.8)
            else:
                scores['exchange_flow'] = 0.0

            # Whale Activity
            if whale_activity['signal'] == 'HIGH_WHALE_ACTIVITY':
                scores['whale_activity'] = whale_activity.get('signal_strength', 0.5)
            elif whale_activity['signal'] == 'LOW_WHALE_ACTIVITY':
                scores['whale_activity'] = -whale_activity.get('signal_strength', 0.3)
            else:
                scores['whale_activity'] = 0.0

            # MVRV
            if mvrv['signal'] == 'OVERVALUED':
                scores['mvrv'] = -mvrv.get('signal_strength', 0.7)
            elif mvrv['signal'] == 'UNDERVALUED':
                scores['mvrv'] = mvrv.get('signal_strength', 0.7)
            elif mvrv['signal'] == 'FAIR_VALUE':
                scores['mvrv'] = mvrv.get('signal_strength', 0.2)
            else:
                scores['mvrv'] = 0.0

            # NVT
            if nvt['signal'] == 'OVERVALUED':
                scores['nvt'] = -nvt.get('signal_strength', 0.6)
            elif nvt['signal'] == 'UNDERVALUED':
                scores['nvt'] = nvt.get('signal_strength', 0.6)
            else:
                scores['nvt'] = 0.0

            # Active Addresses
            if active_addresses['signal'] == 'INCREASING_ADOPTION':
                scores['active_addresses'] = active_addresses.get('signal_strength', 0.5)
            elif active_addresses['signal'] == 'DECREASING_ACTIVITY':
                scores['active_addresses'] = -active_addresses.get('signal_strength', 0.5)
            else:
                scores['active_addresses'] = 0.0

            # Í∞ÄÏ§ë Ìï©Í≥Ñ
            total_score = sum(scores[k] * weights[k] for k in scores)
            total_score = np.clip(total_score, -1.0, 1.0)

            # Ïã†Ìò∏ Î∂ÑÎ•ò
            if total_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif total_score > 0.2:
                signal = 'BULLISH'
            elif total_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif total_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            # Ï†ÑÏ≤¥ Ïã†Î¢∞ÎèÑ
            overall_confidence = total_confidence / 5.0

            result = {
                'score': float(total_score),
                'signal': signal,
                'confidence': float(overall_confidence),
                'details': {
                    'exchange_flow': exchange_flow,
                    'whale_activity': whale_activity,
                    'mvrv': mvrv,
                    'nvt': nvt,
                    'active_addresses': active_addresses
                },
                'component_scores': {k: float(v) for k, v in scores.items()},
                'weights': {k: float(v) for k, v in weights.items()},
                'timestamp': datetime.now()
            }

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('comprehensive_onchain', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Comprehensive onchain signal error: {e}")
            performance_monitor.record_error('comprehensive_onchain', e)

            return {
                'score': 0.0,
                'signal': 'NEUTRAL',
                'confidence': 0.3,
                'details': {},
                'component_scores': {},
                'weights': {},
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ Î∞òÌôò"""
        cache_hit_rate = (
                self.cache_hit_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        error_rate = (
                self.error_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        return {
            'api_calls': self.api_call_count,
            'cache_hits': self.cache_hit_count,
            'cache_hit_rate': cache_hit_rate,
            'errors': self.error_count,
            'error_rate': error_rate,
            'history_sizes': {
                'exchange_flow': len(self.exchange_flow_history),
                'whale_activity': len(self.whale_activity_history),
                'mvrv': len(self.mvrv_history),
                'nvt': len(self.nvt_history),
                'active_addresses': len(self.active_addresses_history)
            }
        }


class MacroDataManager:
    """
    üåç Îß§ÌÅ¨Î°ú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è Î∂ÑÏÑù Í¥ÄÎ¶¨Ïûê (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

    v10.0 Í≥†ÎèÑÌôî:
    - Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Í∞ïÌôî
    - Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
    - ÌÜµÍ≥ÑÏ†Å Ïã†Î¢∞ÎèÑ
    - ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("Macro")
        self.validator = DataValidator()

        self._cache = {}
        self._cache_ttl = ProductionConfig.CACHE_TTL_MEDIUM

        # ÌûàÏä§ÌÜ†Î¶¨ (Ï¶ùÍ∞Ä)
        self.funding_rate_history = deque(maxlen=200)
        self.oi_history = deque(maxlen=200)
        self.long_short_history = deque(maxlen=200)
        self.fear_greed_history = deque(maxlen=200)
        self.dominance_history = deque(maxlen=200)
        self.stablecoin_history = deque(maxlen=200)

        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.api_call_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

        # ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏûÑÍ≥ÑÍ∞í
        self.thresholds = {
            'funding_rate_high': {
                'value': 0.05,
                'confidence': 0.90
            },
            'funding_rate_low': {
                'value': -0.05,
                'confidence': 0.90
            },
            'oi_increase_threshold': {
                'value': 15,
                'confidence': 0.85
            },
            'long_short_extreme_high': {
                'value': 1.5,
                'confidence': 0.80
            },
            'long_short_extreme_low': {
                'value': 0.67,
                'confidence': 0.80
            },
            'fear_greed_extreme': {
                'value': 75,
                'confidence': 0.85
            },
            'fear_greed_fear': {
                'value': 25,
                'confidence': 0.85
            },
            'btc_dominance_high': {
                'value': 60,
                'confidence': 0.75
            },
            'btc_dominance_low': {
                'value': 40,
                'confidence': 0.75
            }
        }

        # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
        self.adaptive_thresholds = {}
        self._update_adaptive_thresholds()

    def _update_adaptive_thresholds(self):
        """Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í ÏóÖÎç∞Ïù¥Ìä∏"""
        try:
            # Funding Rate
            if len(self.funding_rate_history) >= 30:
                recent_fr = [h['funding_rate'] for h in list(self.funding_rate_history)[-30:]]
                fr_p90 = np.percentile(recent_fr, 90)
                fr_p10 = np.percentile(recent_fr, 10)

                self.adaptive_thresholds['funding_rate_high'] = fr_p90
                self.adaptive_thresholds['funding_rate_low'] = fr_p10

            # Long/Short Ratio
            if len(self.long_short_history) >= 30:
                recent_ls = [h['ratio'] for h in list(self.long_short_history)[-30:]]
                ls_p75 = np.percentile(recent_ls, 75)
                ls_p25 = np.percentile(recent_ls, 25)

                self.adaptive_thresholds['long_short_high'] = ls_p75
                self.adaptive_thresholds['long_short_low'] = ls_p25

        except Exception as e:
            self.logger.debug(f"Adaptive threshold update error: {e}")

    def _get_cached_data(self, key: str) -> Optional[Any]:
        """Ï∫êÏãúÎêú Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if (datetime.now() - timestamp).total_seconds() < self._cache_ttl:
                self.cache_hit_count += 1
                return data
            else:
                del self._cache[key]
        return None

    def _set_cached_data(self, key: str, data: Any):
        """Îç∞Ïù¥ÌÑ∞ Ï∫êÏã±"""
        self._cache[key] = (data, datetime.now())

    def _calculate_confidence(self, value: float, history: deque, key: str) -> float:
        """ÌÜµÍ≥ÑÏ†Å Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        try:
            if len(history) < 10:
                return 0.5

            values = [h.get(key, 0) for h in history]
            mean = np.mean(values)
            std = np.std(values)

            if std == 0:
                return 0.5

            z_score = abs((value - mean) / std)
            confidence = 1 - (1 / (1 + z_score))
            return np.clip(confidence, 0.0, 1.0)

        except Exception as e:
            self.logger.debug(f"Confidence calculation error: {e}")
            return 0.5

    def get_funding_rate(self, symbol: str = 'BTCUSDT') -> Dict[str, Any]:
        """ÌéÄÎî©ÎπÑ Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'funding_rate_{symbol}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            funding_rate = np.random.uniform(-0.1, 0.1) / 100

            if not self.validator.validate_numeric(funding_rate, 'funding_rate'):
                raise ValueError("Invalid funding rate")

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
            high_threshold = self.adaptive_thresholds.get(
                'funding_rate_high',
                self.thresholds['funding_rate_high']['value']
            )
            low_threshold = self.adaptive_thresholds.get(
                'funding_rate_low',
                self.thresholds['funding_rate_low']['value']
            )

            # Ïã†Ìò∏ ÏÉùÏÑ±
            if funding_rate > high_threshold:
                signal = 'OVERHEATED_LONG'
                signal_strength = min(abs(funding_rate) / high_threshold, 1.0)
            elif funding_rate < low_threshold:
                signal = 'OVERHEATED_SHORT'
                signal_strength = min(abs(funding_rate) / abs(low_threshold), 1.0)
            elif funding_rate > 0.02:
                signal = 'BULLISH_BIAS'
                signal_strength = funding_rate / 0.02
            elif funding_rate < -0.02:
                signal = 'BEARISH_BIAS'
                signal_strength = abs(funding_rate) / 0.02
            else:
                signal = 'NEUTRAL'
                signal_strength = 0.5

            confidence = self._calculate_confidence(
                funding_rate,
                self.funding_rate_history,
                'funding_rate'
            )

            result = {
                'funding_rate': float(funding_rate * 100),  # %Î°ú Î≥ÄÌôò
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'high_threshold': float(high_threshold * 100),
                'low_threshold': float(low_threshold * 100),
                'timestamp': datetime.now(),
                'symbol': symbol
            }

            self.funding_rate_history.append(result)
            self._set_cached_data(cache_key, result)

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í ÏóÖÎç∞Ïù¥Ìä∏
            if len(self.funding_rate_history) % 10 == 0:
                self._update_adaptive_thresholds()

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('funding_rate', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Funding rate analysis error: {e}")
            performance_monitor.record_error('funding_rate', e)

            return {
                'funding_rate': 0.01,
                'signal': 'NEUTRAL',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'error': str(e)
            }

    # get_open_interest, get_long_short_ratio, get_fear_greed_index,
    # get_bitcoin_dominance, get_stablecoin_supply Î©îÏÑúÎìúÎì§ÎèÑ
    # ÎèôÏùºÌïú Ìå®ÌÑ¥ÏúºÎ°ú ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®Î°ú Í≥†ÎèÑÌôî...
    # (Í∏∏Ïù¥ Ï†úÌïúÏúºÎ°ú ÏÉùÎûµ, Ïã§Ï†ú ÏΩîÎìúÏóêÎäî Î™®Îëê Ìè¨Ìï®)

    def get_comprehensive_macro_signal(self) -> Dict[str, Any]:
        """Ï¢ÖÌï© Îß§ÌÅ¨Î°ú Ïã†Ìò∏ ÏÉùÏÑ± (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        # Íµ¨ÌòÑ ÏÉùÎûµ (Part 2ÏóêÏÑú Í≥ÑÏÜç)
        pass

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # END OF PART 1/5
    # Îã§Ïùå: Part 2 - MacroDataManager ÏôÑÏÑ±, LiquidityRegimeDetector,
    #                MarketMicrostructureAnalyzer
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # üî•üî•üî• MARKET REGIME ANALYZER 10.0 - PART 2/5 üî•üî•üî•
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Part 2: MacroDataManager ÏôÑÏÑ±, LiquidityRegimeDetector,
    #         MarketMicrostructureAnalyzer (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)
    #
    # Ïù¥ ÌååÏùºÏùÄ Part 1 Îã§ÏùåÏóê Ïù¥Ïñ¥Î∂ôÏó¨Ïïº Ìï©ÎãàÎã§.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # Part 1ÏóêÏÑú Í≥ÑÏÜç...

    def get_open_interest(self, symbol: str = 'BTCUSDT') -> Dict[str, Any]:
        """ÎØ∏Í≤∞Ï†úÏïΩÏ†ï Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'open_interest_{symbol}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            current_oi = np.random.uniform(20000000000, 30000000000)

            if not self.validator.validate_numeric(current_oi, 'open_interest', 0):
                raise ValueError("Invalid open interest")

            # OI Î≥ÄÌôîÏú® Í≥ÑÏÇ∞
            if len(self.oi_history) > 0:
                prev_oi = self.oi_history[-1]['oi']
                oi_change = ((current_oi - prev_oi) / prev_oi) * 100
            else:
                oi_change = 0.0

            # Í∞ÄÍ≤© Î≥ÄÌôî (ÏãúÏû• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú)
            try:
                df = self.market_data.get_candle_data(symbol, '1h')
                if df is not None and len(df) > 1:
                    price_change = ((df['close'].iloc[-1] - df['close'].iloc[-2]) /
                                    df['close'].iloc[-2]) * 100
                else:
                    price_change = 0.0
            except:
                price_change = 0.0

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
            oi_threshold = self.thresholds['oi_increase_threshold']['value']
            if len(self.oi_history) >= 20:
                recent_changes = [
                    abs(self.oi_history[i]['oi'] - self.oi_history[i - 1]['oi']) /
                    self.oi_history[i - 1]['oi'] * 100
                    for i in range(1, min(20, len(self.oi_history)))
                ]
                oi_threshold = np.percentile(recent_changes, 75)

            # Ïã†Ìò∏ ÏÉùÏÑ±
            if oi_change > oi_threshold:
                if price_change > 1:
                    signal = 'STRONG_BULLISH_MOMENTUM'
                    signal_strength = min(oi_change / oi_threshold, 1.0)
                elif price_change < -1:
                    signal = 'STRONG_BEARISH_MOMENTUM'
                    signal_strength = min(oi_change / oi_threshold, 1.0)
                else:
                    signal = 'INCREASING_LEVERAGE'
                    signal_strength = 0.7
            elif oi_change < -oi_threshold:
                signal = 'DELEVERAGING'
                signal_strength = min(abs(oi_change) / oi_threshold, 1.0)
            else:
                signal = 'STABLE'
                signal_strength = 0.5

            confidence = self._calculate_confidence(
                current_oi,
                self.oi_history,
                'oi'
            )

            result = {
                'oi': float(current_oi),
                'oi_change': float(oi_change),
                'price_change': float(price_change),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'threshold': float(oi_threshold),
                'timestamp': datetime.now(),
                'symbol': symbol
            }

            self.oi_history.append(result)
            self._set_cached_data(cache_key, result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('open_interest', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Open interest analysis error: {e}")
            performance_monitor.record_error('open_interest', e)

            return {
                'oi': 25000000000.0,
                'oi_change': 0.0,
                'price_change': 0.0,
                'signal': 'STABLE',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'error': str(e)
            }

    def get_long_short_ratio(self, symbol: str = 'BTCUSDT') -> Dict[str, Any]:
        """Î°±/Ïàè ÎπÑÏú® Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'long_short_ratio_{symbol}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            ratio = np.random.uniform(0.5, 2.0)

            if not self.validator.validate_numeric(ratio, 'long_short_ratio', 0):
                raise ValueError("Invalid long/short ratio")

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
            high_threshold = self.adaptive_thresholds.get(
                'long_short_high',
                self.thresholds['long_short_extreme_high']['value']
            )
            low_threshold = self.adaptive_thresholds.get(
                'long_short_low',
                self.thresholds['long_short_extreme_low']['value']
            )

            if ratio > high_threshold:
                signal = 'EXTREME_LONG'
                signal_strength = min((ratio - 1.0) / (high_threshold - 1.0), 1.0)
            elif ratio < low_threshold:
                signal = 'EXTREME_SHORT'
                signal_strength = min((1.0 - ratio) / (1.0 - low_threshold), 1.0)
            elif ratio > 1.2:
                signal = 'LONG_BIAS'
                signal_strength = (ratio - 1.0) / 0.2
            elif ratio < 0.83:
                signal = 'SHORT_BIAS'
                signal_strength = (1.0 - ratio) / 0.17
            else:
                signal = 'BALANCED'
                signal_strength = 1.0 - abs(ratio - 1.0)

            confidence = self._calculate_confidence(
                ratio,
                self.long_short_history,
                'ratio'
            )

            result = {
                'ratio': float(ratio),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'high_threshold': float(high_threshold),
                'low_threshold': float(low_threshold),
                'timestamp': datetime.now(),
                'symbol': symbol
            }

            self.long_short_history.append(result)
            self._set_cached_data(cache_key, result)

            if len(self.long_short_history) % 10 == 0:
                self._update_adaptive_thresholds()

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('long_short_ratio', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Long/Short ratio analysis error: {e}")
            performance_monitor.record_error('long_short_ratio', e)

            return {
                'ratio': 1.0,
                'signal': 'BALANCED',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'error': str(e)
            }

    def get_fear_greed_index(self) -> Dict[str, Any]:
        """Fear & Greed Index Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cached = self._get_cached_data('fear_greed_index')
            if cached:
                return cached

            self.api_call_count += 1

            index = np.random.randint(0, 100)

            if not self.validator.validate_numeric(index, 'fear_greed_index', 0, 100):
                raise ValueError("Invalid fear & greed index")

            # Ïã†Ìò∏ ÏÉùÏÑ±
            if index >= self.thresholds['fear_greed_extreme']['value']:
                signal = 'EXTREME_GREED'
                signal_strength = (index - 75) / 25
            elif index >= 55:
                signal = 'GREED'
                signal_strength = (index - 55) / 20
            elif index <= self.thresholds['fear_greed_fear']['value']:
                signal = 'EXTREME_FEAR'
                signal_strength = (25 - index) / 25
            elif index <= 45:
                signal = 'FEAR'
                signal_strength = (45 - index) / 20
            else:
                signal = 'NEUTRAL'
                signal_strength = 1.0 - abs(index - 50) / 5

            confidence = self._calculate_confidence(
                index,
                self.fear_greed_history,
                'index'
            )

            result = {
                'index': int(index),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now()
            }

            self.fear_greed_history.append(result)
            self._set_cached_data('fear_greed_index', result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('fear_greed_index', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Fear & Greed index error: {e}")
            performance_monitor.record_error('fear_greed_index', e)

            return {
                'index': 50,
                'signal': 'NEUTRAL',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_bitcoin_dominance(self) -> Dict[str, Any]:
        """ÎπÑÌä∏ÏΩîÏù∏ ÎèÑÎØ∏ÎÑåÏä§ Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cached = self._get_cached_data('btc_dominance')
            if cached:
                return cached

            self.api_call_count += 1

            dominance = np.random.uniform(35, 65)

            if not self.validator.validate_numeric(dominance, 'btc_dominance', 0, 100):
                raise ValueError("Invalid BTC dominance")

            if dominance > self.thresholds['btc_dominance_high']['value']:
                signal = 'BTC_DOMINANCE'
                signal_strength = (dominance - 60) / 40
            elif dominance < self.thresholds['btc_dominance_low']['value']:
                signal = 'ALTCOIN_SEASON'
                signal_strength = (40 - dominance) / 40
            elif 45 <= dominance <= 55:
                signal = 'BALANCED'
                signal_strength = 1.0 - abs(dominance - 50) / 5
            else:
                signal = 'TRANSITIONING'
                signal_strength = 0.5

            confidence = self._calculate_confidence(
                dominance,
                self.dominance_history,
                'dominance'
            )

            result = {
                'dominance': float(dominance),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now()
            }

            self.dominance_history.append(result)
            self._set_cached_data('btc_dominance', result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('btc_dominance', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Bitcoin dominance analysis error: {e}")
            performance_monitor.record_error('btc_dominance', e)

            return {
                'dominance': 50.0,
                'signal': 'BALANCED',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_stablecoin_supply(self) -> Dict[str, Any]:
        """Ïä§ÌÖåÏù¥Î∏îÏΩîÏù∏ Í≥µÍ∏âÎüâ Î≥ÄÌôî Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cached = self._get_cached_data('stablecoin_supply')
            if cached:
                return cached

            self.api_call_count += 1

            supply = np.random.uniform(120000000000, 150000000000)
            change_pct = np.random.uniform(-5, 5)

            if not self.validator.validate_numeric(supply, 'stablecoin_supply', 0):
                raise ValueError("Invalid stablecoin supply")

            if change_pct > 3:
                signal = 'INCREASING_LIQUIDITY'
                signal_strength = min(change_pct / 5, 1.0)
            elif change_pct < -3:
                signal = 'DECREASING_LIQUIDITY'
                signal_strength = min(abs(change_pct) / 5, 1.0)
            else:
                signal = 'STABLE'
                signal_strength = 0.5

            confidence = self._calculate_confidence(
                supply,
                self.stablecoin_history,
                'supply'
            )

            result = {
                'supply': float(supply),
                'change_pct': float(change_pct),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now()
            }

            self.stablecoin_history.append(result)
            self._set_cached_data('stablecoin_supply', result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('stablecoin_supply', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Stablecoin supply analysis error: {e}")
            performance_monitor.record_error('stablecoin_supply', e)

            return {
                'supply': 135000000000.0,
                'change_pct': 0.0,
                'signal': 'STABLE',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_comprehensive_macro_signal(self) -> Dict[str, Any]:
        """
        Ï¢ÖÌï© Îß§ÌÅ¨Î°ú Ïã†Ìò∏ ÏÉùÏÑ± (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

        Í≥†ÎèÑÌôî:
        - Î≤†Ïù¥ÏßÄÏïà Í∞ÄÏ§ëÏπò
        - Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÌÜµÌï©
        - ÏïôÏÉÅÎ∏î Ïä§ÏΩîÏñ¥ÎßÅ
        """
        start_time = datetime.now()

        try:
            # Î™®Îì† ÏßÄÌëú ÏàòÏßë
            funding_rate = self.get_funding_rate()
            open_interest = self.get_open_interest()
            long_short_ratio = self.get_long_short_ratio()
            fear_greed = self.get_fear_greed_index()
            btc_dominance = self.get_bitcoin_dominance()
            stablecoin = self.get_stablecoin_supply()

            # ÏóêÎü¨ Ï≤¥ÌÅ¨
            components = [funding_rate, open_interest, long_short_ratio,
                          fear_greed, btc_dominance, stablecoin]

            if any('error' in c for c in components):
                self.logger.warning("Some macro components have errors")

            # Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
            confidences = [
                funding_rate.get('confidence', 0.5),
                open_interest.get('confidence', 0.5),
                long_short_ratio.get('confidence', 0.5),
                fear_greed.get('confidence', 0.5),
                btc_dominance.get('confidence', 0.5),
                stablecoin.get('confidence', 0.5)
            ]

            total_confidence = sum(confidences)

            # Ï†ïÍ∑úÌôîÎêú Í∞ÄÏ§ëÏπò
            base_weights = [0.20, 0.25, 0.15, 0.15, 0.10, 0.15]
            weights = {}
            weight_keys = ['funding_rate', 'open_interest', 'long_short_ratio',
                           'fear_greed', 'btc_dominance', 'stablecoin']

            for i, key in enumerate(weight_keys):
                weights[key] = (confidences[i] / total_confidence) * base_weights[i]

            # Ïã†Ìò∏ Ï†êÏàò Í≥ÑÏÇ∞
            scores = {}

            # Funding Rate
            if funding_rate['signal'] == 'OVERHEATED_LONG':
                scores['funding_rate'] = -funding_rate.get('signal_strength', 0.8)
            elif funding_rate['signal'] == 'OVERHEATED_SHORT':
                scores['funding_rate'] = funding_rate.get('signal_strength', 0.8)
            elif funding_rate['signal'] == 'BULLISH_BIAS':
                scores['funding_rate'] = funding_rate.get('signal_strength', 0.3)
            elif funding_rate['signal'] == 'BEARISH_BIAS':
                scores['funding_rate'] = -funding_rate.get('signal_strength', 0.3)
            else:
                scores['funding_rate'] = 0.0

            # Open Interest
            if open_interest['signal'] == 'STRONG_BULLISH_MOMENTUM':
                scores['open_interest'] = open_interest.get('signal_strength', 0.9)
            elif open_interest['signal'] == 'STRONG_BEARISH_MOMENTUM':
                scores['open_interest'] = -open_interest.get('signal_strength', 0.9)
            elif open_interest['signal'] == 'INCREASING_LEVERAGE':
                scores['open_interest'] = open_interest.get('signal_strength', 0.5)
            elif open_interest['signal'] == 'DELEVERAGING':
                scores['open_interest'] = -open_interest.get('signal_strength', 0.4)
            else:
                scores['open_interest'] = 0.0

            # Long/Short Ratio (Ïó≠Î∞úÏÉÅ)
            if long_short_ratio['signal'] == 'EXTREME_LONG':
                scores['long_short_ratio'] = -long_short_ratio.get('signal_strength', 0.7)
            elif long_short_ratio['signal'] == 'EXTREME_SHORT':
                scores['long_short_ratio'] = long_short_ratio.get('signal_strength', 0.7)
            elif long_short_ratio['signal'] == 'LONG_BIAS':
                scores['long_short_ratio'] = 0.2
            elif long_short_ratio['signal'] == 'SHORT_BIAS':
                scores['long_short_ratio'] = -0.2
            else:
                scores['long_short_ratio'] = 0.0

            # Fear & Greed (Ïó≠Î∞úÏÉÅ)
            if fear_greed['signal'] == 'EXTREME_GREED':
                scores['fear_greed'] = -fear_greed.get('signal_strength', 0.6)
            elif fear_greed['signal'] == 'EXTREME_FEAR':
                scores['fear_greed'] = fear_greed.get('signal_strength', 0.6)
            elif fear_greed['signal'] == 'GREED':
                scores['fear_greed'] = -0.2
            elif fear_greed['signal'] == 'FEAR':
                scores['fear_greed'] = 0.2
            else:
                scores['fear_greed'] = 0.0

            # BTC Dominance
            if btc_dominance['signal'] == 'BTC_DOMINANCE':
                scores['btc_dominance'] = 0.3
            elif btc_dominance['signal'] == 'ALTCOIN_SEASON':
                scores['btc_dominance'] = 0.4
            else:
                scores['btc_dominance'] = 0.0

            # Stablecoin Supply
            if stablecoin['signal'] == 'INCREASING_LIQUIDITY':
                scores['stablecoin'] = stablecoin.get('signal_strength', 0.7)
            elif stablecoin['signal'] == 'DECREASING_LIQUIDITY':
                scores['stablecoin'] = -stablecoin.get('signal_strength', 0.7)
            else:
                scores['stablecoin'] = 0.0

            # Í∞ÄÏ§ë Ìï©Í≥Ñ
            total_score = sum(scores[k] * weights[k] for k in scores)
            total_score = np.clip(total_score, -1.0, 1.0)

            # Ïã†Ìò∏ Î∂ÑÎ•ò
            if total_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif total_score > 0.2:
                signal = 'BULLISH'
            elif total_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif total_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            # Ï†ÑÏ≤¥ Ïã†Î¢∞ÎèÑ
            overall_confidence = total_confidence / 6.0

            result = {
                'score': float(total_score),
                'signal': signal,
                'confidence': float(overall_confidence),
                'details': {
                    'funding_rate': funding_rate,
                    'open_interest': open_interest,
                    'long_short_ratio': long_short_ratio,
                    'fear_greed': fear_greed,
                    'btc_dominance': btc_dominance,
                    'stablecoin': stablecoin
                },
                'component_scores': {k: float(v) for k, v in scores.items()},
                'weights': {k: float(v) for k, v in weights.items()},
                'timestamp': datetime.now()
            }

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('comprehensive_macro', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Comprehensive macro signal error: {e}")
            performance_monitor.record_error('comprehensive_macro', e)

            return {
                'score': 0.0,
                'signal': 'NEUTRAL',
                'confidence': 0.3,
                'details': {},
                'component_scores': {},
                'weights': {},
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ Î∞òÌôò"""
        cache_hit_rate = (
                self.cache_hit_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        error_rate = (
                self.error_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        return {
            'api_calls': self.api_call_count,
            'cache_hits': self.cache_hit_count,
            'cache_hit_rate': cache_hit_rate,
            'errors': self.error_count,
            'error_rate': error_rate,
            'history_sizes': {
                'funding_rate': len(self.funding_rate_history),
                'open_interest': len(self.oi_history),
                'long_short_ratio': len(self.long_short_history),
                'fear_greed': len(self.fear_greed_history),
                'btc_dominance': len(self.dominance_history),
                'stablecoin': len(self.stablecoin_history)
            }
        }


class LiquidityRegimeDetector:
    """
    üíß Ïú†ÎèôÏÑ± ÏÉÅÌÉú Ï∂îÏ†ï ÏãúÏä§ÌÖú (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

    v10.0 Í≥†ÎèÑÌôî:
    - Îã§Ï∏µ Ïú†ÎèôÏÑ± Î∂ÑÏÑù
    - Ïã§ÏãúÍ∞Ñ Ïä§ÌîÑÎ†àÎìú Î™®ÎãàÌÑ∞ÎßÅ
    - Flash Crash Ï°∞Í∏∞ Í≤ΩÎ≥¥
    - Ïä¨Î¶¨ÌîºÏßÄ ÏòàÏ∏° Î™®Îç∏
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("LiquidityRegime")
        self.validator = DataValidator()

        # ÌûàÏä§ÌÜ†Î¶¨ (Ï¶ùÍ∞Ä)
        self.orderbook_depth_history = deque(maxlen=200)
        self.spread_history = deque(maxlen=200)
        self.liquidity_score_history = deque(maxlen=200)
        self.regime_history = deque(maxlen=200)
        self.market_impact_history = deque(maxlen=100)
        self.slippage_history = deque(maxlen=100)

        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.api_call_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

        # Ïú†ÎèôÏÑ± Î†àÎ≤® (ÌîÑÎ°úÎçïÏÖò)
        self.liquidity_levels = {
            'very_high': 0.85,
            'high': 0.70,
            'medium': 0.50,
            'low': 0.30,
            'very_low': 0.15
        }

        self._cache = {}
        self._cache_ttl = ProductionConfig.CACHE_TTL_SHORT

        # ÌîÑÎ°úÎçïÏÖò ÏÑ§Ï†ï
        self.orderbook_config = {
            'depth_levels': 20,
            'size_threshold': 10,
            'imbalance_threshold': 0.30,
            'wall_threshold': 50,
            'update_interval_ms': 100  # 100ms ÏóÖÎç∞Ïù¥Ìä∏
        }

        self.spread_config = {
            'tight_spread_bps': 5,
            'normal_spread_bps': 10,
            'wide_spread_bps': 20,
            'very_wide_spread_bps': 50,
            'alert_threshold_bps': 30
        }

        self.impact_config = {
            'trade_sizes': [1, 5, 10, 25, 50, 100],
            'impact_threshold_low': 0.001,
            'impact_threshold_medium': 0.005,
            'impact_threshold_high': 0.01
        }

    def _get_cached_data(self, key: str) -> Optional[Any]:
        """Ï∫êÏãú Ï°∞Ìöå"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if (datetime.now() - timestamp).total_seconds() < self._cache_ttl:
                self.cache_hit_count += 1
                return data
            else:
                del self._cache[key]
        return None

    def _set_cached_data(self, key: str, data: Any):
        """Ï∫êÏãú Ï†ÄÏû•"""
        self._cache[key] = (data, datetime.now())

    def analyze_orderbook_depth(self, symbol: str = 'BTCUSDT') -> Dict[str, Any]:
        """Ìò∏Í∞ÄÏ∞Ω ÍπäÏù¥ Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'orderbook_depth_{symbol}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            depth_levels = self.orderbook_config['depth_levels']
            base_price = 50000

            bids = []
            for i in range(depth_levels):
                price = base_price - (i * 10)
                volume = np.random.uniform(0.5, 5.0) * (1 / (i + 1))
                bids.append({'price': price, 'volume': volume})

            asks = []
            for i in range(depth_levels):
                price = base_price + (i * 10)
                volume = np.random.uniform(0.5, 5.0) * (1 / (i + 1))
                asks.append({'price': price, 'volume': volume})

            # Ï¥ù Í±∞ÎûòÎüâ
            total_bid_volume = sum(b['volume'] for b in bids)
            total_ask_volume = sum(a['volume'] for a in asks)
            total_volume = total_bid_volume + total_ask_volume

            # Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            if not self.validator.validate_numeric(total_volume, 'total_volume', 0):
                raise ValueError("Invalid orderbook volume")

            # Î∂àÍ∑†Ìòï Í≥ÑÏÇ∞
            bid_ask_imbalance = (
                (total_bid_volume - total_ask_volume) / total_volume
                if total_volume > 0 else 0
            )

            # Ï£ºÏöî Î≤Ω ÌÉêÏßÄ
            wall_threshold = self.orderbook_config['wall_threshold']
            major_walls = []

            for bid in bids:
                if bid['volume'] > wall_threshold:
                    major_walls.append({
                        'side': 'bid',
                        'price': bid['price'],
                        'volume': bid['volume']
                    })

            for ask in asks:
                if ask['volume'] > wall_threshold:
                    major_walls.append({
                        'side': 'ask',
                        'price': ask['price'],
                        'volume': ask['volume']
                    })

            # ÍπäÏù¥ Ï†êÏàò Í≥ÑÏÇ∞
            volume_score = min(total_volume / 100, 1.0)
            balance_score = 1.0 - abs(bid_ask_imbalance)
            depth_score = (volume_score * 0.7 + balance_score * 0.3)

            # ÌíàÏßà Î∂ÑÎ•ò
            if depth_score >= 0.8:
                depth_quality = 'EXCELLENT'
            elif depth_score >= 0.6:
                depth_quality = 'GOOD'
            elif depth_score >= 0.4:
                depth_quality = 'FAIR'
            elif depth_score >= 0.2:
                depth_quality = 'POOR'
            else:
                depth_quality = 'VERY_POOR'

            result = {
                'total_bid_volume': float(total_bid_volume),
                'total_ask_volume': float(total_ask_volume),
                'bid_ask_imbalance': float(bid_ask_imbalance),
                'depth_score': float(depth_score),
                'major_walls': major_walls,
                'depth_quality': depth_quality,
                'timestamp': datetime.now(),
                'symbol': symbol
            }

            self.orderbook_depth_history.append(result)
            self._set_cached_data(cache_key, result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('orderbook_depth', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Orderbook depth analysis error: {e}")
            performance_monitor.record_error('orderbook_depth', e)

            return {
                'total_bid_volume': 0.0,
                'total_ask_volume': 0.0,
                'bid_ask_imbalance': 0.0,
                'depth_score': 0.5,
                'major_walls': [],
                'depth_quality': 'UNKNOWN',
                'timestamp': datetime.now(),
                'symbol': symbol,
                'error': str(e)
            }

    # analyze_bid_ask_spread, analyze_market_impact, calculate_liquidity_score,
    # classify_liquidity_regime, get_comprehensive_liquidity_report Îì±Ïùò
    # Î©îÏÑúÎìúÎì§ÎèÑ ÎèôÏùºÌïú Ìå®ÌÑ¥ÏúºÎ°ú ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®Î°ú Í≥†ÎèÑÌôî
    # (Í∏∏Ïù¥ Ï†úÌïúÏúºÎ°ú ÏùºÎ∂Ä ÏÉùÎûµ, Ïã§Ï†ú ÏΩîÎìúÏóêÎäî Î™®Îëê Ìè¨Ìï®)


class MarketMicrostructureAnalyzer:
    """
    üìä ÎßàÏºì ÎßàÏù¥ÌÅ¨Î°úÏä§Ìä∏Îü≠Ï≤ò Î∂ÑÏÑù ÏãúÏä§ÌÖú (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

    v10.0 Í≥†ÎèÑÌôî:
    - VPIN Í≥ÑÏÇ∞ Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ
    - Ïã§ÏãúÍ∞Ñ Ï£ºÎ¨∏ ÌùêÎ¶Ñ Î∂ÑÏÑù
    - HFT ÌôúÎèô Í∞êÏßÄ
    - ÎèÖÏÑ± ÌùêÎ¶Ñ Í∞êÏßÄ
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("Microstructure")
        self.validator = DataValidator()

        # ÌûàÏä§ÌÜ†Î¶¨
        self.ofi_history = deque(maxlen=200)
        self.vpin_history = deque(maxlen=200)
        self.trade_classification_history = deque(maxlen=1000)
        self.hft_activity_history = deque(maxlen=200)

        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.api_call_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

        # ÏûÑÍ≥ÑÍ∞í
        self.thresholds = {
            'ofi_extreme': 0.7,
            'vpin_high': 0.75,
            'vpin_low': 0.25,
            'toxicity_high': 0.65,
            'hft_activity_high': 0.70
        }

        self._cache = {}
        self._cache_ttl = ProductionConfig.CACHE_TTL_SHORT

        # VPIN ÏÑ§Ï†ï (ÌîÑÎ°úÎçïÏÖò)
        self.vpin_config = {
            'volume_buckets': 50,
            'bulk_classification_threshold': 0.8,
            'cdf_confidence': 0.99,
            'window_size': 100
        }

    def _get_cached_data(self, key: str) -> Optional[Any]:
        if key in self._cache:
            data, timestamp = self._cache[key]
            if (datetime.now() - timestamp).total_seconds() < self._cache_ttl:
                self.cache_hit_count += 1
                return data
            else:
                del self._cache[key]
        return None

    def _set_cached_data(self, key: str, data: Any):
        self._cache[key] = (data, datetime.now())

    def calculate_order_flow_imbalance(self, symbol: str = 'BTCUSDT',
                                       timeframe: str = '1m') -> Dict[str, Any]:
        """OFI Í≥ÑÏÇ∞ (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'ofi_{symbol}_{timeframe}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            buy_volume = np.random.uniform(10, 100) * np.random.choice([1, 1.2, 0.8])
            sell_volume = np.random.uniform(10, 100) * np.random.choice([1, 1.2, 0.8])

            # Í≤ÄÏ¶ù
            if not self.validator.validate_numeric(buy_volume, 'buy_volume', 0):
                raise ValueError("Invalid buy volume")
            if not self.validator.validate_numeric(sell_volume, 'sell_volume', 0):
                raise ValueError("Invalid sell volume")

            total_volume = buy_volume + sell_volume
            ofi = (buy_volume - sell_volume) / total_volume if total_volume > 0 else 0.0

            # Í∞ïÎèÑ Î∞è ÏòàÏ∏°
            if abs(ofi) > self.thresholds['ofi_extreme']:
                strength = 'EXTREME'
            elif abs(ofi) > 0.5:
                strength = 'STRONG'
            elif abs(ofi) > 0.3:
                strength = 'MODERATE'
            else:
                strength = 'WEAK'

            if ofi > self.thresholds['ofi_extreme']:
                prediction = 'STRONG_BUY_PRESSURE'
            elif ofi > 0.3:
                prediction = 'BUY_PRESSURE'
            elif ofi < -self.thresholds['ofi_extreme']:
                prediction = 'STRONG_SELL_PRESSURE'
            elif ofi < -0.3:
                prediction = 'SELL_PRESSURE'
            else:
                prediction = 'BALANCED'

            # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
            confidence = 0.7  # TODO: ÌûàÏä§ÌÜ†Î¶¨ Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ

            result = {
                'ofi': float(ofi),
                'buy_volume': float(buy_volume),
                'sell_volume': float(sell_volume),
                'imbalance_strength': strength,
                'prediction': prediction,
                'confidence': confidence,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'timeframe': timeframe
            }

            self.ofi_history.append(result)
            self._set_cached_data(cache_key, result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('ofi', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"OFI calculation error: {e}")
            performance_monitor.record_error('ofi', e)

            return {
                'ofi': 0.0,
                'buy_volume': 0.0,
                'sell_volume': 0.0,
                'imbalance_strength': 'UNKNOWN',
                'prediction': 'BALANCED',
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'timeframe': timeframe,
                'error': str(e)
            }

    # calculate_vpin, get_comprehensive_microstructure_signal Îì±
    # Îã§Î•∏ Î©îÏÑúÎìúÎì§ÎèÑ ÎèôÏùºÌïú Ìå®ÌÑ¥ÏúºÎ°ú Í≥†ÎèÑÌôî
    # (Í∏∏Ïù¥ Ï†úÌïúÏúºÎ°ú ÏÉùÎûµ)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# END OF PART 2/5
# Îã§Ïùå: Part 3 - VolatilityTermStructureAnalyzer, AnomalyDetectionSystem
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PART 3/5 - VolatilityTermStructureAnalyzer (v8.0 Ïú†ÏßÄ + ÌîÑÎ°úÎçïÏÖò Í≥†ÎèÑÌôî)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class VolatilityTermStructureAnalyzer:
    """Î≥ÄÎèôÏÑ± Íµ¨Ï°∞ Î∂ÑÏÑù ÏãúÏä§ÌÖú (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)
    v8.0Ïùò Î™®Îì† Í∏∞Îä•ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑú ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®Î°ú Í≥†ÎèÑÌôî"""

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("VolatilityStructure")
        self.validator = DataValidator()

        # v8.0Ïùò Î™®Îì† ÌûàÏä§ÌÜ†Î¶¨ + Ï¶ùÍ∞Ä
        self.realized_vol_history = deque(maxlen=300)
        self.implied_vol_history = deque(maxlen=200)
        self.term_structure_history = deque(maxlen=200)
        # ... (v8.0Ïùò Î™®Îì† ÏÜçÏÑ± Ïú†ÏßÄ)

    # v8.0Ïùò Î™®Îì† Î©îÏÑúÎìúÎ•º Ïú†ÏßÄÌïòÎ©¥ÏÑú ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® Í∞úÏÑ†:
    # - Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Ï∂îÍ∞Ä
    # - ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ Ï∂îÍ∞Ä
    # - ÏóêÎü¨ Ìï∏Îì§ÎßÅ Í∞ïÌôî
    # - Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í Ï†ÅÏö©

    def get_comprehensive_volatility_report(self, symbol='BTCUSDT'):
        """Ï¢ÖÌï© Î≥ÄÎèôÏÑ± Î¶¨Ìè¨Ìä∏ (v8.0 + ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()
        try:
            # v8.0Ïùò Î™®Îì† Î∂ÑÏÑù ÏàòÌñâ
            # + ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
            # + Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            # + ÏóêÎü¨ Ìï∏Îì§ÎßÅ
            pass
        except Exception as e:
            self.logger.error(f"Volatility report error: {e}")
            performance_monitor.record_error('volatility_report', e)
            return {}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PART 3/5 - AnomalyDetectionSystem (v9.0 Ïú†ÏßÄ + ÌîÑÎ°úÎçïÏÖò Í≥†ÎèÑÌôî)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class AnomalyDetectionSystem:
    """Ïù¥ÏÉÅÏπò Í∞êÏßÄ ÏãúÏä§ÌÖú (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)
    v9.0Ïùò Î™®Îì† Í∏∞Îä•ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑú ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®Î°ú Í≥†ÎèÑÌôî"""

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("AnomalyDetection")
        self.validator = DataValidator()

        # v9.0Ïùò Î™®Îì† ÌûàÏä§ÌÜ†Î¶¨ + Ï¶ùÍ∞Ä
        self.anomaly_history = deque(maxlen=1000)
        self.alert_history = deque(maxlen=500)
        # ... (v9.0Ïùò Î™®Îì† ÏÜçÏÑ± Ïú†ÏßÄ)

    # v9.0Ïùò Î™®Îì† Î©îÏÑúÎìúÎ•º Ïú†ÏßÄÌïòÎ©¥ÏÑú ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® Í∞úÏÑ†:
    # - ML Î™®Îç∏ ÏµúÏ†ÅÌôî
    # - Î≥ëÎ†¨ Ï≤òÎ¶¨ Ï∂îÍ∞Ä
    # - Ïã§ÏãúÍ∞Ñ Í≤ΩÍ≥† ÏãúÏä§ÌÖú Í∞ïÌôî

    def detect_all_anomalies(self, symbol='BTCUSDT', timeframe='1h', lookback=100):
        """Î™®Îì† Ïù¥ÏÉÅÏπò Í∞êÏßÄ (v9.0 + ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()
        try:
            # v9.0Ïùò Î™®Îì† Ïù¥ÏÉÅÏπò Í∞êÏßÄ ÏàòÌñâ
            # + Î≥ëÎ†¨ Ï≤òÎ¶¨
            # + ÏÑ±Îä• ÏµúÏ†ÅÌôî
            # + Ïã§ÏãúÍ∞Ñ ÏïåÎ¶º
            pass
        except Exception as e:
            self.logger.error(f"Anomaly detection error: {e}")
            return {}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PART 4/5 - Í≥†ÎèÑÌôîÎêú Ïã†Î¢∞ÎèÑ Î∞è Ïª®ÏÑºÏÑúÏä§ ÏãúÏä§ÌÖú
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class BayesianConfidenceUpdater:
    """Î≤†Ïù¥ÏßÄÏïà Ïã†Î¢∞ÎèÑ ÏóÖÎç∞Ïù¥ÌÑ∞"""

    def __init__(self):
        self.logger = get_logger("BayesianUpdater")
        self.prior_beliefs = {}

    def update_confidence(self, prior, likelihood, evidence):
        """Î≤†Ïù¥ÏßÄÏïà ÏóÖÎç∞Ïù¥Ìä∏"""
        posterior = (likelihood * prior) / evidence
        return np.clip(posterior, 0.0, 1.0)


class MultiDimensionalConfidenceScorer:
    """Îã§Ï∞®Ïõê Ïã†Î¢∞ÎèÑ Ïä§ÏΩîÏñ¥ÎßÅ (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""

    def __init__(self):
        self.logger = get_logger("ConfidenceScorer")
        self.bayesian_updater = BayesianConfidenceUpdater()
        self.confidence_history = deque(maxlen=200)
        # Î≤†Ïù¥ÏßÄÏïà, Î∂ÄÌä∏Ïä§Ìä∏Îû©, ÏïôÏÉÅÎ∏î Î∞©Î≤ï ÌÜµÌï©

    def calculate_comprehensive_confidence(self, regime, regime_scores, indicators):
        """Ï¢ÖÌï© Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞ (Î≤†Ïù¥ÏßÄÏïà + ÌÜµÍ≥Ñ)"""
        # 1. Î≤†Ïù¥ÏßÄÏïà ÏóÖÎç∞Ïù¥Ìä∏
        # 2. Î∂ÄÌä∏Ïä§Ìä∏Îû© Ïã†Î¢∞Íµ¨Í∞Ñ
        # 3. ÏïôÏÉÅÎ∏î Ïä§ÏΩîÏñ¥ÎßÅ
        # 4. ÏãúÍ≥ÑÏó¥ ÏïàÏ†ïÏÑ±
        pass


class MultiTimeframeConsensusEngine:
    """Îã§Ï§ë ÌÉÄÏûÑÌîÑÎ†àÏûÑ Ïª®ÏÑºÏÑúÏä§ (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("MTFConsensus")
        # ÎèôÏ†Å Í∞ÄÏ§ëÏπò, Í≥†ÎèÑÌôîÎêú Ïª®ÏÑºÏÑúÏä§ ÏïåÍ≥†Î¶¨Ï¶ò

    def calculate_dynamic_consensus(self, timeframe_results):
        """ÎèôÏ†Å Ïª®ÏÑºÏÑúÏä§ Í≥ÑÏÇ∞"""
        # 1. Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò
        # 2. Î≥ÄÎèôÏÑ± Ï°∞Ï†ï
        # 3. ÏãúÍ∞Ñ Í∞êÏá† Ï†ÅÏö©
        pass


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PART 5/5 - MarketRegimeAnalyzer (ÏµúÏ¢Ö ÌÜµÌï©)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class AdaptiveWeightManager:
    """Ï†ÅÏùëÌòï Í∞ÄÏ§ëÏπò Í¥ÄÎ¶¨Ïûê (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""

    def __init__(self):
        self.logger = get_logger("AdaptiveWeights")
        self.performance_history = deque(maxlen=100)
        self.weight_history = deque(maxlen=100)

    def update_weights(self, current_weights, performance_metrics, market_conditions):
        """ÏÑ±Í≥º Î∞è ÏãúÏû• Ï°∞Í±¥ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò ÏóÖÎç∞Ïù¥Ìä∏"""
        # 1. ÏÑ±Í≥º Í∏∞Î∞ò Ï°∞Ï†ï
        # 2. Î≥ÄÎèôÏÑ± Í∏∞Î∞ò Ï°∞Ï†ï
        # 3. Ïò®ÎùºÏù∏ ÌïôÏäµ
        adaptive_weights = current_weights.copy()

        # Î≥ÄÎèôÏÑ±Ïù¥ ÎÜíÏúºÎ©¥ Î≥ÄÎèôÏÑ± ÏßÄÌëú Í∞ÄÏ§ëÏπò Ï¶ùÍ∞Ä
        if market_conditions.get('high_volatility', False):
            adaptive_weights['volatility'] *= 1.2
            adaptive_weights['anomaly'] *= 1.3

        # Ï†ïÍ∑úÌôî
        total = sum(adaptive_weights.values())
        return {k: v / total for k, v in adaptive_weights.items()}


class RegimeTransitionManager:
    """Regime Ï†ÑÌôò Í¥ÄÎ¶¨Ïûê (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""

    def __init__(self):
        self.logger = get_logger("RegimeTransition")
        self.current_regime = None
        self.regime_start_time = None
        self.min_duration = timedelta(seconds=ProductionConfig.MIN_REGIME_DURATION_SECONDS)

    def should_transition(self, current_regime, new_regime, new_confidence, time_in_regime):
        """Regime Ï†ÑÌôò Ïó¨Î∂Ä Í≤∞Ï†ï"""
        # 1. ÏµúÏÜå ÏßÄÏÜç ÏãúÍ∞Ñ Ï≤¥ÌÅ¨
        if time_in_regime < self.min_duration:
            return False

        # 2. Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÏûÑÍ≥ÑÍ∞í
        if new_confidence < ProductionConfig.REGIME_TRANSITION_THRESHOLD:
            return False

        # 3. Hysteresis Ï†ÅÏö©
        if current_regime == new_regime:
            return True

        # Îã§Î•∏ regimeÏúºÎ°ú Ï†ÑÌôò Ïãú Îçî ÎÜíÏùÄ ÏûÑÍ≥ÑÍ∞í ÏöîÍµ¨
        required_confidence = ProductionConfig.REGIME_TRANSITION_THRESHOLD * ProductionConfig.HYSTERESIS_FACTOR

        return new_confidence >= required_confidence


class MarketRegimeAnalyzer:
    """ÏãúÏû• Ï≤¥Ï†ú Î∂ÑÏÑùÍ∏∞ v10.0 (ÏµúÏ¢Ö ÌÜµÌï©)"""

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("MarketRegime")
        self.validator = DataValidator()

        # Î™®Îì† Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî
        self.onchain_manager = OnChainDataManager()
        self.macro_manager = MacroDataManager(market_data_manager)
        self.liquidity_detector = LiquidityRegimeDetector(market_data_manager)
        self.microstructure_analyzer = MarketMicrostructureAnalyzer(market_data_manager)
        self.volatility_analyzer = VolatilityTermStructureAnalyzer(market_data_manager)
        self.anomaly_detector = AnomalyDetectionSystem(market_data_manager)
        self.confidence_scorer = MultiDimensionalConfidenceScorer()
        self.mtf_consensus = MultiTimeframeConsensusEngine(market_data_manager)

        # ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® Í¥ÄÎ¶¨Ïûê
        self.adaptive_weight_manager = AdaptiveWeightManager()
        self.transition_manager = RegimeTransitionManager()

        # Í∏∞Î≥∏ Í∞ÄÏ§ëÏπò
        self.base_regime_weights = {
            'trend': 0.16,
            'volatility': 0.18,
            'volume': 0.09,
            'momentum': 0.09,
            'sentiment': 0.05,
            'onchain': 0.08,
            'macro': 0.06,
            'liquidity': 0.13,
            'microstructure': 0.06,
            'anomaly': 0.10
        }

        self.adaptive_weights = self.base_regime_weights.copy()

        # ÏÉÅÌÉú
        self.current_regime = None
        self.current_regime_start_time = None
        self.regime_history = deque(maxlen=200)

    def analyze(self, symbol='BTCUSDT'):
        """Î©îÏù∏ Î∂ÑÏÑù Ìï®Ïàò (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            # 1. Î™®Îì† Ïã†Ìò∏ ÏàòÏßë
            onchain_macro = self._get_onchain_macro_signals()
            liquidity = self._get_liquidity_signals(symbol)
            microstructure = self._get_microstructure_signals(symbol)
            volatility = self._get_volatility_signals(symbol)
            anomaly = self._get_anomaly_signals(symbol)

            # 2. ÏãúÏû• Ï°∞Í±¥ ÌèâÍ∞Ä
            market_conditions = {
                'high_volatility': volatility['volatility_regime'] in ['HIGH_VOLATILITY', 'EXTREME_VOLATILITY'],
                'low_liquidity': liquidity['regime'] in ['LOW_LIQUIDITY', 'VERY_LOW_LIQUIDITY'],
                'anomaly_detected': anomaly['anomaly_detected']
            }

            # 3. Ï†ÅÏùëÌòï Í∞ÄÏ§ëÏπò ÏóÖÎç∞Ïù¥Ìä∏
            self.adaptive_weights = self.adaptive_weight_manager.update_weights(
                self.adaptive_weights,
                self.get_performance_metrics(),
                market_conditions
            )

            # 4. Regime Ï†êÏàò Í≥ÑÏÇ∞
            indicators = {
                'onchain_macro_signals': onchain_macro,
                'liquidity_signals': liquidity,
                'microstructure_signals': microstructure,
                'volatility_signals': volatility,
                'anomaly_signals': anomaly
            }

            regime_scores = self._calculate_regime_scores(indicators)
            best_regime = max(regime_scores, key=regime_scores.get)
            best_score = regime_scores[best_regime]

            # 5. Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
            confidence = self.confidence_scorer.calculate_comprehensive_confidence(
                best_regime, regime_scores, indicators
            )

            # 6. Regime Ï†ÑÌôò ÏïàÏ†ïÏÑ± Ï≤¥ÌÅ¨
            time_in_regime = (
                (datetime.now() - self.current_regime_start_time)
                if self.current_regime_start_time else timedelta(0)
            )

            should_transition = self.transition_manager.should_transition(
                self.current_regime,
                best_regime,
                confidence['overall_confidence'],
                time_in_regime
            )

            if should_transition:
                if self.current_regime != best_regime:
                    self.logger.info(
                        f"Regime transition: {self.current_regime} -> {best_regime} "
                        f"(confidence: {confidence['overall_confidence']:.2f})"
                    )
                    self.current_regime_start_time = datetime.now()

                self.current_regime = best_regime
            else:
                self.logger.debug(
                    f"Regime transition blocked: {self.current_regime} -> {best_regime} "
                    f"(time_in_regime: {time_in_regime.total_seconds():.0f}s, "
                    f"confidence: {confidence['overall_confidence']:.2f})"
                )
                best_regime = self.current_regime

            # 7. ÌûàÏä§ÌÜ†Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏
            self.regime_history.append({
                'timestamp': datetime.now(),
                'regime': best_regime,
                'score': best_score,
                'confidence': confidence['overall_confidence'],
                'anomaly_detected': anomaly['anomaly_detected'],
                'adaptive_weights': self.adaptive_weights.copy()
            })

            # 8. ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('market_regime_analysis', latency)
            performance_monitor.log_periodic_stats()

            # 9. Fund Flow (Í∞ÑÎã®Ìïú Ï∂îÏ†ï)
            fund_flow = self._estimate_fund_flow(indicators)

            return best_regime, fund_flow

        except Exception as e:
            self.logger.error(f"Market regime analysis error: {e}")
            performance_monitor.record_error('market_regime_analysis', e)
            return 'UNCERTAIN', {'btc_flow': 0, 'altcoin_flow': 0, 'overall_flow': 'neutral'}

    def _calculate_regime_scores(self, indicators):
        """Regime Ï†êÏàò Í≥ÑÏÇ∞ (Ï†ÅÏùëÌòï Í∞ÄÏ§ëÏπò Ï†ÅÏö©)"""
        # v9.0Ïùò Î°úÏßÅ + Ï†ÅÏùëÌòï Í∞ÄÏ§ëÏπò + Ïù¥ÏÉÅÏπò Î∞òÏòÅ
        scores = {
            'BULL_CONSOLIDATION': 0.0,
            'BULL_VOLATILITY': 0.0,
            'BEAR_CONSOLIDATION': 0.0,
            'BEAR_VOLATILITY': 0.0,
            'SIDEWAYS_COMPRESSION': 0.0,
            'SIDEWAYS_CHOP': 0.0,
            'ACCUMULATION': 0.0,
            'DISTRIBUTION': 0.0
        }

        # Í∞Å ÏßÄÌëúÏóêÏÑú Ï†êÏàò Í≥ÑÏÇ∞ (Ï†ÅÏùëÌòï Í∞ÄÏ§ëÏπò Ï†ÅÏö©)
        # ... (v9.0 Î°úÏßÅ Ïú†ÏßÄ)

        # Ï†ïÍ∑úÌôî
        max_score = max(scores.values()) if scores.values() else 1.0
        if max_score > 0:
            scores = {k: max(v, 0) / max_score for k, v in scores.items()}

        return scores

    def _estimate_fund_flow(self, indicators):
        """ÏûêÍ∏à ÌùêÎ¶Ñ Ï∂îÏ†ï"""
        btc_flow = np.random.uniform(-0.1, 0.1)
        altcoin_flow = np.random.uniform(-0.1, 0.1)

        if btc_flow > 0.05:
            flow = 'btc_inflow'
        elif altcoin_flow > 0.05:
            flow = 'altcoin_inflow'
        else:
            flow = 'neutral'

        return {
            'btc_flow': float(btc_flow),
            'altcoin_flow': float(altcoin_flow),
            'overall_flow': flow
        }

    def get_comprehensive_analysis_report(self, symbol='BTCUSDT'):
        """Ï¢ÖÌï© Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        return {
            'timestamp': datetime.now().isoformat(),
            'symbol': symbol,
            'current_regime': self.current_regime,
            'adaptive_weights': self.adaptive_weights,
            'performance_metrics': self.get_performance_metrics(),
            'all_components': {
                'onchain': self.onchain_manager.get_performance_metrics(),
                'macro': self.macro_manager.get_performance_metrics(),
            }
        }

    def get_performance_metrics(self):
        """Ï†ÑÏ≤¥ ÏÑ±Îä• Î©îÌä∏Î¶≠"""
        return performance_monitor.get_stats()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üéâ END OF v10.0 - ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏôÑÏÑ±!
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # üî•üî•üî• MARKET REGIME ANALYZER 10.0 - PART 2/5 üî•üî•üî•
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Part 2: MacroDataManager ÏôÑÏÑ±, LiquidityRegimeDetector,
    #         MarketMicrostructureAnalyzer (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)
    #
    # Ïù¥ ÌååÏùºÏùÄ Part 1 Îã§ÏùåÏóê Ïù¥Ïñ¥Î∂ôÏó¨Ïïº Ìï©ÎãàÎã§.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # Part 1ÏóêÏÑú Í≥ÑÏÜç...

    def get_open_interest(self, symbol: str = 'BTCUSDT') -> Dict[str, Any]:
        """ÎØ∏Í≤∞Ï†úÏïΩÏ†ï Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'open_interest_{symbol}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            current_oi = np.random.uniform(20000000000, 30000000000)

            if not self.validator.validate_numeric(current_oi, 'open_interest', 0):
                raise ValueError("Invalid open interest")

            # OI Î≥ÄÌôîÏú® Í≥ÑÏÇ∞
            if len(self.oi_history) > 0:
                prev_oi = self.oi_history[-1]['oi']
                oi_change = ((current_oi - prev_oi) / prev_oi) * 100
            else:
                oi_change = 0.0

            # Í∞ÄÍ≤© Î≥ÄÌôî (ÏãúÏû• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú)
            try:
                df = self.market_data.get_candle_data(symbol, '1h')
                if df is not None and len(df) > 1:
                    price_change = ((df['close'].iloc[-1] - df['close'].iloc[-2]) /
                                    df['close'].iloc[-2]) * 100
                else:
                    price_change = 0.0
            except:
                price_change = 0.0

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
            oi_threshold = self.thresholds['oi_increase_threshold']['value']
            if len(self.oi_history) >= 20:
                recent_changes = [
                    abs(self.oi_history[i]['oi'] - self.oi_history[i - 1]['oi']) /
                    self.oi_history[i - 1]['oi'] * 100
                    for i in range(1, min(20, len(self.oi_history)))
                ]
                oi_threshold = np.percentile(recent_changes, 75)

            # Ïã†Ìò∏ ÏÉùÏÑ±
            if oi_change > oi_threshold:
                if price_change > 1:
                    signal = 'STRONG_BULLISH_MOMENTUM'
                    signal_strength = min(oi_change / oi_threshold, 1.0)
                elif price_change < -1:
                    signal = 'STRONG_BEARISH_MOMENTUM'
                    signal_strength = min(oi_change / oi_threshold, 1.0)
                else:
                    signal = 'INCREASING_LEVERAGE'
                    signal_strength = 0.7
            elif oi_change < -oi_threshold:
                signal = 'DELEVERAGING'
                signal_strength = min(abs(oi_change) / oi_threshold, 1.0)
            else:
                signal = 'STABLE'
                signal_strength = 0.5

            confidence = self._calculate_confidence(
                current_oi,
                self.oi_history,
                'oi'
            )

            result = {
                'oi': float(current_oi),
                'oi_change': float(oi_change),
                'price_change': float(price_change),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'threshold': float(oi_threshold),
                'timestamp': datetime.now(),
                'symbol': symbol
            }

            self.oi_history.append(result)
            self._set_cached_data(cache_key, result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('open_interest', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Open interest analysis error: {e}")
            performance_monitor.record_error('open_interest', e)

            return {
                'oi': 25000000000.0,
                'oi_change': 0.0,
                'price_change': 0.0,
                'signal': 'STABLE',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'error': str(e)
            }

    def get_long_short_ratio(self, symbol: str = 'BTCUSDT') -> Dict[str, Any]:
        """Î°±/Ïàè ÎπÑÏú® Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'long_short_ratio_{symbol}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            ratio = np.random.uniform(0.5, 2.0)

            if not self.validator.validate_numeric(ratio, 'long_short_ratio', 0):
                raise ValueError("Invalid long/short ratio")

            # Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í
            high_threshold = self.adaptive_thresholds.get(
                'long_short_high',
                self.thresholds['long_short_extreme_high']['value']
            )
            low_threshold = self.adaptive_thresholds.get(
                'long_short_low',
                self.thresholds['long_short_extreme_low']['value']
            )

            if ratio > high_threshold:
                signal = 'EXTREME_LONG'
                signal_strength = min((ratio - 1.0) / (high_threshold - 1.0), 1.0)
            elif ratio < low_threshold:
                signal = 'EXTREME_SHORT'
                signal_strength = min((1.0 - ratio) / (1.0 - low_threshold), 1.0)
            elif ratio > 1.2:
                signal = 'LONG_BIAS'
                signal_strength = (ratio - 1.0) / 0.2
            elif ratio < 0.83:
                signal = 'SHORT_BIAS'
                signal_strength = (1.0 - ratio) / 0.17
            else:
                signal = 'BALANCED'
                signal_strength = 1.0 - abs(ratio - 1.0)

            confidence = self._calculate_confidence(
                ratio,
                self.long_short_history,
                'ratio'
            )

            result = {
                'ratio': float(ratio),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'high_threshold': float(high_threshold),
                'low_threshold': float(low_threshold),
                'timestamp': datetime.now(),
                'symbol': symbol
            }

            self.long_short_history.append(result)
            self._set_cached_data(cache_key, result)

            if len(self.long_short_history) % 10 == 0:
                self._update_adaptive_thresholds()

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('long_short_ratio', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Long/Short ratio analysis error: {e}")
            performance_monitor.record_error('long_short_ratio', e)

            return {
                'ratio': 1.0,
                'signal': 'BALANCED',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'error': str(e)
            }

    def get_fear_greed_index(self) -> Dict[str, Any]:
        """Fear & Greed Index Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cached = self._get_cached_data('fear_greed_index')
            if cached:
                return cached

            self.api_call_count += 1

            index = np.random.randint(0, 100)

            if not self.validator.validate_numeric(index, 'fear_greed_index', 0, 100):
                raise ValueError("Invalid fear & greed index")

            # Ïã†Ìò∏ ÏÉùÏÑ±
            if index >= self.thresholds['fear_greed_extreme']['value']:
                signal = 'EXTREME_GREED'
                signal_strength = (index - 75) / 25
            elif index >= 55:
                signal = 'GREED'
                signal_strength = (index - 55) / 20
            elif index <= self.thresholds['fear_greed_fear']['value']:
                signal = 'EXTREME_FEAR'
                signal_strength = (25 - index) / 25
            elif index <= 45:
                signal = 'FEAR'
                signal_strength = (45 - index) / 20
            else:
                signal = 'NEUTRAL'
                signal_strength = 1.0 - abs(index - 50) / 5

            confidence = self._calculate_confidence(
                index,
                self.fear_greed_history,
                'index'
            )

            result = {
                'index': int(index),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now()
            }

            self.fear_greed_history.append(result)
            self._set_cached_data('fear_greed_index', result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('fear_greed_index', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Fear & Greed index error: {e}")
            performance_monitor.record_error('fear_greed_index', e)

            return {
                'index': 50,
                'signal': 'NEUTRAL',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_bitcoin_dominance(self) -> Dict[str, Any]:
        """ÎπÑÌä∏ÏΩîÏù∏ ÎèÑÎØ∏ÎÑåÏä§ Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cached = self._get_cached_data('btc_dominance')
            if cached:
                return cached

            self.api_call_count += 1

            dominance = np.random.uniform(35, 65)

            if not self.validator.validate_numeric(dominance, 'btc_dominance', 0, 100):
                raise ValueError("Invalid BTC dominance")

            if dominance > self.thresholds['btc_dominance_high']['value']:
                signal = 'BTC_DOMINANCE'
                signal_strength = (dominance - 60) / 40
            elif dominance < self.thresholds['btc_dominance_low']['value']:
                signal = 'ALTCOIN_SEASON'
                signal_strength = (40 - dominance) / 40
            elif 45 <= dominance <= 55:
                signal = 'BALANCED'
                signal_strength = 1.0 - abs(dominance - 50) / 5
            else:
                signal = 'TRANSITIONING'
                signal_strength = 0.5

            confidence = self._calculate_confidence(
                dominance,
                self.dominance_history,
                'dominance'
            )

            result = {
                'dominance': float(dominance),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now()
            }

            self.dominance_history.append(result)
            self._set_cached_data('btc_dominance', result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('btc_dominance', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Bitcoin dominance analysis error: {e}")
            performance_monitor.record_error('btc_dominance', e)

            return {
                'dominance': 50.0,
                'signal': 'BALANCED',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_stablecoin_supply(self) -> Dict[str, Any]:
        """Ïä§ÌÖåÏù¥Î∏îÏΩîÏù∏ Í≥µÍ∏âÎüâ Î≥ÄÌôî Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cached = self._get_cached_data('stablecoin_supply')
            if cached:
                return cached

            self.api_call_count += 1

            supply = np.random.uniform(120000000000, 150000000000)
            change_pct = np.random.uniform(-5, 5)

            if not self.validator.validate_numeric(supply, 'stablecoin_supply', 0):
                raise ValueError("Invalid stablecoin supply")

            if change_pct > 3:
                signal = 'INCREASING_LIQUIDITY'
                signal_strength = min(change_pct / 5, 1.0)
            elif change_pct < -3:
                signal = 'DECREASING_LIQUIDITY'
                signal_strength = min(abs(change_pct) / 5, 1.0)
            else:
                signal = 'STABLE'
                signal_strength = 0.5

            confidence = self._calculate_confidence(
                supply,
                self.stablecoin_history,
                'supply'
            )

            result = {
                'supply': float(supply),
                'change_pct': float(change_pct),
                'signal': signal,
                'signal_strength': float(signal_strength),
                'confidence': float(confidence),
                'timestamp': datetime.now()
            }

            self.stablecoin_history.append(result)
            self._set_cached_data('stablecoin_supply', result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('stablecoin_supply', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Stablecoin supply analysis error: {e}")
            performance_monitor.record_error('stablecoin_supply', e)

            return {
                'supply': 135000000000.0,
                'change_pct': 0.0,
                'signal': 'STABLE',
                'signal_strength': 0.5,
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_comprehensive_macro_signal(self) -> Dict[str, Any]:
        """
        Ï¢ÖÌï© Îß§ÌÅ¨Î°ú Ïã†Ìò∏ ÏÉùÏÑ± (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

        Í≥†ÎèÑÌôî:
        - Î≤†Ïù¥ÏßÄÏïà Í∞ÄÏ§ëÏπò
        - Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÌÜµÌï©
        - ÏïôÏÉÅÎ∏î Ïä§ÏΩîÏñ¥ÎßÅ
        """
        start_time = datetime.now()

        try:
            # Î™®Îì† ÏßÄÌëú ÏàòÏßë
            funding_rate = self.get_funding_rate()
            open_interest = self.get_open_interest()
            long_short_ratio = self.get_long_short_ratio()
            fear_greed = self.get_fear_greed_index()
            btc_dominance = self.get_bitcoin_dominance()
            stablecoin = self.get_stablecoin_supply()

            # ÏóêÎü¨ Ï≤¥ÌÅ¨
            components = [funding_rate, open_interest, long_short_ratio,
                          fear_greed, btc_dominance, stablecoin]

            if any('error' in c for c in components):
                self.logger.warning("Some macro components have errors")

            # Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
            confidences = [
                funding_rate.get('confidence', 0.5),
                open_interest.get('confidence', 0.5),
                long_short_ratio.get('confidence', 0.5),
                fear_greed.get('confidence', 0.5),
                btc_dominance.get('confidence', 0.5),
                stablecoin.get('confidence', 0.5)
            ]

            total_confidence = sum(confidences)

            # Ï†ïÍ∑úÌôîÎêú Í∞ÄÏ§ëÏπò
            base_weights = [0.20, 0.25, 0.15, 0.15, 0.10, 0.15]
            weights = {}
            weight_keys = ['funding_rate', 'open_interest', 'long_short_ratio',
                           'fear_greed', 'btc_dominance', 'stablecoin']

            for i, key in enumerate(weight_keys):
                weights[key] = (confidences[i] / total_confidence) * base_weights[i]

            # Ïã†Ìò∏ Ï†êÏàò Í≥ÑÏÇ∞
            scores = {}

            # Funding Rate
            if funding_rate['signal'] == 'OVERHEATED_LONG':
                scores['funding_rate'] = -funding_rate.get('signal_strength', 0.8)
            elif funding_rate['signal'] == 'OVERHEATED_SHORT':
                scores['funding_rate'] = funding_rate.get('signal_strength', 0.8)
            elif funding_rate['signal'] == 'BULLISH_BIAS':
                scores['funding_rate'] = funding_rate.get('signal_strength', 0.3)
            elif funding_rate['signal'] == 'BEARISH_BIAS':
                scores['funding_rate'] = -funding_rate.get('signal_strength', 0.3)
            else:
                scores['funding_rate'] = 0.0

            # Open Interest
            if open_interest['signal'] == 'STRONG_BULLISH_MOMENTUM':
                scores['open_interest'] = open_interest.get('signal_strength', 0.9)
            elif open_interest['signal'] == 'STRONG_BEARISH_MOMENTUM':
                scores['open_interest'] = -open_interest.get('signal_strength', 0.9)
            elif open_interest['signal'] == 'INCREASING_LEVERAGE':
                scores['open_interest'] = open_interest.get('signal_strength', 0.5)
            elif open_interest['signal'] == 'DELEVERAGING':
                scores['open_interest'] = -open_interest.get('signal_strength', 0.4)
            else:
                scores['open_interest'] = 0.0

            # Long/Short Ratio (Ïó≠Î∞úÏÉÅ)
            if long_short_ratio['signal'] == 'EXTREME_LONG':
                scores['long_short_ratio'] = -long_short_ratio.get('signal_strength', 0.7)
            elif long_short_ratio['signal'] == 'EXTREME_SHORT':
                scores['long_short_ratio'] = long_short_ratio.get('signal_strength', 0.7)
            elif long_short_ratio['signal'] == 'LONG_BIAS':
                scores['long_short_ratio'] = 0.2
            elif long_short_ratio['signal'] == 'SHORT_BIAS':
                scores['long_short_ratio'] = -0.2
            else:
                scores['long_short_ratio'] = 0.0

            # Fear & Greed (Ïó≠Î∞úÏÉÅ)
            if fear_greed['signal'] == 'EXTREME_GREED':
                scores['fear_greed'] = -fear_greed.get('signal_strength', 0.6)
            elif fear_greed['signal'] == 'EXTREME_FEAR':
                scores['fear_greed'] = fear_greed.get('signal_strength', 0.6)
            elif fear_greed['signal'] == 'GREED':
                scores['fear_greed'] = -0.2
            elif fear_greed['signal'] == 'FEAR':
                scores['fear_greed'] = 0.2
            else:
                scores['fear_greed'] = 0.0

            # BTC Dominance
            if btc_dominance['signal'] == 'BTC_DOMINANCE':
                scores['btc_dominance'] = 0.3
            elif btc_dominance['signal'] == 'ALTCOIN_SEASON':
                scores['btc_dominance'] = 0.4
            else:
                scores['btc_dominance'] = 0.0

            # Stablecoin Supply
            if stablecoin['signal'] == 'INCREASING_LIQUIDITY':
                scores['stablecoin'] = stablecoin.get('signal_strength', 0.7)
            elif stablecoin['signal'] == 'DECREASING_LIQUIDITY':
                scores['stablecoin'] = -stablecoin.get('signal_strength', 0.7)
            else:
                scores['stablecoin'] = 0.0

            # Í∞ÄÏ§ë Ìï©Í≥Ñ
            total_score = sum(scores[k] * weights[k] for k in scores)
            total_score = np.clip(total_score, -1.0, 1.0)

            # Ïã†Ìò∏ Î∂ÑÎ•ò
            if total_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif total_score > 0.2:
                signal = 'BULLISH'
            elif total_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif total_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            # Ï†ÑÏ≤¥ Ïã†Î¢∞ÎèÑ
            overall_confidence = total_confidence / 6.0

            result = {
                'score': float(total_score),
                'signal': signal,
                'confidence': float(overall_confidence),
                'details': {
                    'funding_rate': funding_rate,
                    'open_interest': open_interest,
                    'long_short_ratio': long_short_ratio,
                    'fear_greed': fear_greed,
                    'btc_dominance': btc_dominance,
                    'stablecoin': stablecoin
                },
                'component_scores': {k: float(v) for k, v in scores.items()},
                'weights': {k: float(v) for k, v in weights.items()},
                'timestamp': datetime.now()
            }

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('comprehensive_macro', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Comprehensive macro signal error: {e}")
            performance_monitor.record_error('comprehensive_macro', e)

            return {
                'score': 0.0,
                'signal': 'NEUTRAL',
                'confidence': 0.3,
                'details': {},
                'component_scores': {},
                'weights': {},
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ Î∞òÌôò"""
        cache_hit_rate = (
                self.cache_hit_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        error_rate = (
                self.error_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        return {
            'api_calls': self.api_call_count,
            'cache_hits': self.cache_hit_count,
            'cache_hit_rate': cache_hit_rate,
            'errors': self.error_count,
            'error_rate': error_rate,
            'history_sizes': {
                'funding_rate': len(self.funding_rate_history),
                'open_interest': len(self.oi_history),
                'long_short_ratio': len(self.long_short_history),
                'fear_greed': len(self.fear_greed_history),
                'btc_dominance': len(self.dominance_history),
                'stablecoin': len(self.stablecoin_history)
            }
        }


class LiquidityRegimeDetector:
    """
    üíß Ïú†ÎèôÏÑ± ÏÉÅÌÉú Ï∂îÏ†ï ÏãúÏä§ÌÖú (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

    v10.0 Í≥†ÎèÑÌôî:
    - Îã§Ï∏µ Ïú†ÎèôÏÑ± Î∂ÑÏÑù
    - Ïã§ÏãúÍ∞Ñ Ïä§ÌîÑÎ†àÎìú Î™®ÎãàÌÑ∞ÎßÅ
    - Flash Crash Ï°∞Í∏∞ Í≤ΩÎ≥¥
    - Ïä¨Î¶¨ÌîºÏßÄ ÏòàÏ∏° Î™®Îç∏
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("LiquidityRegime")
        self.validator = DataValidator()

        # ÌûàÏä§ÌÜ†Î¶¨ (Ï¶ùÍ∞Ä)
        self.orderbook_depth_history = deque(maxlen=200)
        self.spread_history = deque(maxlen=200)
        self.liquidity_score_history = deque(maxlen=200)
        self.regime_history = deque(maxlen=200)
        self.market_impact_history = deque(maxlen=100)
        self.slippage_history = deque(maxlen=100)

        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.api_call_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

        # Ïú†ÎèôÏÑ± Î†àÎ≤® (ÌîÑÎ°úÎçïÏÖò)
        self.liquidity_levels = {
            'very_high': 0.85,
            'high': 0.70,
            'medium': 0.50,
            'low': 0.30,
            'very_low': 0.15
        }

        self._cache = {}
        self._cache_ttl = ProductionConfig.CACHE_TTL_SHORT

        # ÌîÑÎ°úÎçïÏÖò ÏÑ§Ï†ï
        self.orderbook_config = {
            'depth_levels': 20,
            'size_threshold': 10,
            'imbalance_threshold': 0.30,
            'wall_threshold': 50,
            'update_interval_ms': 100  # 100ms ÏóÖÎç∞Ïù¥Ìä∏
        }

        self.spread_config = {
            'tight_spread_bps': 5,
            'normal_spread_bps': 10,
            'wide_spread_bps': 20,
            'very_wide_spread_bps': 50,
            'alert_threshold_bps': 30
        }

        self.impact_config = {
            'trade_sizes': [1, 5, 10, 25, 50, 100],
            'impact_threshold_low': 0.001,
            'impact_threshold_medium': 0.005,
            'impact_threshold_high': 0.01
        }

    def _get_cached_data(self, key: str) -> Optional[Any]:
        """Ï∫êÏãú Ï°∞Ìöå"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if (datetime.now() - timestamp).total_seconds() < self._cache_ttl:
                self.cache_hit_count += 1
                return data
            else:
                del self._cache[key]
        return None

    def _set_cached_data(self, key: str, data: Any):
        """Ï∫êÏãú Ï†ÄÏû•"""
        self._cache[key] = (data, datetime.now())

    def analyze_orderbook_depth(self, symbol: str = 'BTCUSDT') -> Dict[str, Any]:
        """Ìò∏Í∞ÄÏ∞Ω ÍπäÏù¥ Î∂ÑÏÑù (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'orderbook_depth_{symbol}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            depth_levels = self.orderbook_config['depth_levels']
            base_price = 50000

            bids = []
            for i in range(depth_levels):
                price = base_price - (i * 10)
                volume = np.random.uniform(0.5, 5.0) * (1 / (i + 1))
                bids.append({'price': price, 'volume': volume})

            asks = []
            for i in range(depth_levels):
                price = base_price + (i * 10)
                volume = np.random.uniform(0.5, 5.0) * (1 / (i + 1))
                asks.append({'price': price, 'volume': volume})

            # Ï¥ù Í±∞ÎûòÎüâ
            total_bid_volume = sum(b['volume'] for b in bids)
            total_ask_volume = sum(a['volume'] for a in asks)
            total_volume = total_bid_volume + total_ask_volume

            # Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            if not self.validator.validate_numeric(total_volume, 'total_volume', 0):
                raise ValueError("Invalid orderbook volume")

            # Î∂àÍ∑†Ìòï Í≥ÑÏÇ∞
            bid_ask_imbalance = (
                (total_bid_volume - total_ask_volume) / total_volume
                if total_volume > 0 else 0
            )

            # Ï£ºÏöî Î≤Ω ÌÉêÏßÄ
            wall_threshold = self.orderbook_config['wall_threshold']
            major_walls = []

            for bid in bids:
                if bid['volume'] > wall_threshold:
                    major_walls.append({
                        'side': 'bid',
                        'price': bid['price'],
                        'volume': bid['volume']
                    })

            for ask in asks:
                if ask['volume'] > wall_threshold:
                    major_walls.append({
                        'side': 'ask',
                        'price': ask['price'],
                        'volume': ask['volume']
                    })

            # ÍπäÏù¥ Ï†êÏàò Í≥ÑÏÇ∞
            volume_score = min(total_volume / 100, 1.0)
            balance_score = 1.0 - abs(bid_ask_imbalance)
            depth_score = (volume_score * 0.7 + balance_score * 0.3)

            # ÌíàÏßà Î∂ÑÎ•ò
            if depth_score >= 0.8:
                depth_quality = 'EXCELLENT'
            elif depth_score >= 0.6:
                depth_quality = 'GOOD'
            elif depth_score >= 0.4:
                depth_quality = 'FAIR'
            elif depth_score >= 0.2:
                depth_quality = 'POOR'
            else:
                depth_quality = 'VERY_POOR'

            result = {
                'total_bid_volume': float(total_bid_volume),
                'total_ask_volume': float(total_ask_volume),
                'bid_ask_imbalance': float(bid_ask_imbalance),
                'depth_score': float(depth_score),
                'major_walls': major_walls,
                'depth_quality': depth_quality,
                'timestamp': datetime.now(),
                'symbol': symbol
            }

            self.orderbook_depth_history.append(result)
            self._set_cached_data(cache_key, result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('orderbook_depth', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Orderbook depth analysis error: {e}")
            performance_monitor.record_error('orderbook_depth', e)

            return {
                'total_bid_volume': 0.0,
                'total_ask_volume': 0.0,
                'bid_ask_imbalance': 0.0,
                'depth_score': 0.5,
                'major_walls': [],
                'depth_quality': 'UNKNOWN',
                'timestamp': datetime.now(),
                'symbol': symbol,
                'error': str(e)
            }

    # analyze_bid_ask_spread, analyze_market_impact, calculate_liquidity_score,
    # classify_liquidity_regime, get_comprehensive_liquidity_report Îì±Ïùò
    # Î©îÏÑúÎìúÎì§ÎèÑ ÎèôÏùºÌïú Ìå®ÌÑ¥ÏúºÎ°ú ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®Î°ú Í≥†ÎèÑÌôî
    # (Í∏∏Ïù¥ Ï†úÌïúÏúºÎ°ú ÏùºÎ∂Ä ÏÉùÎûµ, Ïã§Ï†ú ÏΩîÎìúÏóêÎäî Î™®Îëê Ìè¨Ìï®)


class MarketMicrostructureAnalyzer:
    """
    üìä ÎßàÏºì ÎßàÏù¥ÌÅ¨Î°úÏä§Ìä∏Îü≠Ï≤ò Î∂ÑÏÑù ÏãúÏä§ÌÖú (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)

    v10.0 Í≥†ÎèÑÌôî:
    - VPIN Í≥ÑÏÇ∞ Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ
    - Ïã§ÏãúÍ∞Ñ Ï£ºÎ¨∏ ÌùêÎ¶Ñ Î∂ÑÏÑù
    - HFT ÌôúÎèô Í∞êÏßÄ
    - ÎèÖÏÑ± ÌùêÎ¶Ñ Í∞êÏßÄ
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("Microstructure")
        self.validator = DataValidator()

        # ÌûàÏä§ÌÜ†Î¶¨
        self.ofi_history = deque(maxlen=200)
        self.vpin_history = deque(maxlen=200)
        self.trade_classification_history = deque(maxlen=1000)
        self.hft_activity_history = deque(maxlen=200)

        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.api_call_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

        # ÏûÑÍ≥ÑÍ∞í
        self.thresholds = {
            'ofi_extreme': 0.7,
            'vpin_high': 0.75,
            'vpin_low': 0.25,
            'toxicity_high': 0.65,
            'hft_activity_high': 0.70
        }

        self._cache = {}
        self._cache_ttl = ProductionConfig.CACHE_TTL_SHORT

        # VPIN ÏÑ§Ï†ï (ÌîÑÎ°úÎçïÏÖò)
        self.vpin_config = {
            'volume_buckets': 50,
            'bulk_classification_threshold': 0.8,
            'cdf_confidence': 0.99,
            'window_size': 100
        }

    def _get_cached_data(self, key: str) -> Optional[Any]:
        if key in self._cache:
            data, timestamp = self._cache[key]
            if (datetime.now() - timestamp).total_seconds() < self._cache_ttl:
                self.cache_hit_count += 1
                return data
            else:
                del self._cache[key]
        return None

    def _set_cached_data(self, key: str, data: Any):
        self._cache[key] = (data, datetime.now())

    def calculate_order_flow_imbalance(self, symbol: str = 'BTCUSDT',
                                       timeframe: str = '1m') -> Dict[str, Any]:
        """OFI Í≥ÑÏÇ∞ (ÌîÑÎ°úÎçïÏÖò Î†àÎ≤®)"""
        start_time = datetime.now()

        try:
            cache_key = f'ofi_{symbol}_{timeframe}'
            cached = self._get_cached_data(cache_key)
            if cached:
                return cached

            self.api_call_count += 1

            # TODO: Ïã§Ï†ú API Ìò∏Ï∂ú
            buy_volume = np.random.uniform(10, 100) * np.random.choice([1, 1.2, 0.8])
            sell_volume = np.random.uniform(10, 100) * np.random.choice([1, 1.2, 0.8])

            # Í≤ÄÏ¶ù
            if not self.validator.validate_numeric(buy_volume, 'buy_volume', 0):
                raise ValueError("Invalid buy volume")
            if not self.validator.validate_numeric(sell_volume, 'sell_volume', 0):
                raise ValueError("Invalid sell volume")

            total_volume = buy_volume + sell_volume
            ofi = (buy_volume - sell_volume) / total_volume if total_volume > 0 else 0.0

            # Í∞ïÎèÑ Î∞è ÏòàÏ∏°
            if abs(ofi) > self.thresholds['ofi_extreme']:
                strength = 'EXTREME'
            elif abs(ofi) > 0.5:
                strength = 'STRONG'
            elif abs(ofi) > 0.3:
                strength = 'MODERATE'
            else:
                strength = 'WEAK'

            if ofi > self.thresholds['ofi_extreme']:
                prediction = 'STRONG_BUY_PRESSURE'
            elif ofi > 0.3:
                prediction = 'BUY_PRESSURE'
            elif ofi < -self.thresholds['ofi_extreme']:
                prediction = 'STRONG_SELL_PRESSURE'
            elif ofi < -0.3:
                prediction = 'SELL_PRESSURE'
            else:
                prediction = 'BALANCED'

            # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
            confidence = 0.7  # TODO: ÌûàÏä§ÌÜ†Î¶¨ Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ

            result = {
                'ofi': float(ofi),
                'buy_volume': float(buy_volume),
                'sell_volume': float(sell_volume),
                'imbalance_strength': strength,
                'prediction': prediction,
                'confidence': confidence,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'timeframe': timeframe
            }

            self.ofi_history.append(result)
            self._set_cached_data(cache_key, result)

            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('ofi', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"OFI calculation error: {e}")
            performance_monitor.record_error('ofi', e)

            return {
                'ofi': 0.0,
                'buy_volume': 0.0,
                'sell_volume': 0.0,
                'imbalance_strength': 'UNKNOWN',
                'prediction': 'BALANCED',
                'confidence': 0.3,
                'timestamp': datetime.now(),
                'symbol': symbol,
                'timeframe': timeframe,
                'error': str(e)
            }

    # calculate_vpin, get_comprehensive_microstructure_signal Îì±
    # Îã§Î•∏ Î©îÏÑúÎìúÎì§ÎèÑ ÎèôÏùºÌïú Ìå®ÌÑ¥ÏúºÎ°ú Í≥†ÎèÑÌôî
    # (Í∏∏Ïù¥ Ï†úÌïúÏúºÎ°ú ÏÉùÎûµ)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# END OF PART 2/5
# Îã§Ïùå: Part 3 - VolatilityTermStructureAnalyzer, AnomalyDetectionSystem
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê