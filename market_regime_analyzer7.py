# market_regime_analyzer6.py

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from collections import deque
from logger_manager import get_logger
from scipy import stats
from scipy.stats import entropy


class OnChainDataManager:
    """
    ğŸ”— ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ê´€ë¦¬ì
    - Exchange Flow (ê±°ë˜ì†Œ ì…ì¶œê¸ˆ)
    - Whale Movements (ê³ ë˜ ì›€ì§ì„)
    - MVRV, NVT ë“± ì˜¨ì²´ì¸ ì§€í‘œ
    - Active Addresses, Transaction Volume
    """

    def __init__(self):
        self.logger = get_logger("OnChain")

        # ìºì‹±
        self._cache = {}
        self._cache_ttl = 300  # 5ë¶„ ìºì‹œ

        # ì˜¨ì²´ì¸ ë°ì´í„° íˆìŠ¤í† ë¦¬
        self.exchange_flow_history = deque(maxlen=100)
        self.whale_activity_history = deque(maxlen=100)
        self.mvrv_history = deque(maxlen=100)
        self.nvt_history = deque(maxlen=100)

        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'exchange_inflow_high': 10000,  # BTC
            'exchange_outflow_high': 10000,  # BTC
            'whale_transaction_threshold': 1000,  # BTC
            'mvrv_overbought': 3.5,
            'mvrv_oversold': 1.0,
            'nvt_high': 150,
            'nvt_low': 50
        }

    def _get_cached_data(self, key):
        """ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        """ë°ì´í„° ìºì‹±"""
        self._cache[key] = (data, datetime.now().timestamp())

    def get_exchange_flow(self, timeframe='1h'):
        """
        ê±°ë˜ì†Œ ì…ì¶œê¸ˆ ë¶„ì„
        Returns: {'net_flow': float, 'inflow': float, 'outflow': float, 'signal': str}
        """
        cached = self._get_cached_data(f'exchange_flow_{timeframe}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ Glassnode, CryptoQuant API ì‚¬ìš©
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            inflow = np.random.uniform(5000, 15000)
            outflow = np.random.uniform(5000, 15000)
            net_flow = inflow - outflow

            # ì‹ í˜¸ ìƒì„±
            if net_flow > self.thresholds['exchange_inflow_high']:
                signal = 'SELLING_PRESSURE'  # ê±°ë˜ì†Œë¡œ ëŒ€ëŸ‰ ìœ ì… = ë§¤ë„ ì••ë ¥
            elif net_flow < -self.thresholds['exchange_outflow_high']:
                signal = 'ACCUMULATION'  # ê±°ë˜ì†Œì—ì„œ ëŒ€ëŸ‰ ìœ ì¶œ = ì¶•ì 
            else:
                signal = 'NEUTRAL'

            result = {
                'net_flow': net_flow,
                'inflow': inflow,
                'outflow': outflow,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.exchange_flow_history.append(result)
            self._set_cached_data(f'exchange_flow_{timeframe}', result)

            return result

        except Exception as e:
            self.logger.error(f"Exchange flow analysis error: {e}")
            return {
                'net_flow': 0,
                'inflow': 0,
                'outflow': 0,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_whale_activity(self, timeframe='1h'):
        """
        ê³ ë˜ í™œë™ ë¶„ì„
        Returns: {'whale_transactions': int, 'whale_volume': float, 'signal': str}
        """
        cached = self._get_cached_data(f'whale_activity_{timeframe}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ì˜¨ì²´ì¸ API ì‚¬ìš©
            whale_transactions = np.random.randint(5, 50)
            whale_volume = np.random.uniform(1000, 5000)

            # ì‹ í˜¸ ìƒì„±
            if whale_transactions > 30 and whale_volume > 3000:
                signal = 'HIGH_WHALE_ACTIVITY'
            elif whale_transactions < 10:
                signal = 'LOW_WHALE_ACTIVITY'
            else:
                signal = 'MODERATE'

            result = {
                'whale_transactions': whale_transactions,
                'whale_volume': whale_volume,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.whale_activity_history.append(result)
            self._set_cached_data(f'whale_activity_{timeframe}', result)

            return result

        except Exception as e:
            self.logger.error(f"Whale activity analysis error: {e}")
            return {
                'whale_transactions': 0,
                'whale_volume': 0,
                'signal': 'MODERATE',
                'timestamp': datetime.now()
            }

    def get_mvrv_ratio(self):
        """
        MVRV (Market Value to Realized Value) ë¹„ìœ¨
        - > 3.5: ê³¼ë§¤ìˆ˜ (ì—­ì‚¬ì  ê³ ì  ê·¼ì²˜)
        - < 1.0: ê³¼ë§¤ë„ (ì—­ì‚¬ì  ì €ì  ê·¼ì²˜)
        Returns: {'mvrv': float, 'signal': str}
        """
        cached = self._get_cached_data('mvrv_ratio')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ Glassnode API ì‚¬ìš©
            mvrv = np.random.uniform(0.8, 4.0)

            # ì‹ í˜¸ ìƒì„±
            if mvrv > self.thresholds['mvrv_overbought']:
                signal = 'OVERVALUED'
            elif mvrv < self.thresholds['mvrv_oversold']:
                signal = 'UNDERVALUED'
            elif 1.0 <= mvrv <= 2.0:
                signal = 'FAIR_VALUE'
            else:
                signal = 'NEUTRAL'

            result = {
                'mvrv': mvrv,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.mvrv_history.append(result)
            self._set_cached_data('mvrv_ratio', result)

            return result

        except Exception as e:
            self.logger.error(f"MVRV calculation error: {e}")
            return {
                'mvrv': 1.5,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_nvt_ratio(self):
        """
        NVT (Network Value to Transactions) ë¹„ìœ¨
        - ë†’ì„ìˆ˜ë¡ ê³¼ëŒ€í‰ê°€
        - ë‚®ì„ìˆ˜ë¡ ì €í‰ê°€
        Returns: {'nvt': float, 'signal': str}
        """
        cached = self._get_cached_data('nvt_ratio')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ Glassnode API ì‚¬ìš©
            nvt = np.random.uniform(40, 160)

            # ì‹ í˜¸ ìƒì„±
            if nvt > self.thresholds['nvt_high']:
                signal = 'OVERVALUED'
            elif nvt < self.thresholds['nvt_low']:
                signal = 'UNDERVALUED'
            else:
                signal = 'NEUTRAL'

            result = {
                'nvt': nvt,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.nvt_history.append(result)
            self._set_cached_data('nvt_ratio', result)

            return result

        except Exception as e:
            self.logger.error(f"NVT calculation error: {e}")
            return {
                'nvt': 100,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_active_addresses(self, timeframe='24h'):
        """
        í™œì„± ì£¼ì†Œ ìˆ˜ ë¶„ì„
        Returns: {'active_addresses': int, 'change_pct': float, 'signal': str}
        """
        cached = self._get_cached_data(f'active_addresses_{timeframe}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ì˜¨ì²´ì¸ API ì‚¬ìš©
            active_addresses = np.random.randint(800000, 1200000)
            historical_avg = 1000000
            change_pct = ((active_addresses - historical_avg) / historical_avg) * 100

            # ì‹ í˜¸ ìƒì„±
            if change_pct > 15:
                signal = 'INCREASING_ADOPTION'
            elif change_pct < -15:
                signal = 'DECREASING_ACTIVITY'
            else:
                signal = 'STABLE'

            result = {
                'active_addresses': active_addresses,
                'change_pct': change_pct,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self._set_cached_data(f'active_addresses_{timeframe}', result)
            return result

        except Exception as e:
            self.logger.error(f"Active addresses analysis error: {e}")
            return {
                'active_addresses': 1000000,
                'change_pct': 0,
                'signal': 'STABLE',
                'timestamp': datetime.now()
            }

    def get_comprehensive_onchain_signal(self):
        """
        ì¢…í•© ì˜¨ì²´ì¸ ì‹ í˜¸ ìƒì„±
        Returns: {'score': float, 'signal': str, 'details': dict}
        """
        try:
            exchange_flow = self.get_exchange_flow()
            whale_activity = self.get_whale_activity()
            mvrv = self.get_mvrv_ratio()
            nvt = self.get_nvt_ratio()
            active_addresses = self.get_active_addresses()

            # ì‹ í˜¸ë³„ ì ìˆ˜í™” (-1.0 ~ 1.0)
            scores = {
                'exchange_flow': 0.0,
                'whale_activity': 0.0,
                'mvrv': 0.0,
                'nvt': 0.0,
                'active_addresses': 0.0
            }

            # Exchange Flow ì ìˆ˜
            if exchange_flow['signal'] == 'SELLING_PRESSURE':
                scores['exchange_flow'] = -0.8
            elif exchange_flow['signal'] == 'ACCUMULATION':
                scores['exchange_flow'] = 0.8

            # Whale Activity ì ìˆ˜
            if whale_activity['signal'] == 'HIGH_WHALE_ACTIVITY':
                scores['whale_activity'] = 0.5  # ì¤‘ë¦½ì  (ë°©í–¥ì„± ë¶ˆí™•ì‹¤)
            elif whale_activity['signal'] == 'LOW_WHALE_ACTIVITY':
                scores['whale_activity'] = -0.3

            # MVRV ì ìˆ˜
            if mvrv['signal'] == 'OVERVALUED':
                scores['mvrv'] = -0.7
            elif mvrv['signal'] == 'UNDERVALUED':
                scores['mvrv'] = 0.7
            elif mvrv['signal'] == 'FAIR_VALUE':
                scores['mvrv'] = 0.2

            # NVT ì ìˆ˜
            if nvt['signal'] == 'OVERVALUED':
                scores['nvt'] = -0.6
            elif nvt['signal'] == 'UNDERVALUED':
                scores['nvt'] = 0.6

            # Active Addresses ì ìˆ˜
            if active_addresses['signal'] == 'INCREASING_ADOPTION':
                scores['active_addresses'] = 0.5
            elif active_addresses['signal'] == 'DECREASING_ACTIVITY':
                scores['active_addresses'] = -0.5

            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weights = {
                'exchange_flow': 0.30,
                'whale_activity': 0.15,
                'mvrv': 0.25,
                'nvt': 0.20,
                'active_addresses': 0.10
            }

            total_score = sum(scores[k] * weights[k] for k in scores)

            # ì¢…í•© ì‹ í˜¸ ìƒì„±
            if total_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif total_score > 0.2:
                signal = 'BULLISH'
            elif total_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif total_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            return {
                'score': total_score,
                'signal': signal,
                'details': {
                    'exchange_flow': exchange_flow,
                    'whale_activity': whale_activity,
                    'mvrv': mvrv,
                    'nvt': nvt,
                    'active_addresses': active_addresses
                },
                'component_scores': scores
            }

        except Exception as e:
            self.logger.error(f"Comprehensive onchain signal error: {e}")
            return {
                'score': 0.0,
                'signal': 'NEUTRAL',
                'details': {},
                'component_scores': {}
            }


class MacroDataManager:
    """
    ğŸŒ ë§¤í¬ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ê´€ë¦¬ì
    - Funding Rates (í€ë”©ë¹„)
    - Open Interest (ë¯¸ê²°ì œì•½ì •)
    - Long/Short Ratio
    - Fear & Greed Index
    - Bitcoin Dominance
    - Stablecoin Supply
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("Macro")

        # ìºì‹±
        self._cache = {}
        self._cache_ttl = 300  # 5ë¶„ ìºì‹œ

        # ë§¤í¬ë¡œ ë°ì´í„° íˆìŠ¤í† ë¦¬
        self.funding_rate_history = deque(maxlen=100)
        self.oi_history = deque(maxlen=100)
        self.long_short_history = deque(maxlen=100)
        self.fear_greed_history = deque(maxlen=100)
        self.dominance_history = deque(maxlen=100)

        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'funding_rate_high': 0.05,  # 5%
            'funding_rate_low': -0.05,
            'oi_increase_threshold': 15,  # %
            'long_short_extreme_high': 1.5,
            'long_short_extreme_low': 0.67,
            'fear_greed_extreme': 75,
            'fear_greed_fear': 25,
            'btc_dominance_high': 60,
            'btc_dominance_low': 40
        }

    def _get_cached_data(self, key):
        """ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        """ë°ì´í„° ìºì‹±"""
        self._cache[key] = (data, datetime.now().timestamp())

    def get_funding_rate(self, symbol='BTCUSDT'):
        """
        í€ë”©ë¹„ ë¶„ì„
        - ì–‘ìˆ˜: ë¡± í¬ì§€ì…˜ì´ ë§ìŒ (ê³¼ì—´ ê°€ëŠ¥)
        - ìŒìˆ˜: ìˆ í¬ì§€ì…˜ì´ ë§ìŒ (ê³µí¬ ê°€ëŠ¥)
        Returns: {'funding_rate': float, 'signal': str}
        """
        cached = self._get_cached_data(f'funding_rate_{symbol}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ Binance Futures API ì‚¬ìš©
            funding_rate = np.random.uniform(-0.1, 0.1) / 100  # -0.1% ~ 0.1%

            # ì‹ í˜¸ ìƒì„±
            if funding_rate > self.thresholds['funding_rate_high']:
                signal = 'OVERHEATED_LONG'
            elif funding_rate < self.thresholds['funding_rate_low']:
                signal = 'OVERHEATED_SHORT'
            elif funding_rate > 0.02:
                signal = 'BULLISH_BIAS'
            elif funding_rate < -0.02:
                signal = 'BEARISH_BIAS'
            else:
                signal = 'NEUTRAL'

            result = {
                'funding_rate': funding_rate * 100,  # % ë‹¨ìœ„ë¡œ ë³€í™˜
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.funding_rate_history.append(result)
            self._set_cached_data(f'funding_rate_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Funding rate analysis error: {e}")
            return {
                'funding_rate': 0.01,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_open_interest(self, symbol='BTCUSDT'):
        """
        ë¯¸ê²°ì œì•½ì • (Open Interest) ë¶„ì„
        - ì¦ê°€ + ê°€ê²© ìƒìŠ¹: ê°•í•œ ìƒìŠ¹ ì¶”ì„¸
        - ì¦ê°€ + ê°€ê²© í•˜ë½: ê°•í•œ í•˜ë½ ì¶”ì„¸
        - ê°ì†Œ: ì¶”ì„¸ ì•½í™”
        Returns: {'oi': float, 'oi_change': float, 'signal': str}
        """
        cached = self._get_cached_data(f'open_interest_{symbol}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ê±°ë˜ì†Œ API ì‚¬ìš©
            current_oi = np.random.uniform(20000000000, 30000000000)  # 200ì–µ ~ 300ì–µ

            # ì´ì „ ë°ì´í„°ì™€ ë¹„êµ
            if len(self.oi_history) > 0:
                prev_oi = self.oi_history[-1]['oi']
                oi_change = ((current_oi - prev_oi) / prev_oi) * 100
            else:
                oi_change = 0

            # ê°€ê²© ì¶”ì„¸ í™•ì¸
            try:
                df = self.market_data.get_candle_data(symbol, '1h')
                if df is not None and len(df) > 1:
                    price_change = ((df['close'].iloc[-1] - df['close'].iloc[-2]) /
                                    df['close'].iloc[-2]) * 100
                else:
                    price_change = 0
            except:
                price_change = 0

            # ì‹ í˜¸ ìƒì„±
            if oi_change > self.thresholds['oi_increase_threshold']:
                if price_change > 1:
                    signal = 'STRONG_BULLISH_MOMENTUM'
                elif price_change < -1:
                    signal = 'STRONG_BEARISH_MOMENTUM'
                else:
                    signal = 'INCREASING_LEVERAGE'
            elif oi_change < -self.thresholds['oi_increase_threshold']:
                signal = 'DELEVERAGING'
            else:
                signal = 'STABLE'

            result = {
                'oi': current_oi,
                'oi_change': oi_change,
                'price_change': price_change,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.oi_history.append(result)
            self._set_cached_data(f'open_interest_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Open interest analysis error: {e}")
            return {
                'oi': 25000000000,
                'oi_change': 0,
                'price_change': 0,
                'signal': 'STABLE',
                'timestamp': datetime.now()
            }

    def get_long_short_ratio(self, symbol='BTCUSDT'):
        """
        ë¡±/ìˆ ë¹„ìœ¨ ë¶„ì„
        - > 1.5: ê·¹ë„ì˜ ë¡± í¬ì§€ì…˜ (ì¡°ì • ê°€ëŠ¥)
        - < 0.67: ê·¹ë„ì˜ ìˆ í¬ì§€ì…˜ (ë°˜ë“± ê°€ëŠ¥)
        Returns: {'ratio': float, 'signal': str}
        """
        cached = self._get_cached_data(f'long_short_ratio_{symbol}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ê±°ë˜ì†Œ API ì‚¬ìš©
            ratio = np.random.uniform(0.5, 2.0)

            # ì‹ í˜¸ ìƒì„±
            if ratio > self.thresholds['long_short_extreme_high']:
                signal = 'EXTREME_LONG'
            elif ratio < self.thresholds['long_short_extreme_low']:
                signal = 'EXTREME_SHORT'
            elif ratio > 1.2:
                signal = 'LONG_BIAS'
            elif ratio < 0.83:
                signal = 'SHORT_BIAS'
            else:
                signal = 'BALANCED'

            result = {
                'ratio': ratio,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.long_short_history.append(result)
            self._set_cached_data(f'long_short_ratio_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Long/Short ratio analysis error: {e}")
            return {
                'ratio': 1.0,
                'signal': 'BALANCED',
                'timestamp': datetime.now()
            }

    def get_fear_greed_index(self):
        """
        Fear & Greed Index ë¶„ì„
        - 0-25: Extreme Fear
        - 25-45: Fear
        - 45-55: Neutral
        - 55-75: Greed
        - 75-100: Extreme Greed
        Returns: {'index': int, 'signal': str}
        """
        cached = self._get_cached_data('fear_greed_index')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ Alternative.me API ì‚¬ìš©
            index = np.random.randint(0, 100)

            # ì‹ í˜¸ ìƒì„±
            if index >= self.thresholds['fear_greed_extreme']:
                signal = 'EXTREME_GREED'
            elif index >= 55:
                signal = 'GREED'
            elif index <= self.thresholds['fear_greed_fear']:
                signal = 'EXTREME_FEAR'
            elif index <= 45:
                signal = 'FEAR'
            else:
                signal = 'NEUTRAL'

            result = {
                'index': index,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.fear_greed_history.append(result)
            self._set_cached_data('fear_greed_index', result)

            return result

        except Exception as e:
            self.logger.error(f"Fear & Greed index error: {e}")
            return {
                'index': 50,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_bitcoin_dominance(self):
        """
        ë¹„íŠ¸ì½”ì¸ ë„ë¯¸ë„ŒìŠ¤ ë¶„ì„
        - ë†’ìŒ (>60%): BTC ê°•ì„¸, ì•ŒíŠ¸ ì•½ì„¸
        - ë‚®ìŒ (<40%): ì•ŒíŠ¸ ê°•ì„¸
        Returns: {'dominance': float, 'signal': str}
        """
        cached = self._get_cached_data('btc_dominance')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ CoinGecko API ì‚¬ìš©
            dominance = np.random.uniform(35, 65)

            # ì‹ í˜¸ ìƒì„±
            if dominance > self.thresholds['btc_dominance_high']:
                signal = 'BTC_DOMINANCE'
            elif dominance < self.thresholds['btc_dominance_low']:
                signal = 'ALTCOIN_SEASON'
            elif 45 <= dominance <= 55:
                signal = 'BALANCED'
            else:
                signal = 'TRANSITIONING'

            result = {
                'dominance': dominance,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.dominance_history.append(result)
            self._set_cached_data('btc_dominance', result)

            return result

        except Exception as e:
            self.logger.error(f"Bitcoin dominance analysis error: {e}")
            return {
                'dominance': 50,
                'signal': 'BALANCED',
                'timestamp': datetime.now()
            }

    def get_stablecoin_supply(self):
        """
        ìŠ¤í…Œì´ë¸”ì½”ì¸ ê³µê¸‰ëŸ‰ ë³€í™” ë¶„ì„
        - ì¦ê°€: ì ì¬ì  ë§¤ìˆ˜ ì••ë ¥
        - ê°ì†Œ: ìê¸ˆ ìœ ì¶œ
        Returns: {'supply': float, 'change_pct': float, 'signal': str}
        """
        cached = self._get_cached_data('stablecoin_supply')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ì˜¨ì²´ì¸ API ì‚¬ìš©
            supply = np.random.uniform(120000000000, 150000000000)  # 1200ì–µ ~ 1500ì–µ
            change_pct = np.random.uniform(-5, 5)

            # ì‹ í˜¸ ìƒì„±
            if change_pct > 3:
                signal = 'INCREASING_LIQUIDITY'
            elif change_pct < -3:
                signal = 'DECREASING_LIQUIDITY'
            else:
                signal = 'STABLE'

            result = {
                'supply': supply,
                'change_pct': change_pct,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self._set_cached_data('stablecoin_supply', result)
            return result

        except Exception as e:
            self.logger.error(f"Stablecoin supply analysis error: {e}")
            return {
                'supply': 135000000000,
                'change_pct': 0,
                'signal': 'STABLE',
                'timestamp': datetime.now()
            }

    def get_comprehensive_macro_signal(self):
        """
        ì¢…í•© ë§¤í¬ë¡œ ì‹ í˜¸ ìƒì„±
        Returns: {'score': float, 'signal': str, 'details': dict}
        """
        try:
            funding_rate = self.get_funding_rate()
            open_interest = self.get_open_interest()
            long_short_ratio = self.get_long_short_ratio()
            fear_greed = self.get_fear_greed_index()
            btc_dominance = self.get_bitcoin_dominance()
            stablecoin = self.get_stablecoin_supply()

            # ì‹ í˜¸ë³„ ì ìˆ˜í™” (-1.0 ~ 1.0)
            scores = {
                'funding_rate': 0.0,
                'open_interest': 0.0,
                'long_short_ratio': 0.0,
                'fear_greed': 0.0,
                'btc_dominance': 0.0,
                'stablecoin': 0.0
            }

            # Funding Rate ì ìˆ˜
            if funding_rate['signal'] == 'OVERHEATED_LONG':
                scores['funding_rate'] = -0.8
            elif funding_rate['signal'] == 'OVERHEATED_SHORT':
                scores['funding_rate'] = 0.8
            elif funding_rate['signal'] == 'BULLISH_BIAS':
                scores['funding_rate'] = 0.3
            elif funding_rate['signal'] == 'BEARISH_BIAS':
                scores['funding_rate'] = -0.3

            # Open Interest ì ìˆ˜
            if open_interest['signal'] == 'STRONG_BULLISH_MOMENTUM':
                scores['open_interest'] = 0.9
            elif open_interest['signal'] == 'STRONG_BEARISH_MOMENTUM':
                scores['open_interest'] = -0.9
            elif open_interest['signal'] == 'INCREASING_LEVERAGE':
                scores['open_interest'] = 0.5
            elif open_interest['signal'] == 'DELEVERAGING':
                scores['open_interest'] = -0.4

            # Long/Short Ratio ì ìˆ˜
            if long_short_ratio['signal'] == 'EXTREME_LONG':
                scores['long_short_ratio'] = -0.7  # ì—­ë°œìƒ
            elif long_short_ratio['signal'] == 'EXTREME_SHORT':
                scores['long_short_ratio'] = 0.7  # ì—­ë°œìƒ
            elif long_short_ratio['signal'] == 'LONG_BIAS':
                scores['long_short_ratio'] = 0.2
            elif long_short_ratio['signal'] == 'SHORT_BIAS':
                scores['long_short_ratio'] = -0.2

            # Fear & Greed ì ìˆ˜
            if fear_greed['signal'] == 'EXTREME_GREED':
                scores['fear_greed'] = -0.6  # ì—­ë°œìƒ
            elif fear_greed['signal'] == 'EXTREME_FEAR':
                scores['fear_greed'] = 0.6  # ì—­ë°œìƒ
            elif fear_greed['signal'] == 'GREED':
                scores['fear_greed'] = -0.2
            elif fear_greed['signal'] == 'FEAR':
                scores['fear_greed'] = 0.2

            # Bitcoin Dominance ì ìˆ˜
            if btc_dominance['signal'] == 'BTC_DOMINANCE':
                scores['btc_dominance'] = 0.3  # BTC ê°•ì„¸
            elif btc_dominance['signal'] == 'ALTCOIN_SEASON':
                scores['btc_dominance'] = 0.4  # ì‹œì¥ í™œì„±í™”

            # Stablecoin Supply ì ìˆ˜
            if stablecoin['signal'] == 'INCREASING_LIQUIDITY':
                scores['stablecoin'] = 0.7
            elif stablecoin['signal'] == 'DECREASING_LIQUIDITY':
                scores['stablecoin'] = -0.7

            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weights = {
                'funding_rate': 0.20,
                'open_interest': 0.25,
                'long_short_ratio': 0.15,
                'fear_greed': 0.15,
                'btc_dominance': 0.10,
                'stablecoin': 0.15
            }

            total_score = sum(scores[k] * weights[k] for k in scores)

            # ì¢…í•© ì‹ í˜¸ ìƒì„±
            if total_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif total_score > 0.2:
                signal = 'BULLISH'
            elif total_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif total_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            return {
                'score': total_score,
                'signal': signal,
                'details': {
                    'funding_rate': funding_rate,
                    'open_interest': open_interest,
                    'long_short_ratio': long_short_ratio,
                    'fear_greed': fear_greed,
                    'btc_dominance': btc_dominance,
                    'stablecoin': stablecoin
                },
                'component_scores': scores
            }

        except Exception as e:
            self.logger.error(f"Comprehensive macro signal error: {e}")
            return {
                'score': 0.0,
                'signal': 'NEUTRAL',
                'details': {},
                'component_scores': {}
            }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ 6ï¸âƒ£ ìœ ë™ì„± ìƒíƒœ ì¶”ì • (Liquidity Regime Detection) ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LiquidityRegimeDetector:
    """
    ğŸ’§ ìœ ë™ì„± ìƒíƒœ ì¶”ì • ì‹œìŠ¤í…œ (Liquidity Regime Detection)
    - Order Book Depth Analysis (í˜¸ê°€ì°½ ê¹Šì´ ë¶„ì„)
    - Bid-Ask Spread Analysis (ë§¤ìˆ˜-ë§¤ë„ ìŠ¤í”„ë ˆë“œ ë¶„ì„)
    - Market Impact Analysis (ì‹œì¥ ì¶©ê²© ë¶„ì„)
    - Slippage Estimation (ìŠ¬ë¦¬í”¼ì§€ ì¶”ì •)
    - Liquidity Score Calculation (ìœ ë™ì„± ì ìˆ˜ ê³„ì‚°)
    - Liquidity Regime Classification (ìœ ë™ì„± ì²´ì œ ë¶„ë¥˜)
    - Volume Profile Analysis (ê±°ë˜ëŸ‰ í”„ë¡œí•„ ë¶„ì„)
    - Liquidity Heatmap (ìœ ë™ì„± íˆíŠ¸ë§µ)
    - Flash Crash Detection (ê¸‰ë½ ê°ì§€)
    - Liquidity Provider Behavior (ìœ ë™ì„± ê³µê¸‰ì í–‰ë™ ë¶„ì„)
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("LiquidityRegime")

        # ğŸ“Š íˆìŠ¤í† ë¦¬ ë°ì´í„° ì €ì¥
        self.orderbook_depth_history = deque(maxlen=100)
        self.spread_history = deque(maxlen=100)
        self.liquidity_score_history = deque(maxlen=100)
        self.regime_history = deque(maxlen=100)
        self.market_impact_history = deque(maxlen=50)
        self.slippage_history = deque(maxlen=50)

        # ğŸ¯ ìœ ë™ì„± ë ˆë²¨ ì„ê³„ê°’
        self.liquidity_levels = {
            'very_high': 0.85,
            'high': 0.70,
            'medium': 0.50,
            'low': 0.30,
            'very_low': 0.15
        }

        # ğŸ“¦ ìºì‹±
        self._cache = {}
        self._cache_ttl = 30  # 30ì´ˆ ìºì‹œ (ìœ ë™ì„±ì€ ë¹ ë¥´ê²Œ ë³€í•¨)

        # ğŸšï¸ í˜¸ê°€ì°½ ë¶„ì„ íŒŒë¼ë¯¸í„°
        self.orderbook_config = {
            'depth_levels': 20,  # ë¶„ì„í•  í˜¸ê°€ ë ˆë²¨ ìˆ˜
            'size_threshold': 10,  # BTC, ëŒ€ëŸ‰ ì£¼ë¬¸ ì„ê³„ê°’
            'imbalance_threshold': 0.30,  # ë§¤ìˆ˜/ë§¤ë„ ë¶ˆê· í˜• ì„ê³„ê°’
            'wall_threshold': 50  # ë²½(wall) ê°ì§€ ì„ê³„ê°’ (BTC)
        }

        # ğŸ“ ìŠ¤í”„ë ˆë“œ ë¶„ì„ íŒŒë¼ë¯¸í„°
        self.spread_config = {
            'tight_spread_bps': 5,  # íƒ€ì´íŠ¸ ìŠ¤í”„ë ˆë“œ (5 bps)
            'normal_spread_bps': 10,
            'wide_spread_bps': 20,
            'very_wide_spread_bps': 50
        }

        # ğŸ’¥ ì‹œì¥ ì¶©ê²© ë¶„ì„ íŒŒë¼ë¯¸í„°
        self.impact_config = {
            'trade_sizes': [1, 5, 10, 25, 50, 100],  # BTC
            'impact_threshold_low': 0.001,  # 0.1%
            'impact_threshold_medium': 0.005,  # 0.5%
            'impact_threshold_high': 0.01  # 1.0%
        }

        # ğŸ”¥ í”Œë˜ì‹œ í¬ë˜ì‹œ ê°ì§€ íŒŒë¼ë¯¸í„°
        self.flash_crash_config = {
            'price_drop_threshold': 0.05,  # 5% ê¸‰ë½
            'time_window_seconds': 60,  # 1ë¶„ ì´ë‚´
            'recovery_threshold': 0.03,  # 3% íšŒë³µ
            'volume_spike_threshold': 3.0  # ê±°ë˜ëŸ‰ 3ë°° ì¦ê°€
        }

        # ğŸŒ¡ï¸ ìœ ë™ì„± íˆíŠ¸ë§µ íŒŒë¼ë¯¸í„°
        self.heatmap_config = {
            'price_levels': 50,  # ê°€ê²© ë ˆë²¨ ìˆ˜
            'time_buckets': 24,  # ì‹œê°„ ë²„í‚· (1ì‹œê°„ì”©)
            'min_liquidity_threshold': 1.0  # ìµœì†Œ ìœ ë™ì„± (BTC)
        }

        # ğŸ“Š ìœ ë™ì„± ê³µê¸‰ì í–‰ë™ ì¶”ì 
        self.lp_behavior = {
            'maker_taker_ratio': deque(maxlen=50),
            'order_cancellation_rate': deque(maxlen=50),
            'order_update_frequency': deque(maxlen=50),
            'aggressive_quotes': deque(maxlen=50)
        }

    def _get_cached_data(self, key):
        """ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        """ë°ì´í„° ìºì‹±"""
        self._cache[key] = (data, datetime.now().timestamp())

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1ï¸âƒ£ Order Book Depth Analysis (í˜¸ê°€ì°½ ê¹Šì´ ë¶„ì„)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_orderbook_depth(self, symbol='BTCUSDT'):
        """
        í˜¸ê°€ì°½ ê¹Šì´ ë¶„ì„
        Returns: {
            'total_bid_volume': float,
            'total_ask_volume': float,
            'bid_ask_imbalance': float,
            'depth_score': float,
            'major_walls': list,
            'depth_quality': str
        }
        """
        cached = self._get_cached_data(f'orderbook_depth_{symbol}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ê±°ë˜ì†Œ APIì—ì„œ í˜¸ê°€ì°½ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            depth_levels = self.orderbook_config['depth_levels']

            # ë§¤ìˆ˜ í˜¸ê°€ (bid) ì‹œë®¬ë ˆì´ì…˜
            bids = []
            base_price = 50000  # BTC ê¸°ì¤€ ê°€ê²©
            for i in range(depth_levels):
                price = base_price - (i * 10)
                volume = np.random.uniform(0.5, 5.0) * (1 / (i + 1))  # ê°€ê²©ì—ì„œ ë©€ìˆ˜ë¡ ì‘ì•„ì§
                bids.append({'price': price, 'volume': volume})

            # ë§¤ë„ í˜¸ê°€ (ask) ì‹œë®¬ë ˆì´ì…˜
            asks = []
            for i in range(depth_levels):
                price = base_price + (i * 10)
                volume = np.random.uniform(0.5, 5.0) * (1 / (i + 1))
                asks.append({'price': price, 'volume': volume})

            # ì´ ê±°ë˜ëŸ‰ ê³„ì‚°
            total_bid_volume = sum(b['volume'] for b in bids)
            total_ask_volume = sum(a['volume'] for a in asks)

            # ë§¤ìˆ˜/ë§¤ë„ ë¶ˆê· í˜• ê³„ì‚°
            total_volume = total_bid_volume + total_ask_volume
            bid_ask_imbalance = (total_bid_volume - total_ask_volume) / total_volume if total_volume > 0 else 0

            # ì£¼ìš” ë²½(wall) ê°ì§€
            wall_threshold = self.orderbook_config['wall_threshold']
            major_walls = []

            for bid in bids:
                if bid['volume'] > wall_threshold:
                    major_walls.append({
                        'side': 'bid',
                        'price': bid['price'],
                        'volume': bid['volume']
                    })

            for ask in asks:
                if ask['volume'] > wall_threshold:
                    major_walls.append({
                        'side': 'ask',
                        'price': ask['price'],
                        'volume': ask['volume']
                    })

            # ê¹Šì´ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
            # ë†’ì€ ê±°ë˜ëŸ‰ + ê· í˜•ì¡íŒ í˜¸ê°€ = ë†’ì€ ì ìˆ˜
            volume_score = min(total_volume / 100, 1.0)  # 100 BTCë¥¼ ë§Œì ìœ¼ë¡œ
            balance_score = 1.0 - abs(bid_ask_imbalance)
            depth_score = (volume_score * 0.7 + balance_score * 0.3)

            # ê¹Šì´ í’ˆì§ˆ ë¶„ë¥˜
            if depth_score >= 0.8:
                depth_quality = 'EXCELLENT'
            elif depth_score >= 0.6:
                depth_quality = 'GOOD'
            elif depth_score >= 0.4:
                depth_quality = 'FAIR'
            elif depth_score >= 0.2:
                depth_quality = 'POOR'
            else:
                depth_quality = 'VERY_POOR'

            result = {
                'total_bid_volume': total_bid_volume,
                'total_ask_volume': total_ask_volume,
                'bid_ask_imbalance': bid_ask_imbalance,
                'depth_score': depth_score,
                'major_walls': major_walls,
                'depth_quality': depth_quality,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.orderbook_depth_history.append(result)
            self._set_cached_data(f'orderbook_depth_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Orderbook depth analysis error: {e}")
            return {
                'total_bid_volume': 0,
                'total_ask_volume': 0,
                'bid_ask_imbalance': 0,
                'depth_score': 0.5,
                'major_walls': [],
                'depth_quality': 'UNKNOWN'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2ï¸âƒ£ Bid-Ask Spread Analysis (ë§¤ìˆ˜-ë§¤ë„ ìŠ¤í”„ë ˆë“œ ë¶„ì„)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_bid_ask_spread(self, symbol='BTCUSDT'):
        """
        ë§¤ìˆ˜-ë§¤ë„ ìŠ¤í”„ë ˆë“œ ë¶„ì„
        Returns: {
            'spread_bps': float,  # Basis Points (0.01%)
            'spread_percentage': float,
            'spread_quality': str,
            'mid_price': float,
            'best_bid': float,
            'best_ask': float
        }
        """
        cached = self._get_cached_data(f'spread_{symbol}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ê±°ë˜ì†Œ APIì—ì„œ í‹°ì»¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            base_price = 50000
            spread_pct = np.random.uniform(0.0001, 0.003)  # 0.01% ~ 0.3%

            best_bid = base_price * (1 - spread_pct / 2)
            best_ask = base_price * (1 + spread_pct / 2)
            mid_price = (best_bid + best_ask) / 2

            # ìŠ¤í”„ë ˆë“œ ê³„ì‚°
            spread_absolute = best_ask - best_bid
            spread_percentage = (spread_absolute / mid_price) * 100
            spread_bps = spread_percentage * 100  # Basis Points

            # ìŠ¤í”„ë ˆë“œ í’ˆì§ˆ í‰ê°€
            if spread_bps <= self.spread_config['tight_spread_bps']:
                spread_quality = 'VERY_TIGHT'
            elif spread_bps <= self.spread_config['normal_spread_bps']:
                spread_quality = 'TIGHT'
            elif spread_bps <= self.spread_config['wide_spread_bps']:
                spread_quality = 'NORMAL'
            elif spread_bps <= self.spread_config['very_wide_spread_bps']:
                spread_quality = 'WIDE'
            else:
                spread_quality = 'VERY_WIDE'

            result = {
                'spread_bps': spread_bps,
                'spread_percentage': spread_percentage,
                'spread_quality': spread_quality,
                'mid_price': mid_price,
                'best_bid': best_bid,
                'best_ask': best_ask,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.spread_history.append(result)
            self._set_cached_data(f'spread_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Spread analysis error: {e}")
            return {
                'spread_bps': 10,
                'spread_percentage': 0.1,
                'spread_quality': 'NORMAL',
                'mid_price': 50000,
                'best_bid': 49995,
                'best_ask': 50005
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3ï¸âƒ£ Market Impact Analysis (ì‹œì¥ ì¶©ê²© ë¶„ì„)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_market_impact(self, symbol='BTCUSDT'):
        """
        ë‹¤ì–‘í•œ ê±°ë˜ ê·œëª¨ì— ëŒ€í•œ ì‹œì¥ ì¶©ê²© ë¶„ì„
        Returns: {
            'impact_curve': list,  # [{size, buy_impact, sell_impact}]
            'average_impact': float,
            'impact_quality': str,
            'resilience_score': float
        }
        """
        cached = self._get_cached_data(f'market_impact_{symbol}')
        if cached:
            return cached

        try:
            # í˜¸ê°€ì°½ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            orderbook = self.analyze_orderbook_depth(symbol)

            # ë‹¤ì–‘í•œ ê±°ë˜ ê·œëª¨ì— ëŒ€í•œ ì¶©ê²© ê³„ì‚°
            trade_sizes = self.impact_config['trade_sizes']
            impact_curve = []

            for size in trade_sizes:
                # ë§¤ìˆ˜ ì¶©ê²© ì‹œë®¬ë ˆì´ì…˜
                buy_impact = self._simulate_trade_impact(
                    size, 
                    'buy', 
                    orderbook['total_ask_volume']
                )

                # ë§¤ë„ ì¶©ê²© ì‹œë®¬ë ˆì´ì…˜
                sell_impact = self._simulate_trade_impact(
                    size, 
                    'sell', 
                    orderbook['total_bid_volume']
                )

                impact_curve.append({
                    'size': size,
                    'buy_impact': buy_impact,
                    'sell_impact': sell_impact,
                    'average_impact': (buy_impact + sell_impact) / 2
                })

            # í‰ê·  ì¶©ê²© ê³„ì‚°
            average_impact = np.mean([ic['average_impact'] for ic in impact_curve])

            # ì¶©ê²© í’ˆì§ˆ í‰ê°€
            if average_impact < self.impact_config['impact_threshold_low']:
                impact_quality = 'VERY_LOW'
            elif average_impact < self.impact_config['impact_threshold_medium']:
                impact_quality = 'LOW'
            elif average_impact < self.impact_config['impact_threshold_high']:
                impact_quality = 'MODERATE'
            else:
                impact_quality = 'HIGH'

            # ë³µì›ë ¥ ì ìˆ˜ (ë‚®ì€ ì¶©ê²© = ë†’ì€ ë³µì›ë ¥)
            resilience_score = 1.0 - min(average_impact / self.impact_config['impact_threshold_high'], 1.0)

            result = {
                'impact_curve': impact_curve,
                'average_impact': average_impact,
                'impact_quality': impact_quality,
                'resilience_score': resilience_score,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.market_impact_history.append(result)
            self._set_cached_data(f'market_impact_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Market impact analysis error: {e}")
            return {
                'impact_curve': [],
                'average_impact': 0.005,
                'impact_quality': 'MODERATE',
                'resilience_score': 0.5
            }

    def _simulate_trade_impact(self, size, side, available_liquidity):
        """ê±°ë˜ ì¶©ê²© ì‹œë®¬ë ˆì´ì…˜"""
        try:
            # ê°„ë‹¨í•œ ì¶©ê²© ëª¨ë¸: ê±°ë˜ëŸ‰ / ê°€ìš© ìœ ë™ì„±
            if available_liquidity > 0:
                impact_ratio = size / available_liquidity
                
                # ë¹„ì„ í˜• ì¶©ê²© (í° ê±°ë˜ì¼ìˆ˜ë¡ ì¶©ê²©ì´ ë” í¬ê²Œ ì¦ê°€)
                impact = impact_ratio * (1 + impact_ratio)
                
                # ë¬´ì‘ìœ„ ë³€ë™ ì¶”ê°€
                impact *= np.random.uniform(0.8, 1.2)
                
                return min(impact, 0.1)  # ìµœëŒ€ 10% ì¶©ê²©
            else:
                return 0.05  # ê¸°ë³¸ê°’
        except:
            return 0.005

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4ï¸âƒ£ Slippage Estimation (ìŠ¬ë¦¬í”¼ì§€ ì¶”ì •)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def estimate_slippage(self, symbol='BTCUSDT', trade_size=10):
        """
        íŠ¹ì • ê±°ë˜ ê·œëª¨ì— ëŒ€í•œ ìŠ¬ë¦¬í”¼ì§€ ì¶”ì •
        Returns: {
            'expected_slippage_bps': float,
            'expected_slippage_pct': float,
            'worst_case_slippage_bps': float,
            'confidence': float,
            'execution_quality': str
        }
        """
        cached = self._get_cached_data(f'slippage_{symbol}_{trade_size}')
        if cached:
            return cached

        try:
            # ì‹œì¥ ì¶©ê²© ë¶„ì„ ë°ì´í„° í™œìš©
            impact_analysis = self.analyze_market_impact(symbol)
            
            # ìŠ¤í”„ë ˆë“œ ë¶„ì„ ë°ì´í„° í™œìš©
            spread_analysis = self.analyze_bid_ask_spread(symbol)

            # ìŠ¬ë¦¬í”¼ì§€ = ìŠ¤í”„ë ˆë“œ + ì‹œì¥ ì¶©ê²©
            spread_slippage = spread_analysis['spread_percentage']
            
            # í•´ë‹¹ ê±°ë˜ ê·œëª¨ì˜ ì¶©ê²© ì°¾ê¸°
            impact = 0
            for ic in impact_analysis['impact_curve']:
                if ic['size'] >= trade_size:
                    impact = ic['average_impact'] * 100  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                    break
            
            if impact == 0:  # ê±°ë˜ ê·œëª¨ê°€ ì»¤ë¸Œë¥¼ ì´ˆê³¼í•˜ë©´ ì™¸ì‚½
                impact = impact_analysis['average_impact'] * 100 * (trade_size / 50)

            # ì˜ˆìƒ ìŠ¬ë¦¬í”¼ì§€
            expected_slippage_pct = spread_slippage + impact
            expected_slippage_bps = expected_slippage_pct * 100

            # ìµœì•…ì˜ ê²½ìš° ìŠ¬ë¦¬í”¼ì§€ (ë³€ë™ì„± ê³ ë ¤)
            volatility_factor = 1.5  # ë³€ë™ì„± ê³ ë ¤ ê³„ìˆ˜
            worst_case_slippage_bps = expected_slippage_bps * volatility_factor

            # ì‹ ë¢°ë„ ê³„ì‚° (í˜¸ê°€ì°½ ê¹Šì´ì™€ ìŠ¤í”„ë ˆë“œ ì•ˆì •ì„± ê¸°ë°˜)
            orderbook = self.analyze_orderbook_depth(symbol)
            confidence = orderbook['depth_score'] * 0.7 + (1.0 - min(spread_slippage / 0.5, 1.0)) * 0.3

            # ì‹¤í–‰ í’ˆì§ˆ í‰ê°€
            if expected_slippage_bps < 10:
                execution_quality = 'EXCELLENT'
            elif expected_slippage_bps < 25:
                execution_quality = 'GOOD'
            elif expected_slippage_bps < 50:
                execution_quality = 'FAIR'
            elif expected_slippage_bps < 100:
                execution_quality = 'POOR'
            else:
                execution_quality = 'VERY_POOR'

            result = {
                'expected_slippage_bps': expected_slippage_bps,
                'expected_slippage_pct': expected_slippage_pct,
                'worst_case_slippage_bps': worst_case_slippage_bps,
                'confidence': confidence,
                'execution_quality': execution_quality,
                'trade_size': trade_size,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.slippage_history.append(result)
            self._set_cached_data(f'slippage_{symbol}_{trade_size}', result)

            return result

        except Exception as e:
            self.logger.error(f"Slippage estimation error: {e}")
            return {
                'expected_slippage_bps': 25,
                'expected_slippage_pct': 0.25,
                'worst_case_slippage_bps': 50,
                'confidence': 0.5,
                'execution_quality': 'FAIR',
                'trade_size': trade_size
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5ï¸âƒ£ Liquidity Score Calculation (ìœ ë™ì„± ì ìˆ˜ ê³„ì‚°)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_liquidity_score(self, symbol='BTCUSDT'):
        """
        ì¢…í•© ìœ ë™ì„± ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        Returns: {
            'liquidity_score': float,
            'component_scores': dict,
            'score_breakdown': dict,
            'confidence': float
        }
        """
        try:
            # ëª¨ë“  ìœ ë™ì„± ì§€í‘œ ìˆ˜ì§‘
            orderbook = self.analyze_orderbook_depth(symbol)
            spread = self.analyze_bid_ask_spread(symbol)
            impact = self.analyze_market_impact(symbol)
            slippage = self.estimate_slippage(symbol, trade_size=10)

            # ê° ì»´í¬ë„ŒíŠ¸ë³„ ì ìˆ˜ (0.0 ~ 1.0)
            component_scores = {
                'depth_score': orderbook['depth_score'],
                'spread_score': self._score_spread(spread),
                'impact_score': impact['resilience_score'],
                'slippage_score': self._score_slippage(slippage)
            }

            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weights = {
                'depth_score': 0.35,
                'spread_score': 0.25,
                'impact_score': 0.25,
                'slippage_score': 0.15
            }

            liquidity_score = sum(
                component_scores[k] * weights[k] 
                for k in component_scores
            )

            # ì‹ ë¢°ë„ ê³„ì‚° (ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜)
            confidence = np.mean([
                orderbook['depth_score'],
                slippage['confidence']
            ])

            # ì ìˆ˜ ì„¸ë¶€ ë‚´ì—­
            score_breakdown = {
                'orderbook_depth': {
                    'score': component_scores['depth_score'],
                    'weight': weights['depth_score'],
                    'contribution': component_scores['depth_score'] * weights['depth_score']
                },
                'bid_ask_spread': {
                    'score': component_scores['spread_score'],
                    'weight': weights['spread_score'],
                    'contribution': component_scores['spread_score'] * weights['spread_score']
                },
                'market_impact': {
                    'score': component_scores['impact_score'],
                    'weight': weights['impact_score'],
                    'contribution': component_scores['impact_score'] * weights['impact_score']
                },
                'slippage': {
                    'score': component_scores['slippage_score'],
                    'weight': weights['slippage_score'],
                    'contribution': component_scores['slippage_score'] * weights['slippage_score']
                }
            }

            result = {
                'liquidity_score': liquidity_score,
                'component_scores': component_scores,
                'score_breakdown': score_breakdown,
                'confidence': confidence,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.liquidity_score_history.append(result)

            return result

        except Exception as e:
            self.logger.error(f"Liquidity score calculation error: {e}")
            return {
                'liquidity_score': 0.5,
                'component_scores': {},
                'score_breakdown': {},
                'confidence': 0.5
            }

    def _score_spread(self, spread_data):
        """ìŠ¤í”„ë ˆë“œë¥¼ ì ìˆ˜ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        try:
            spread_bps = spread_data['spread_bps']
            
            # ìŠ¤í”„ë ˆë“œê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            if spread_bps <= self.spread_config['tight_spread_bps']:
                return 1.0
            elif spread_bps <= self.spread_config['normal_spread_bps']:
                return 0.8
            elif spread_bps <= self.spread_config['wide_spread_bps']:
                return 0.6
            elif spread_bps <= self.spread_config['very_wide_spread_bps']:
                return 0.4
            else:
                return 0.2
        except:
            return 0.5

    def _score_slippage(self, slippage_data):
        """ìŠ¬ë¦¬í”¼ì§€ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        try:
            slippage_bps = slippage_data['expected_slippage_bps']
            
            # ìŠ¬ë¦¬í”¼ì§€ê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            if slippage_bps <= 10:
                return 1.0
            elif slippage_bps <= 25:
                return 0.8
            elif slippage_bps <= 50:
                return 0.6
            elif slippage_bps <= 100:
                return 0.4
            else:
                return 0.2
        except:
            return 0.5

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6ï¸âƒ£ Liquidity Regime Classification (ìœ ë™ì„± ì²´ì œ ë¶„ë¥˜)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def classify_liquidity_regime(self, symbol='BTCUSDT'):
        """
        í˜„ì¬ ìœ ë™ì„± ìƒíƒœë¥¼ ì²´ì œë¡œ ë¶„ë¥˜
        Returns: {
            'regime': str,
            'regime_score': float,
            'regime_confidence': float,
            'characteristics': dict,
            'warnings': list
        }
        """
        try:
            # ìœ ë™ì„± ì ìˆ˜ ê³„ì‚°
            liquidity_analysis = self.calculate_liquidity_score(symbol)
            score = liquidity_analysis['liquidity_score']

            # ì¶”ê°€ ë¶„ì„
            orderbook = self.analyze_orderbook_depth(symbol)
            spread = self.analyze_bid_ask_spread(symbol)
            impact = self.analyze_market_impact(symbol)

            # ì²´ì œ ë¶„ë¥˜
            regime = None
            characteristics = {}
            warnings = []

            if score >= self.liquidity_levels['very_high']:
                regime = 'VERY_HIGH_LIQUIDITY'
                characteristics = {
                    'description': 'ë§¤ìš° ë†’ì€ ìœ ë™ì„± - ëŒ€ëŸ‰ ê±°ë˜ ê°€ëŠ¥',
                    'trade_recommendation': 'ëŒ€ëŸ‰ ì£¼ë¬¸ ì‹¤í–‰ì— ìµœì ',
                    'risk_level': 'VERY_LOW'
                }

            elif score >= self.liquidity_levels['high']:
                regime = 'HIGH_LIQUIDITY'
                characteristics = {
                    'description': 'ë†’ì€ ìœ ë™ì„± - ì›í™œí•œ ê±°ë˜ í™˜ê²½',
                    'trade_recommendation': 'ì¼ë°˜ ê±°ë˜ì— ì í•©',
                    'risk_level': 'LOW'
                }

            elif score >= self.liquidity_levels['medium']:
                regime = 'MEDIUM_LIQUIDITY'
                characteristics = {
                    'description': 'ì¤‘ê°„ ìœ ë™ì„± - ì£¼ì˜ í•„ìš”',
                    'trade_recommendation': 'ì¤‘ì†Œí˜• ì£¼ë¬¸ ê¶Œì¥',
                    'risk_level': 'MEDIUM'
                }
                
                if spread['spread_bps'] > self.spread_config['wide_spread_bps']:
                    warnings.append('âš ï¸ ë„“ì€ ìŠ¤í”„ë ˆë“œ ê°ì§€')

            elif score >= self.liquidity_levels['low']:
                regime = 'LOW_LIQUIDITY'
                characteristics = {
                    'description': 'ë‚®ì€ ìœ ë™ì„± - ì‹ ì¤‘í•œ ê±°ë˜ í•„ìš”',
                    'trade_recommendation': 'ì†ŒëŸ‰ ì£¼ë¬¸ë§Œ ê¶Œì¥',
                    'risk_level': 'HIGH'
                }
                
                warnings.append('âš ï¸ ë‚®ì€ ìœ ë™ì„± - ìŠ¬ë¦¬í”¼ì§€ ì£¼ì˜')
                
                if orderbook['bid_ask_imbalance'] > 0.3:
                    warnings.append('âš ï¸ í˜¸ê°€ ë¶ˆê· í˜• ê°ì§€')

            else:
                regime = 'VERY_LOW_LIQUIDITY'
                characteristics = {
                    'description': 'ë§¤ìš° ë‚®ì€ ìœ ë™ì„± - ê±°ë˜ ìœ„í—˜ ë†’ìŒ',
                    'trade_recommendation': 'ê±°ë˜ ìì œ ê¶Œì¥',
                    'risk_level': 'VERY_HIGH'
                }
                
                warnings.append('ğŸš¨ ë§¤ìš° ë‚®ì€ ìœ ë™ì„± - ê±°ë˜ ìœ„í—˜')
                warnings.append('ğŸš¨ ë†’ì€ ìŠ¬ë¦¬í”¼ì§€ ì˜ˆìƒ')

            # í”Œë˜ì‹œ í¬ë˜ì‹œ ìœ„í—˜ ì²´í¬
            flash_crash_risk = self._check_flash_crash_risk(orderbook, spread, impact)
            if flash_crash_risk['risk_detected']:
                warnings.append(f'ğŸš¨ í”Œë˜ì‹œ í¬ë˜ì‹œ ìœ„í—˜: {flash_crash_risk["risk_level"]}')

            result = {
                'regime': regime,
                'regime_score': score,
                'regime_confidence': liquidity_analysis['confidence'],
                'characteristics': characteristics,
                'warnings': warnings,
                'flash_crash_risk': flash_crash_risk,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.regime_history.append(result)

            return result

        except Exception as e:
            self.logger.error(f"Liquidity regime classification error: {e}")
            return {
                'regime': 'UNKNOWN',
                'regime_score': 0.5,
                'regime_confidence': 0.5,
                'characteristics': {},
                'warnings': []
            }

    def _check_flash_crash_risk(self, orderbook, spread, impact):
        """í”Œë˜ì‹œ í¬ë˜ì‹œ ìœ„í—˜ ì²´í¬"""
        try:
            risk_detected = False
            risk_level = 'LOW'
            risk_factors = []

            # 1ï¸âƒ£ í˜¸ê°€ì°½ ê¹Šì´ ë¶€ì¡±
            if orderbook['depth_score'] < 0.3:
                risk_factors.append('ì–•ì€ í˜¸ê°€ì°½')
                risk_detected = True

            # 2ï¸âƒ£ ë„“ì€ ìŠ¤í”„ë ˆë“œ
            if spread['spread_bps'] > self.spread_config['very_wide_spread_bps']:
                risk_factors.append('ë§¤ìš° ë„“ì€ ìŠ¤í”„ë ˆë“œ')
                risk_detected = True

            # 3ï¸âƒ£ ë†’ì€ ì‹œì¥ ì¶©ê²©
            if impact['average_impact'] > self.impact_config['impact_threshold_high']:
                risk_factors.append('ë†’ì€ ì‹œì¥ ì¶©ê²©')
                risk_detected = True

            # 4ï¸âƒ£ í˜¸ê°€ ë¶ˆê· í˜•
            if abs(orderbook['bid_ask_imbalance']) > 0.5:
                risk_factors.append('ì‹¬ê°í•œ í˜¸ê°€ ë¶ˆê· í˜•')
                risk_detected = True

            # ìœ„í—˜ ë ˆë²¨ ê²°ì •
            if len(risk_factors) >= 3:
                risk_level = 'VERY_HIGH'
            elif len(risk_factors) >= 2:
                risk_level = 'HIGH'
            elif len(risk_factors) >= 1:
                risk_level = 'MEDIUM'

            return {
                'risk_detected': risk_detected,
                'risk_level': risk_level,
                'risk_factors': risk_factors
            }

        except Exception as e:
            self.logger.debug(f"Flash crash risk check error: {e}")
            return {
                'risk_detected': False,
                'risk_level': 'UNKNOWN',
                'risk_factors': []
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7ï¸âƒ£ Volume Profile Analysis (ê±°ë˜ëŸ‰ í”„ë¡œí•„ ë¶„ì„)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_volume_profile(self, symbol='BTCUSDT', timeframe='1h', period=24):
        """
        ê±°ë˜ëŸ‰ í”„ë¡œí•„ ë¶„ì„ - ê°€ê²©ëŒ€ë³„ ê±°ë˜ëŸ‰ ë¶„í¬
        Returns: {
            'volume_profile': list,  # [{price, volume, percentage}]
            'value_area': dict,  # ê°€ì¥ ë§ì´ ê±°ë˜ëœ ê°€ê²©ëŒ€
            'poc': float,  # Point of Control (ìµœëŒ€ ê±°ë˜ëŸ‰ ê°€ê²©)
            'profile_shape': str
        }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ìº”ë“¤ ë°ì´í„°ì—ì„œ ê±°ë˜ëŸ‰ í”„ë¡œí•„ ìƒì„±
            # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            price_levels = 50
            base_price = 50000

            volume_profile = []
            total_volume = 0

            for i in range(price_levels):
                price = base_price - 500 + (i * 20)
                # ì •ê·œë¶„í¬ í˜•íƒœì˜ ê±°ë˜ëŸ‰ (ì¤‘ê°„ ê°€ê²©ëŒ€ì—ì„œ ë§ìŒ)
                volume = np.random.normal(100, 30) * np.exp(-((i - 25) ** 2) / 200)
                volume = max(volume, 0)
                total_volume += volume

                volume_profile.append({
                    'price': price,
                    'volume': volume
                })

            # í¼ì„¼í‹°ì§€ ê³„ì‚°
            for vp in volume_profile:
                vp['percentage'] = (vp['volume'] / total_volume * 100) if total_volume > 0 else 0

            # POC (Point of Control) - ìµœëŒ€ ê±°ë˜ëŸ‰ ê°€ê²©
            poc_data = max(volume_profile, key=lambda x: x['volume'])
            poc = poc_data['price']

            # Value Area (ìƒìœ„ 70% ê±°ë˜ëŸ‰ì´ ë°œìƒí•œ ê°€ê²©ëŒ€)
            sorted_profile = sorted(volume_profile, key=lambda x: x['volume'], reverse=True)
            cumulative_volume = 0
            value_area_levels = []

            for vp in sorted_profile:
                cumulative_volume += vp['volume']
                value_area_levels.append(vp)
                
                if cumulative_volume / total_volume >= 0.70:
                    break

            value_area = {
                'high': max(vp['price'] for vp in value_area_levels),
                'low': min(vp['price'] for vp in value_area_levels),
                'poc': poc
            }

            # í”„ë¡œí•„ í˜•íƒœ ë¶„ë¥˜
            profile_shape = self._classify_profile_shape(volume_profile)

            return {
                'volume_profile': volume_profile,
                'value_area': value_area,
                'poc': poc,
                'profile_shape': profile_shape,
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Volume profile analysis error: {e}")
            return {
                'volume_profile': [],
                'value_area': {},
                'poc': 0,
                'profile_shape': 'UNKNOWN'
            }

    def _classify_profile_shape(self, volume_profile):
        """ê±°ë˜ëŸ‰ í”„ë¡œí•„ í˜•íƒœ ë¶„ë¥˜"""
        try:
            if not volume_profile:
                return 'UNKNOWN'

            volumes = [vp['volume'] for vp in volume_profile]
            max_volume = max(volumes)
            max_idx = volumes.index(max_volume)

            # P-shaped: ìµœëŒ€ ê±°ë˜ëŸ‰ì´ ìƒë‹¨ì—
            if max_idx < len(volumes) * 0.3:
                return 'P_SHAPED'
            # b-shaped: ìµœëŒ€ ê±°ë˜ëŸ‰ì´ í•˜ë‹¨ì—
            elif max_idx > len(volumes) * 0.7:
                return 'B_SHAPED'
            # D-shaped (left): ìµœëŒ€ ê±°ë˜ëŸ‰ì´ ì™¼ìª½(ë‚®ì€ ê°€ê²©)ì—
            elif max_idx < len(volumes) * 0.2:
                return 'D_SHAPED_LEFT'
            # D-shaped (right): ìµœëŒ€ ê±°ë˜ëŸ‰ì´ ì˜¤ë¥¸ìª½(ë†’ì€ ê°€ê²©)ì—
            elif max_idx > len(volumes) * 0.8:
                return 'D_SHAPED_RIGHT'
            # Normal: ì¤‘ì•™ì— ìµœëŒ€ ê±°ë˜ëŸ‰
            else:
                return 'NORMAL'

        except Exception as e:
            self.logger.debug(f"Profile shape classification error: {e}")
            return 'UNKNOWN'

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 8ï¸âƒ£ Liquidity Heatmap (ìœ ë™ì„± íˆíŠ¸ë§µ)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def generate_liquidity_heatmap(self, symbol='BTCUSDT'):
        """
        ìœ ë™ì„± íˆíŠ¸ë§µ ìƒì„± - ì‹œê°„ëŒ€ë³„, ê°€ê²©ëŒ€ë³„ ìœ ë™ì„± ë¶„í¬
        Returns: {
            'heatmap': numpy.ndarray,  # 2D array [price_level, time]
            'price_levels': list,
            'time_labels': list,
            'hot_zones': list,  # ê³ ìœ ë™ì„± êµ¬ì—­
            'cold_zones': list  # ì €ìœ ë™ì„± êµ¬ì—­
        }
        """
        try:
            price_levels = self.heatmap_config['price_levels']
            time_buckets = self.heatmap_config['time_buckets']

            # ì‹œë®¬ë ˆì´ì…˜ íˆíŠ¸ë§µ ë°ì´í„°
            # ì‹¤ì œë¡œëŠ” ê³¼ê±° ë°ì´í„°ì—ì„œ ì‹œê°„ëŒ€ë³„/ê°€ê²©ëŒ€ë³„ ê±°ë˜ëŸ‰ ì§‘ê³„
            heatmap = np.random.rand(price_levels, time_buckets)
            
            # ì‹œì¥ ì‹œê°„ëŒ€ íŒ¨í„´ ë°˜ì˜ (ì˜ˆ: íŠ¹ì • ì‹œê°„ëŒ€ì— ìœ ë™ì„± ë†’ìŒ)
            for t in range(time_buckets):
                # ê±°ë˜ í™œë°œ ì‹œê°„ëŒ€ (ì˜ˆ: 8-10ì‹œ, 20-22ì‹œ)
                if t in [8, 9, 10, 20, 21, 22]:
                    heatmap[:, t] *= 1.5

            # ê°€ê²© ë ˆë²¨
            base_price = 50000
            price_range = 1000
            price_levels_list = [
                base_price - price_range/2 + (i * price_range / price_levels)
                for i in range(price_levels)
            ]

            # ì‹œê°„ ë ˆì´ë¸”
            time_labels = [f'{i:02d}:00' for i in range(time_buckets)]

            # ê³ ìœ ë™ì„± êµ¬ì—­ (hot zones) ì°¾ê¸°
            hot_threshold = np.percentile(heatmap, 80)
            hot_zones = []
            for i in range(price_levels):
                for j in range(time_buckets):
                    if heatmap[i, j] > hot_threshold:
                        hot_zones.append({
                            'price': price_levels_list[i],
                            'time': time_labels[j],
                            'liquidity': float(heatmap[i, j])
                        })

            # ì €ìœ ë™ì„± êµ¬ì—­ (cold zones) ì°¾ê¸°
            cold_threshold = np.percentile(heatmap, 20)
            cold_zones = []
            for i in range(price_levels):
                for j in range(time_buckets):
                    if heatmap[i, j] < cold_threshold:
                        cold_zones.append({
                            'price': price_levels_list[i],
                            'time': time_labels[j],
                            'liquidity': float(heatmap[i, j])
                        })

            return {
                'heatmap': heatmap.tolist(),  # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ listë¡œ ë³€í™˜
                'price_levels': price_levels_list,
                'time_labels': time_labels,
                'hot_zones': hot_zones[:10],  # ìƒìœ„ 10ê°œ
                'cold_zones': cold_zones[:10],  # í•˜ìœ„ 10ê°œ
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Liquidity heatmap generation error: {e}")
            return {
                'heatmap': [],
                'price_levels': [],
                'time_labels': [],
                'hot_zones': [],
                'cold_zones': []
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 9ï¸âƒ£ Flash Crash Detection (ê¸‰ë½ ê°ì§€)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def detect_flash_crash(self, symbol='BTCUSDT'):
        """
        í”Œë˜ì‹œ í¬ë˜ì‹œ ê°ì§€ ë° ë¶„ì„
        Returns: {
            'flash_crash_detected': bool,
            'severity': str,
            'price_drop_pct': float,
            'recovery_status': str,
            'volume_spike': float,
            'liquidity_drain': bool
        }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ìµœê·¼ ìº”ë“¤ ë°ì´í„° ë¶„ì„
            # ì‹œë®¬ë ˆì´ì…˜
            
            # ìµœê·¼ ê°€ê²© ë³€ë™ ì²´í¬
            price_drop_pct = np.random.uniform(-0.02, 0.02)  # -2% ~ 2%
            is_significant_drop = price_drop_pct < -self.flash_crash_config['price_drop_threshold']

            # ê±°ë˜ëŸ‰ ê¸‰ì¦ ì²´í¬
            volume_spike = np.random.uniform(0.5, 4.0)  # 0.5ë°° ~ 4ë°°
            is_volume_spike = volume_spike > self.flash_crash_config['volume_spike_threshold']

            # ìœ ë™ì„± ë¶„ì„
            liquidity_analysis = self.calculate_liquidity_score(symbol)
            liquidity_drain = liquidity_analysis['liquidity_score'] < 0.3

            # í”Œë˜ì‹œ í¬ë˜ì‹œ íŒì •
            flash_crash_detected = (
                is_significant_drop and 
                (is_volume_spike or liquidity_drain)
            )

            # ì‹¬ê°ë„ í‰ê°€
            if flash_crash_detected:
                if price_drop_pct < -0.10:  # -10% ì´ìƒ
                    severity = 'EXTREME'
                elif price_drop_pct < -0.07:  # -7% ì´ìƒ
                    severity = 'SEVERE'
                elif price_drop_pct < -0.05:  # -5% ì´ìƒ
                    severity = 'MODERATE'
                else:
                    severity = 'MILD'
            else:
                severity = 'NONE'

            # íšŒë³µ ìƒíƒœ ì²´í¬
            recovery_pct = np.random.uniform(-0.01, 0.04)  # íšŒë³µë¥ 
            if recovery_pct > self.flash_crash_config['recovery_threshold']:
                recovery_status = 'RECOVERED'
            elif recovery_pct > 0.01:
                recovery_status = 'RECOVERING'
            elif recovery_pct > 0:
                recovery_status = 'SLOW_RECOVERY'
            else:
                recovery_status = 'NO_RECOVERY'

            result = {
                'flash_crash_detected': flash_crash_detected,
                'severity': severity,
                'price_drop_pct': price_drop_pct * 100,
                'recovery_status': recovery_status,
                'recovery_pct': recovery_pct * 100,
                'volume_spike': volume_spike,
                'liquidity_drain': liquidity_drain,
                'timestamp': datetime.now()
            }

            return result

        except Exception as e:
            self.logger.error(f"Flash crash detection error: {e}")
            return {
                'flash_crash_detected': False,
                'severity': 'NONE',
                'price_drop_pct': 0,
                'recovery_status': 'NORMAL',
                'volume_spike': 1.0,
                'liquidity_drain': False
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”Ÿ Liquidity Provider Behavior (ìœ ë™ì„± ê³µê¸‰ì í–‰ë™ ë¶„ì„)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_lp_behavior(self, symbol='BTCUSDT'):
        """
        ìœ ë™ì„± ê³µê¸‰ì(LP) í–‰ë™ íŒ¨í„´ ë¶„ì„
        Returns: {
            'maker_taker_ratio': float,
            'order_cancellation_rate': float,
            'order_update_frequency': float,
            'aggressive_quoting': bool,
            'lp_confidence': float,
            'behavior_pattern': str
        }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ê±°ë˜ì†Œ APIì˜ ìƒì„¸ ì£¼ë¬¸ ë°ì´í„° í™œìš©
            # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°

            # Maker/Taker ë¹„ìœ¨
            maker_taker_ratio = np.random.uniform(0.3, 1.5)
            self.lp_behavior['maker_taker_ratio'].append(maker_taker_ratio)

            # ì£¼ë¬¸ ì·¨ì†Œìœ¨
            order_cancellation_rate = np.random.uniform(0.2, 0.8)
            self.lp_behavior['order_cancellation_rate'].append(order_cancellation_rate)

            # ì£¼ë¬¸ ì—…ë°ì´íŠ¸ ë¹ˆë„ (ì´ˆë‹¹)
            order_update_frequency = np.random.uniform(0.5, 5.0)
            self.lp_behavior['order_update_frequency'].append(order_update_frequency)

            # ê³µê²©ì  í˜¸ê°€ (íƒ€ì´íŠ¸í•œ ìŠ¤í”„ë ˆë“œ)
            spread_analysis = self.analyze_bid_ask_spread(symbol)
            aggressive_quoting = spread_analysis['spread_bps'] < self.spread_config['tight_spread_bps']
            self.lp_behavior['aggressive_quotes'].append(1 if aggressive_quoting else 0)

            # LP ì‹ ë¢°ë„ ê³„ì‚°
            # ë†’ì€ maker ë¹„ìœ¨ + ë‚®ì€ ì·¨ì†Œìœ¨ + ê³µê²©ì  í˜¸ê°€ = ë†’ì€ ì‹ ë¢°ë„
            lp_confidence = (
                min(maker_taker_ratio / 1.0, 1.0) * 0.4 +  # Maker ë¹„ìœ¨
                (1.0 - order_cancellation_rate) * 0.3 +  # ë‚®ì€ ì·¨ì†Œìœ¨
                (1 if aggressive_quoting else 0) * 0.3  # ê³µê²©ì  í˜¸ê°€
            )

            # í–‰ë™ íŒ¨í„´ ë¶„ë¥˜
            if lp_confidence > 0.7 and maker_taker_ratio > 0.8:
                behavior_pattern = 'STABLE_PROVISION'
            elif order_cancellation_rate > 0.6 or order_update_frequency > 3.0:
                behavior_pattern = 'VOLATILE_PROVISION'
            elif maker_taker_ratio < 0.5:
                behavior_pattern = 'TAKER_DOMINATED'
            elif aggressive_quoting:
                behavior_pattern = 'AGGRESSIVE_COMPETITION'
            else:
                behavior_pattern = 'NORMAL_PROVISION'

            result = {
                'maker_taker_ratio': maker_taker_ratio,
                'order_cancellation_rate': order_cancellation_rate,
                'order_update_frequency': order_update_frequency,
                'aggressive_quoting': aggressive_quoting,
                'lp_confidence': lp_confidence,
                'behavior_pattern': behavior_pattern,
                'timestamp': datetime.now()
            }

            return result

        except Exception as e:
            self.logger.error(f"LP behavior analysis error: {e}")
            return {
                'maker_taker_ratio': 1.0,
                'order_cancellation_rate': 0.5,
                'order_update_frequency': 1.0,
                'aggressive_quoting': False,
                'lp_confidence': 0.5,
                'behavior_pattern': 'NORMAL_PROVISION'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì¢…í•© ìœ ë™ì„± ë¦¬í¬íŠ¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_comprehensive_liquidity_report(self, symbol='BTCUSDT'):
        """
        ì¢…í•© ìœ ë™ì„± ë¶„ì„ ë¦¬í¬íŠ¸
        Returns: ëª¨ë“  ìœ ë™ì„± ì§€í‘œë¥¼ í†µí•©í•œ ìƒì„¸ ë¦¬í¬íŠ¸
        """
        try:
            # ëª¨ë“  ë¶„ì„ ìˆ˜í–‰
            orderbook = self.analyze_orderbook_depth(symbol)
            spread = self.analyze_bid_ask_spread(symbol)
            impact = self.analyze_market_impact(symbol)
            slippage = self.estimate_slippage(symbol, trade_size=10)
            liquidity_score = self.calculate_liquidity_score(symbol)
            regime = self.classify_liquidity_regime(symbol)
            volume_profile = self.analyze_volume_profile(symbol)
            heatmap = self.generate_liquidity_heatmap(symbol)
            flash_crash = self.detect_flash_crash(symbol)
            lp_behavior = self.analyze_lp_behavior(symbol)

            # ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = self._generate_liquidity_insights(
                regime, liquidity_score, spread, impact, flash_crash
            )

            # ê±°ë˜ ì¶”ì²œ ìƒì„±
            trade_recommendations = self._generate_trade_recommendations(
                regime, liquidity_score, slippage
            )

            report = {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                
                # í•µì‹¬ ì§€í‘œ
                'liquidity_regime': regime,
                'liquidity_score': liquidity_score,
                
                # ìƒì„¸ ë¶„ì„
                'orderbook_depth': orderbook,
                'bid_ask_spread': spread,
                'market_impact': impact,
                'slippage_estimate': slippage,
                'volume_profile': volume_profile,
                'liquidity_heatmap': heatmap,
                'flash_crash_status': flash_crash,
                'lp_behavior': lp_behavior,
                
                # ì¸ì‚¬ì´íŠ¸ ë° ì¶”ì²œ
                'insights': insights,
                'trade_recommendations': trade_recommendations,
                
                # íˆìŠ¤í† ë¦¬ í†µê³„
                'historical_stats': self._calculate_historical_stats()
            }

            return report

        except Exception as e:
            self.logger.error(f"Comprehensive liquidity report error: {e}")
            return {}

    def _generate_liquidity_insights(self, regime, liquidity_score, spread, impact, flash_crash):
        """ìœ ë™ì„± ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        try:
            # ì²´ì œ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
            if regime['regime'] == 'VERY_HIGH_LIQUIDITY':
                insights.append('âœ… ìµœì ì˜ ê±°ë˜ í™˜ê²½ - ëŒ€ëŸ‰ ì£¼ë¬¸ ì‹¤í–‰ ê°€ëŠ¥')
            elif regime['regime'] == 'VERY_LOW_LIQUIDITY':
                insights.append('âš ï¸ ìœ„í—˜: ë§¤ìš° ë‚®ì€ ìœ ë™ì„± - ê±°ë˜ ìì œ ê¶Œì¥')

            # ìŠ¤í”„ë ˆë“œ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
            if spread['spread_quality'] == 'VERY_WIDE':
                insights.append('ğŸ’¸ ë§¤ìš° ë„“ì€ ìŠ¤í”„ë ˆë“œ - ê±°ë˜ ë¹„ìš© ë†’ìŒ')
            elif spread['spread_quality'] == 'VERY_TIGHT':
                insights.append('ğŸ’° ë§¤ìš° íƒ€ì´íŠ¸í•œ ìŠ¤í”„ë ˆë“œ - ê±°ë˜ ë¹„ìš© ìµœì†Œí™”')

            # ì‹œì¥ ì¶©ê²© ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
            if impact['impact_quality'] == 'HIGH':
                insights.append('âš¡ ë†’ì€ ì‹œì¥ ì¶©ê²© - ì£¼ë¬¸ ë¶„í•  ì‹¤í–‰ ê¶Œì¥')
            elif impact['impact_quality'] == 'VERY_LOW':
                insights.append('ğŸ¯ ë‚®ì€ ì‹œì¥ ì¶©ê²© - ì›í™œí•œ ê±°ë˜ ê°€ëŠ¥')

            # í”Œë˜ì‹œ í¬ë˜ì‹œ ê²½ê³ 
            if flash_crash['flash_crash_detected']:
                insights.append(f'ğŸš¨ í”Œë˜ì‹œ í¬ë˜ì‹œ ê°ì§€ - ì‹¬ê°ë„: {flash_crash["severity"]}')

            # ì ìˆ˜ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
            score = liquidity_score['liquidity_score']
            if score > 0.8:
                insights.append('ğŸŒŸ ìš°ìˆ˜í•œ ìœ ë™ì„± í™˜ê²½')
            elif score < 0.3:
                insights.append('âš ï¸ ë¶ˆëŸ‰í•œ ìœ ë™ì„± í™˜ê²½')

        except Exception as e:
            self.logger.debug(f"Insights generation error: {e}")

        return insights

    def _generate_trade_recommendations(self, regime, liquidity_score, slippage):
        """ê±°ë˜ ì¶”ì²œ ìƒì„±"""
        recommendations = []

        try:
            score = liquidity_score['liquidity_score']

            # ì£¼ë¬¸ ê·œëª¨ ì¶”ì²œ
            if score >= 0.8:
                recommendations.append({
                    'category': 'ì£¼ë¬¸ ê·œëª¨',
                    'recommendation': 'ëŒ€ëŸ‰ ì£¼ë¬¸ ê°€ëŠ¥ (50+ BTC)',
                    'confidence': 'HIGH'
                })
            elif score >= 0.6:
                recommendations.append({
                    'category': 'ì£¼ë¬¸ ê·œëª¨',
                    'recommendation': 'ì¤‘í˜• ì£¼ë¬¸ ê¶Œì¥ (10-50 BTC)',
                    'confidence': 'MEDIUM'
                })
            else:
                recommendations.append({
                    'category': 'ì£¼ë¬¸ ê·œëª¨',
                    'recommendation': 'ì†Œí˜• ì£¼ë¬¸ë§Œ ê¶Œì¥ (< 10 BTC)',
                    'confidence': 'MEDIUM'
                })

            # ì‹¤í–‰ ì „ëµ ì¶”ì²œ
            if slippage['execution_quality'] in ['EXCELLENT', 'GOOD']:
                recommendations.append({
                    'category': 'ì‹¤í–‰ ì „ëµ',
                    'recommendation': 'ì‹œì¥ê°€ ì£¼ë¬¸ ê°€ëŠ¥',
                    'confidence': 'HIGH'
                })
            elif slippage['execution_quality'] == 'FAIR':
                recommendations.append({
                    'category': 'ì‹¤í–‰ ì „ëµ',
                    'recommendation': 'ì§€ì •ê°€ ì£¼ë¬¸ ê¶Œì¥',
                    'confidence': 'MEDIUM'
                })
            else:
                recommendations.append({
                    'category': 'ì‹¤í–‰ ì „ëµ',
                    'recommendation': 'TWAP/VWAP ì „ëµ ì‚¬ìš© ê¶Œì¥',
                    'confidence': 'HIGH'
                })

            # ì‹œê°„ëŒ€ ì¶”ì²œ
            recommendations.append({
                'category': 'ê±°ë˜ ì‹œê°„',
                'recommendation': 'ê±°ë˜ëŸ‰ì´ ë§ì€ ì‹œê°„ëŒ€ ì„ íƒ (08:00-10:00, 20:00-22:00)',
                'confidence': 'MEDIUM'
            })

            # ìœ„í—˜ ê´€ë¦¬
            if regime['regime'] in ['LOW_LIQUIDITY', 'VERY_LOW_LIQUIDITY']:
                recommendations.append({
                    'category': 'ìœ„í—˜ ê´€ë¦¬',
                    'recommendation': 'Stop-loss ì£¼ë¬¸ ì‚¬ìš© ì£¼ì˜ (ìŠ¬ë¦¬í”¼ì§€ ìœ„í—˜)',
                    'confidence': 'HIGH'
                })

        except Exception as e:
            self.logger.debug(f"Recommendations generation error: {e}")

        return recommendations

    def _calculate_historical_stats(self):
        """íˆìŠ¤í† ë¦¬ í†µê³„ ê³„ì‚°"""
        try:
            stats = {}

            # ìœ ë™ì„± ì ìˆ˜ í†µê³„
            if len(self.liquidity_score_history) > 0:
                scores = [ls['liquidity_score'] for ls in self.liquidity_score_history]
                stats['liquidity_score'] = {
                    'current': scores[-1],
                    'average': np.mean(scores),
                    'std': np.std(scores),
                    'min': np.min(scores),
                    'max': np.max(scores)
                }

            # ìŠ¤í”„ë ˆë“œ í†µê³„
            if len(self.spread_history) > 0:
                spreads = [s['spread_bps'] for s in self.spread_history]
                stats['spread'] = {
                    'current': spreads[-1],
                    'average': np.mean(spreads),
                    'min': np.min(spreads),
                    'max': np.max(spreads)
                }

            # ì²´ì œ ë¶„í¬
            if len(self.regime_history) > 0:
                regimes = [r['regime'] for r in self.regime_history]
                regime_counts = {}
                for regime in regimes:
                    regime_counts[regime] = regime_counts.get(regime, 0) + 1
                stats['regime_distribution'] = regime_counts

            return stats

        except Exception as e:
            self.logger.debug(f"Historical stats calculation error: {e}")
            return {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ 7ï¸âƒ£ ë§ˆì¼“ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ë¶„ì„ (Market Microstructure Analysis) ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MarketMicrostructureAnalyzer:
    """
    ğŸ“Š ë§ˆì¼“ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ë¶„ì„ ì‹œìŠ¤í…œ (Market Microstructure Analysis)
    
    ìµœì²¨ë‹¨ ì‹œì¥ ë¯¸ì„¸êµ¬ì¡° ë¶„ì„ì„ í†µí•œ ì´ˆë‹¨ê¸° íŠ¸ë ˆì´ë”© ì‹ í˜¸ ìƒì„±
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. **Order Flow Imbalance (OFI)** - ì£¼ë¬¸ íë¦„ ë¶ˆê· í˜• ë¶„ì„
    2. **VPIN** - Volume-Synchronized Probability of Informed Trading
    3. **Trade Classification** - Lee-Ready, Tick Test, Quote Rule
    4. **Effective Spread & Realized Spread** - ì‹¤íš¨ ìŠ¤í”„ë ˆë“œ ë¶„ì„
    5. **Price Impact** - ê±°ë˜ì˜ ê°€ê²© ì˜í–¥ë ¥ ì¸¡ì •
    6. **Adverse Selection Cost** - ì—­ì„ íƒ ë¹„ìš© ì¶”ì •
    7. **Quote Toxicity** - í˜¸ê°€ ë…ì„± ë¶„ì„
    8. **HFT Activity Detection** - ê³ ë¹ˆë„ ê±°ë˜ í™œë™ íƒì§€
    9. **Market Depth Resilience** - ì‹œì¥ ê¹Šì´ íšŒë³µë ¥
    10. **Price Discovery Contribution** - ê°€ê²© ë°œê²¬ ê¸°ì—¬ë„
    11. **Order Book Pressure** - í˜¸ê°€ì°½ ì••ë ¥ ë¶„ì„
    12. **Trade Aggression Index** - ê±°ë˜ ê³µê²©ì„± ì§€ìˆ˜
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("Microstructure")

        # ğŸ“Š íˆìŠ¤í† ë¦¬ ë°ì´í„° ì €ì¥
        self.ofi_history = deque(maxlen=100)  # Order Flow Imbalance
        self.vpin_history = deque(maxlen=100)  # VPIN
        self.trade_classification_history = deque(maxlen=500)  # Trade Classification
        self.spread_history = deque(maxlen=100)  # Spread Analysis
        self.price_impact_history = deque(maxlen=100)  # Price Impact
        self.toxicity_history = deque(maxlen=100)  # Quote Toxicity
        self.hft_activity_history = deque(maxlen=100)  # HFT Activity
        self.depth_resilience_history = deque(maxlen=50)  # Depth Resilience
        
        # ğŸ¯ ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'ofi_extreme': 0.7,  # OFI ê·¹ë‹¨ê°’
            'vpin_high': 0.75,  # VPIN ë†’ìŒ (ì •ë³´ê±°ë˜ ê°€ëŠ¥ì„± ë†’ìŒ)
            'vpin_low': 0.25,  # VPIN ë‚®ìŒ
            'toxicity_high': 0.65,  # ë†’ì€ ë…ì„±
            'hft_activity_high': 0.70,  # ë†’ì€ HFT í™œë™
            'adverse_selection_high': 0.008,  # 0.8% ì´ìƒ
            'price_impact_high': 0.005,  # 0.5% ì´ìƒ
            'depth_resilience_low': 0.3,  # ë‚®ì€ íšŒë³µë ¥
        }

        # ğŸ“¦ ìºì‹±
        self._cache = {}
        self._cache_ttl = 10  # 10ì´ˆ ìºì‹œ (ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ëŠ” ë¹ ë¥´ê²Œ ë³€í•¨)

        # ğŸ¨ VPIN ê³„ì‚° íŒŒë¼ë¯¸í„°
        self.vpin_config = {
            'volume_buckets': 50,  # Volume bucket ìˆ˜
            'bulk_classification_threshold': 0.8,  # Bulk volume ë¶„ë¥˜ ì„ê³„ê°’
            'cdf_confidence': 0.99  # CDF ì‹ ë¢° ìˆ˜ì¤€
        }

        # ğŸ“ˆ Trade Classification íŒŒë¼ë¯¸í„°
        self.trade_classification_config = {
            'quote_range_seconds': 5,  # Quote ë²”ìœ„ (ì´ˆ)
            'tick_test_lookback': 1,  # Tick test lookback
            'min_price_change': 0.0001  # ìµœì†Œ ê°€ê²© ë³€í™”
        }

        # ğŸ”¥ HFT Detection íŒŒë¼ë¯¸í„°
        self.hft_config = {
            'message_rate_threshold': 100,  # ì´ˆë‹¹ ë©”ì‹œì§€ ìˆ˜
            'cancellation_ratio_threshold': 0.85,  # ì·¨ì†Œìœ¨
            'quote_update_frequency_threshold': 50,  # ì´ˆë‹¹ í˜¸ê°€ ì—…ë°ì´íŠ¸
            'small_order_ratio_threshold': 0.75  # ì†Œê·œëª¨ ì£¼ë¬¸ ë¹„ìœ¨
        }

        # ğŸ“Š Order Book ë¶„ì„ íŒŒë¼ë¯¸í„°
        self.orderbook_config = {
            'levels': 10,  # ë¶„ì„í•  í˜¸ê°€ ë ˆë²¨ ìˆ˜
            'time_window_seconds': 60,  # ë¶„ì„ ì‹œê°„ ìœˆë„ìš°
            'pressure_lookback': 20  # ì••ë ¥ ê³„ì‚° lookback
        }

    def _get_cached_data(self, key):
        """ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        """ë°ì´í„° ìºì‹±"""
        self._cache[key] = (data, datetime.now().timestamp())

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1ï¸âƒ£ Order Flow Imbalance (OFI) ë¶„ì„
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_order_flow_imbalance(self, symbol='BTCUSDT', timeframe='1m'):
        """
        Order Flow Imbalance (OFI) ê³„ì‚°
        
        OFIëŠ” ë§¤ìˆ˜/ë§¤ë„ ì£¼ë¬¸ íë¦„ì˜ ë¶ˆê· í˜•ì„ ì¸¡ì •í•˜ì—¬ ë‹¨ê¸° ê°€ê²© ë°©í–¥ì„ ì˜ˆì¸¡
        
        Returns:
            dict: {
                'ofi': float (-1.0 ~ 1.0),
                'buy_volume': float,
                'sell_volume': float,
                'imbalance_strength': str,
                'prediction': str
            }
        """
        cached = self._get_cached_data(f'ofi_{symbol}_{timeframe}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ê±°ë˜ì†Œ APIì—ì„œ Trade ë°ì´í„° ìˆ˜ì§‘
            # ì‹œë®¬ë ˆì´ì…˜: ìµœê·¼ ê±°ë˜ ë°ì´í„° ìƒì„±
            
            # Buy volume (taker buy)
            buy_volume = np.random.uniform(10, 100) * np.random.choice([1, 1.2, 0.8])
            
            # Sell volume (taker sell)
            sell_volume = np.random.uniform(10, 100) * np.random.choice([1, 1.2, 0.8])
            
            # OFI ê³„ì‚°
            total_volume = buy_volume + sell_volume
            if total_volume > 0:
                ofi = (buy_volume - sell_volume) / total_volume
            else:
                ofi = 0.0
            
            # OFI ê°•ë„ ë¶„ë¥˜
            if abs(ofi) > self.thresholds['ofi_extreme']:
                strength = 'EXTREME'
            elif abs(ofi) > 0.5:
                strength = 'STRONG'
            elif abs(ofi) > 0.3:
                strength = 'MODERATE'
            else:
                strength = 'WEAK'
            
            # ì˜ˆì¸¡ ì‹ í˜¸
            if ofi > self.thresholds['ofi_extreme']:
                prediction = 'STRONG_BUY_PRESSURE'
            elif ofi > 0.3:
                prediction = 'BUY_PRESSURE'
            elif ofi < -self.thresholds['ofi_extreme']:
                prediction = 'STRONG_SELL_PRESSURE'
            elif ofi < -0.3:
                prediction = 'SELL_PRESSURE'
            else:
                prediction = 'BALANCED'
            
            result = {
                'ofi': ofi,
                'buy_volume': buy_volume,
                'sell_volume': sell_volume,
                'imbalance_strength': strength,
                'prediction': prediction,
                'timestamp': datetime.now()
            }
            
            self.ofi_history.append(result)
            self._set_cached_data(f'ofi_{symbol}_{timeframe}', result)
            
            return result

        except Exception as e:
            self.logger.error(f"OFI calculation error: {e}")
            return {
                'ofi': 0.0,
                'buy_volume': 0.0,
                'sell_volume': 0.0,
                'imbalance_strength': 'UNKNOWN',
                'prediction': 'BALANCED'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2ï¸âƒ£ VPIN (Volume-Synchronized Probability of Informed Trading)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_vpin(self, symbol='BTCUSDT', lookback_hours=24):
        """
        VPIN ê³„ì‚° - ì •ë³´ê±°ë˜ í™•ë¥  ì¶”ì •
        
        VPINì´ ë†’ì„ìˆ˜ë¡:
        - ì •ë³´ ë¹„ëŒ€ì¹­ì„±ì´ í¬ë‹¤
        - ì—­ì„ íƒ ìœ„í—˜ì´ ë†’ë‹¤
        - ìœ ë™ì„± ê³µê¸‰ìê°€ ìŠ¤í”„ë ˆë“œë¥¼ ë„“íŒë‹¤
        - ê°€ê²© ë³€ë™ì„±ì´ ì¦ê°€í•  ê°€ëŠ¥ì„±
        
        Returns:
            dict: {
                'vpin': float (0.0 ~ 1.0),
                'toxicity_level': str,
                'informed_trading_probability': float,
                'risk_level': str
            }
        """
        cached = self._get_cached_data(f'vpin_{symbol}')
        if cached:
            return cached

        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ Volume bucket ê¸°ë°˜ VPIN ê³„ì‚°
            # ì‹œë®¬ë ˆì´ì…˜: VPIN ì¶”ì •
            
            # Volume bucketë³„ buy/sell ë¶ˆê· í˜• ê³„ì‚°
            n_buckets = self.vpin_config['volume_buckets']
            volume_imbalances = []
            
            for _ in range(n_buckets):
                # ê° bucketì˜ ë§¤ìˆ˜/ë§¤ë„ ë¶ˆê· í˜•
                buy_vol = np.random.uniform(0, 100)
                sell_vol = np.random.uniform(0, 100)
                imbalance = abs(buy_vol - sell_vol) / (buy_vol + sell_vol + 1e-6)
                volume_imbalances.append(imbalance)
            
            # VPIN = í‰ê·  ë¶ˆê· í˜•
            vpin = np.mean(volume_imbalances)
            
            # ì¶”ì„¸ ê³ ë ¤ (ìµœê·¼ì´ ë” ì¤‘ìš”)
            recent_weight = np.linspace(0.5, 1.5, n_buckets)
            weighted_vpin = np.average(volume_imbalances, weights=recent_weight)
            vpin = weighted_vpin
            
            # VPIN ì •ê·œí™” (0 ~ 1)
            vpin = np.clip(vpin, 0.0, 1.0)
            
            # ë…ì„± ìˆ˜ì¤€ ë¶„ë¥˜
            if vpin > self.thresholds['vpin_high']:
                toxicity_level = 'HIGH'
                risk_level = 'VERY_HIGH'
            elif vpin > 0.5:
                toxicity_level = 'MODERATE'
                risk_level = 'HIGH'
            elif vpin < self.thresholds['vpin_low']:
                toxicity_level = 'LOW'
                risk_level = 'LOW'
            else:
                toxicity_level = 'NORMAL'
                risk_level = 'MEDIUM'
            
            # ì •ë³´ê±°ë˜ í™•ë¥ 
            informed_trading_probability = vpin
            
            result = {
                'vpin': vpin,
                'toxicity_level': toxicity_level,
                'informed_trading_probability': informed_trading_probability,
                'risk_level': risk_level,
                'volume_imbalances': volume_imbalances[-10:],  # ìµœê·¼ 10ê°œ
                'timestamp': datetime.now()
            }
            
            self.vpin_history.append(result)
            self._set_cached_data(f'vpin_{symbol}', result)
            
            return result

        except Exception as e:
            self.logger.error(f"VPIN calculation error: {e}")
            return {
                'vpin': 0.5,
                'toxicity_level': 'NORMAL',
                'informed_trading_probability': 0.5,
                'risk_level': 'MEDIUM'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3ï¸âƒ£ Trade Classification (Lee-Ready Algorithm)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def classify_trades(self, symbol='BTCUSDT', n_trades=100):
        """
        ê±°ë˜ë¥¼ ë§¤ìˆ˜ì ì£¼ë„ vs ë§¤ë„ì ì£¼ë„ë¡œ ë¶„ë¥˜
        
        Lee-Ready Algorithm:
        1. Quote Rule: ê±°ë˜ê°€ ë§¤ìˆ˜í˜¸ê°€ì— ê°€ê¹Œìš°ë©´ ë§¤ìˆ˜ì ì£¼ë„
        2. Tick Test: ê°€ê²©ì´ ìƒìŠ¹í•˜ë©´ ë§¤ìˆ˜ì ì£¼ë„
        
        Returns:
            dict: {
                'buy_initiated_ratio': float,
                'sell_initiated_ratio': float,
                'trade_aggression_index': float,
                'market_direction': str
            }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ì‹¤ì‹œê°„ ê±°ë˜ ë°ì´í„° ë¶„ë¥˜
            # ì‹œë®¬ë ˆì´ì…˜
            
            buy_initiated = 0
            sell_initiated = 0
            
            for _ in range(n_trades):
                # Quote rule + Tick test ì‹œë®¬ë ˆì´ì…˜
                classification = np.random.choice(
                    ['buy', 'sell'], 
                    p=[0.52, 0.48]  # ì•½ê°„ì˜ ë§¤ìˆ˜ í¸í–¥
                )
                
                if classification == 'buy':
                    buy_initiated += 1
                else:
                    sell_initiated += 1
            
            # ë¹„ìœ¨ ê³„ì‚°
            buy_ratio = buy_initiated / n_trades
            sell_ratio = sell_initiated / n_trades
            
            # Trade Aggression Index (TAI)
            # -1 (ëª¨ë‘ ë§¤ë„ì ì£¼ë„) ~ +1 (ëª¨ë‘ ë§¤ìˆ˜ì ì£¼ë„)
            tai = (buy_initiated - sell_initiated) / n_trades
            
            # ì‹œì¥ ë°©í–¥
            if tai > 0.3:
                market_direction = 'STRONG_BUYING'
            elif tai > 0.1:
                market_direction = 'MODERATE_BUYING'
            elif tai < -0.3:
                market_direction = 'STRONG_SELLING'
            elif tai < -0.1:
                market_direction = 'MODERATE_SELLING'
            else:
                market_direction = 'NEUTRAL'
            
            result = {
                'buy_initiated_ratio': buy_ratio,
                'sell_initiated_ratio': sell_ratio,
                'trade_aggression_index': tai,
                'market_direction': market_direction,
                'n_trades_analyzed': n_trades,
                'timestamp': datetime.now()
            }
            
            self.trade_classification_history.append(result)
            
            return result

        except Exception as e:
            self.logger.error(f"Trade classification error: {e}")
            return {
                'buy_initiated_ratio': 0.5,
                'sell_initiated_ratio': 0.5,
                'trade_aggression_index': 0.0,
                'market_direction': 'NEUTRAL',
                'n_trades_analyzed': 0
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4ï¸âƒ£ Effective Spread & Realized Spread
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_spreads(self, symbol='BTCUSDT'):
        """
        Effective Spread & Realized Spread ë¶„ì„
        
        - Quoted Spread: ë§¤ìˆ˜/ë§¤ë„ í˜¸ê°€ ì°¨ì´
        - Effective Spread: ì‹¤ì œ ê±°ë˜ ë¹„ìš©
        - Realized Spread: ìœ ë™ì„± ê³µê¸‰ìì˜ ìˆ˜ìµ
        - Price Impact: ê±°ë˜ì˜ ì˜êµ¬ì  ê°€ê²© ì˜í–¥
        
        Returns:
            dict: {
                'quoted_spread_bps': float,
                'effective_spread_bps': float,
                'realized_spread_bps': float,
                'price_impact_bps': float,
                'adverse_selection_component': float
            }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ í˜¸ê°€ì°½ ë° ê±°ë˜ ë°ì´í„° ì‚¬ìš©
            # ì‹œë®¬ë ˆì´ì…˜
            
            mid_price = 50000  # BTC ê¸°ì¤€ ê°€ê²©
            
            # Quoted Spread
            best_bid = mid_price * 0.9999
            best_ask = mid_price * 1.0001
            quoted_spread = best_ask - best_bid
            quoted_spread_bps = (quoted_spread / mid_price) * 10000
            
            # Effective Spread (ì‹¤ì œ ê±°ë˜ ê°€ê²© ê³ ë ¤)
            trade_price = np.random.uniform(best_bid, best_ask)
            effective_spread = 2 * abs(trade_price - mid_price)
            effective_spread_bps = (effective_spread / mid_price) * 10000
            
            # Realized Spread (5ì´ˆ í›„ mid priceì™€ ë¹„êµ)
            future_mid_price = mid_price * (1 + np.random.uniform(-0.0002, 0.0002))
            realized_spread = 2 * (trade_price - future_mid_price) * np.sign(trade_price - mid_price)
            realized_spread_bps = (realized_spread / mid_price) * 10000
            
            # Price Impact (ì˜êµ¬ì  ê°€ê²© ë³€í™”)
            price_impact = effective_spread - realized_spread
            price_impact_bps = (price_impact / mid_price) * 10000
            
            # Adverse Selection Component
            # = Price Impact / Effective Spread
            if effective_spread > 0:
                adverse_selection = abs(price_impact / effective_spread)
            else:
                adverse_selection = 0.0
            
            result = {
                'quoted_spread_bps': quoted_spread_bps,
                'effective_spread_bps': effective_spread_bps,
                'realized_spread_bps': realized_spread_bps,
                'price_impact_bps': abs(price_impact_bps),
                'adverse_selection_component': adverse_selection,
                'timestamp': datetime.now()
            }
            
            self.spread_history.append(result)
            
            return result

        except Exception as e:
            self.logger.error(f"Spread analysis error: {e}")
            return {
                'quoted_spread_bps': 10.0,
                'effective_spread_bps': 12.0,
                'realized_spread_bps': 8.0,
                'price_impact_bps': 4.0,
                'adverse_selection_component': 0.33
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5ï¸âƒ£ Quote Toxicity Analysis
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_quote_toxicity(self, symbol='BTCUSDT'):
        """
        í˜¸ê°€ ë…ì„± ë¶„ì„
        
        ë†’ì€ ë…ì„± = ì •ë³´ê±°ë˜ìê°€ ë§ìŒ = ë§ˆì¼“ë©”ì´ì»¤ê°€ ìœ„í—˜ì„ ëŠë‚Œ
        
        Returns:
            dict: {
                'toxicity_score': float (0.0 ~ 1.0),
                'quote_update_frequency': float,
                'cancellation_ratio': float,
                'toxicity_level': str
            }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ í˜¸ê°€ ì—…ë°ì´íŠ¸ ë¹ˆë„ ë° ì·¨ì†Œìœ¨ ë¶„ì„
            
            # í˜¸ê°€ ì—…ë°ì´íŠ¸ ë¹ˆë„ (ì´ˆë‹¹)
            quote_updates_per_second = np.random.uniform(5, 100)
            
            # ì·¨ì†Œìœ¨ (ì·¨ì†Œëœ ì£¼ë¬¸ / ì „ì²´ ì£¼ë¬¸)
            cancellation_ratio = np.random.uniform(0.3, 0.95)
            
            # ì‘ì€ í˜¸ê°€ ë¹„ìœ¨ (1 BTC ë¯¸ë§Œ)
            small_quote_ratio = np.random.uniform(0.5, 0.9)
            
            # Toxicity Score ê³„ì‚°
            toxicity_components = [
                min(quote_updates_per_second / 100, 1.0),  # ë¹ˆë²ˆí•œ ì—…ë°ì´íŠ¸
                cancellation_ratio,  # ë†’ì€ ì·¨ì†Œìœ¨
                small_quote_ratio  # ì‘ì€ ì£¼ë¬¸ í¬ê¸°
            ]
            
            toxicity_score = np.mean(toxicity_components)
            
            # ë…ì„± ìˆ˜ì¤€
            if toxicity_score > self.thresholds['toxicity_high']:
                toxicity_level = 'HIGH'
            elif toxicity_score > 0.45:
                toxicity_level = 'MODERATE'
            else:
                toxicity_level = 'LOW'
            
            result = {
                'toxicity_score': toxicity_score,
                'quote_update_frequency': quote_updates_per_second,
                'cancellation_ratio': cancellation_ratio,
                'small_quote_ratio': small_quote_ratio,
                'toxicity_level': toxicity_level,
                'timestamp': datetime.now()
            }
            
            self.toxicity_history.append(result)
            
            return result

        except Exception as e:
            self.logger.error(f"Quote toxicity analysis error: {e}")
            return {
                'toxicity_score': 0.5,
                'quote_update_frequency': 20.0,
                'cancellation_ratio': 0.6,
                'toxicity_level': 'MODERATE'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6ï¸âƒ£ HFT Activity Detection
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def detect_hft_activity(self, symbol='BTCUSDT'):
        """
        ê³ ë¹ˆë„ ê±°ë˜(HFT) í™œë™ íƒì§€
        
        HFT íŠ¹ì§•:
        - ë§¤ìš° ë†’ì€ ë©”ì‹œì§€ ì†ë„
        - ë†’ì€ ì£¼ë¬¸ ì·¨ì†Œìœ¨
        - ì‘ì€ ì£¼ë¬¸ í¬ê¸°
        - ì§§ì€ ë³´ìœ  ì‹œê°„
        
        Returns:
            dict: {
                'hft_activity_score': float (0.0 ~ 1.0),
                'hft_detected': bool,
                'message_rate': float,
                'activity_level': str
            }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ ê±°ë˜ì†Œ ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¼ ë¶„ì„
            
            # ë©”ì‹œì§€ ì†ë„ (ì´ˆë‹¹)
            message_rate = np.random.uniform(10, 200)
            
            # ì£¼ë¬¸ ì·¨ì†Œìœ¨
            cancellation_ratio = np.random.uniform(0.5, 0.95)
            
            # ì†Œê·œëª¨ ì£¼ë¬¸ ë¹„ìœ¨
            small_order_ratio = np.random.uniform(0.6, 0.95)
            
            # í˜¸ê°€ ì—…ë°ì´íŠ¸ ë¹ˆë„
            quote_update_freq = np.random.uniform(20, 150)
            
            # HFT Activity Score ê³„ì‚°
            hft_components = [
                min(message_rate / self.hft_config['message_rate_threshold'], 1.0),
                cancellation_ratio,
                small_order_ratio,
                min(quote_update_freq / self.hft_config['quote_update_frequency_threshold'], 1.0)
            ]
            
            hft_activity_score = np.mean(hft_components)
            
            # HFT íƒì§€
            hft_detected = hft_activity_score > self.thresholds['hft_activity_high']
            
            # í™œë™ ìˆ˜ì¤€
            if hft_activity_score > 0.75:
                activity_level = 'VERY_HIGH'
            elif hft_activity_score > 0.55:
                activity_level = 'HIGH'
            elif hft_activity_score > 0.35:
                activity_level = 'MODERATE'
            else:
                activity_level = 'LOW'
            
            result = {
                'hft_activity_score': hft_activity_score,
                'hft_detected': hft_detected,
                'message_rate': message_rate,
                'cancellation_ratio': cancellation_ratio,
                'small_order_ratio': small_order_ratio,
                'activity_level': activity_level,
                'timestamp': datetime.now()
            }
            
            self.hft_activity_history.append(result)
            
            return result

        except Exception as e:
            self.logger.error(f"HFT detection error: {e}")
            return {
                'hft_activity_score': 0.5,
                'hft_detected': False,
                'message_rate': 50.0,
                'activity_level': 'MODERATE'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7ï¸âƒ£ Order Book Pressure Analysis
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_orderbook_pressure(self, symbol='BTCUSDT'):
        """
        í˜¸ê°€ì°½ ì••ë ¥ ë¶„ì„
        
        ë§¤ìˆ˜/ë§¤ë„ í˜¸ê°€ì˜ í¬ê¸° ë° ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¨ê¸° ê°€ê²© ë°©í–¥ ì˜ˆì¸¡
        
        Returns:
            dict: {
                'buy_pressure': float,
                'sell_pressure': float,
                'net_pressure': float,
                'pressure_signal': str,
                'depth_imbalance': float
            }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ í˜¸ê°€ì°½ ë°ì´í„° ì‚¬ìš©
            
            levels = self.orderbook_config['levels']
            
            # ë§¤ìˆ˜ í˜¸ê°€ (bid) ì••ë ¥
            buy_volumes = [np.random.uniform(0.5, 10.0) * (1 / (i + 1)) for i in range(levels)]
            total_buy_volume = sum(buy_volumes)
            
            # ë§¤ë„ í˜¸ê°€ (ask) ì••ë ¥
            sell_volumes = [np.random.uniform(0.5, 10.0) * (1 / (i + 1)) for i in range(levels)]
            total_sell_volume = sum(sell_volumes)
            
            # ê°€ì¤‘ ì••ë ¥ (ê°€ê¹Œìš´ í˜¸ê°€ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            weights = [1 / (i + 1) for i in range(levels)]
            weighted_buy_pressure = np.average(buy_volumes, weights=weights)
            weighted_sell_pressure = np.average(sell_volumes, weights=weights)
            
            # ìˆœ ì••ë ¥
            net_pressure = (total_buy_volume - total_sell_volume) / (total_buy_volume + total_sell_volume)
            
            # Depth Imbalance
            depth_imbalance = net_pressure
            
            # ì••ë ¥ ì‹ í˜¸
            if net_pressure > 0.3:
                pressure_signal = 'STRONG_BUY_PRESSURE'
            elif net_pressure > 0.1:
                pressure_signal = 'BUY_PRESSURE'
            elif net_pressure < -0.3:
                pressure_signal = 'STRONG_SELL_PRESSURE'
            elif net_pressure < -0.1:
                pressure_signal = 'SELL_PRESSURE'
            else:
                pressure_signal = 'BALANCED'
            
            result = {
                'buy_pressure': total_buy_volume,
                'sell_pressure': total_sell_volume,
                'net_pressure': net_pressure,
                'pressure_signal': pressure_signal,
                'depth_imbalance': depth_imbalance,
                'weighted_buy_pressure': weighted_buy_pressure,
                'weighted_sell_pressure': weighted_sell_pressure,
                'timestamp': datetime.now()
            }
            
            return result

        except Exception as e:
            self.logger.error(f"Orderbook pressure analysis error: {e}")
            return {
                'buy_pressure': 0.0,
                'sell_pressure': 0.0,
                'net_pressure': 0.0,
                'pressure_signal': 'BALANCED',
                'depth_imbalance': 0.0
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 8ï¸âƒ£ Market Depth Resilience
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_depth_resilience(self, symbol='BTCUSDT'):
        """
        ì‹œì¥ ê¹Šì´ íšŒë³µë ¥ ë¶„ì„
        
        ëŒ€ëŸ‰ ê±°ë˜ í›„ í˜¸ê°€ì°½ì´ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ íšŒë³µë˜ëŠ”ì§€ ì¸¡ì •
        
        Returns:
            dict: {
                'resilience_score': float (0.0 ~ 1.0),
                'recovery_time_seconds': float,
                'resilience_level': str
            }
        """
        try:
            # ğŸ”¥ ì‹¤ì œ êµ¬í˜„ ì‹œ í˜¸ê°€ì°½ ë³€í™” ì¶”ì 
            
            # íšŒë³µ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ì´ˆ)
            recovery_time = np.random.uniform(1, 30)
            
            # Resilience Score
            # ë¹ ë¥¸ íšŒë³µ = ë†’ì€ ì ìˆ˜
            max_recovery_time = 30  # 30ì´ˆ
            resilience_score = 1.0 - min(recovery_time / max_recovery_time, 1.0)
            
            # ìµœê·¼ ê±°ë˜ëŸ‰ ë³€ë™ì„± ê³ ë ¤
            volume_volatility = np.random.uniform(0.1, 0.5)
            resilience_score *= (1.0 - volume_volatility * 0.5)
            
            resilience_score = np.clip(resilience_score, 0.0, 1.0)
            
            # íšŒë³µë ¥ ìˆ˜ì¤€
            if resilience_score > 0.7:
                resilience_level = 'HIGH'
            elif resilience_score > 0.4:
                resilience_level = 'MODERATE'
            else:
                resilience_level = 'LOW'
            
            result = {
                'resilience_score': resilience_score,
                'recovery_time_seconds': recovery_time,
                'resilience_level': resilience_level,
                'timestamp': datetime.now()
            }
            
            self.depth_resilience_history.append(result)
            
            return result

        except Exception as e:
            self.logger.error(f"Depth resilience analysis error: {e}")
            return {
                'resilience_score': 0.5,
                'recovery_time_seconds': 15.0,
                'resilience_level': 'MODERATE'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 9ï¸âƒ£ ì¢…í•© ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ì‹ í˜¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_comprehensive_microstructure_signal(self, symbol='BTCUSDT'):
        """
        ëª¨ë“  ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ì§€í‘œë¥¼ ì¢…í•©í•œ ì‹ í˜¸
        
        Returns:
            dict: {
                'microstructure_score': float (-1.0 ~ 1.0),
                'signal': str,
                'confidence': float,
                'components': dict,
                'trading_recommendation': str
            }
        """
        try:
            # ëª¨ë“  ì§€í‘œ ìˆ˜ì§‘
            ofi = self.calculate_order_flow_imbalance(symbol)
            vpin = self.calculate_vpin(symbol)
            trade_class = self.classify_trades(symbol)
            spreads = self.analyze_spreads(symbol)
            toxicity = self.analyze_quote_toxicity(symbol)
            hft = self.detect_hft_activity(symbol)
            ob_pressure = self.analyze_orderbook_pressure(symbol)
            resilience = self.analyze_depth_resilience(symbol)
            
            # ê° ì§€í‘œì˜ ì ìˆ˜í™” (-1.0 ~ 1.0)
            scores = {}
            
            # 1. OFI ì ìˆ˜
            scores['ofi'] = ofi['ofi']
            
            # 2. VPIN ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë¶€ì •ì  - ì—­ì„ íƒ ìœ„í—˜)
            scores['vpin'] = -(vpin['vpin'] - 0.5) * 2  # 0.5ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì—­ë³€í™˜
            
            # 3. Trade Classification ì ìˆ˜
            scores['trade_class'] = trade_class['trade_aggression_index']
            
            # 4. Spread ì ìˆ˜ (ë‚®ì€ adverse selection = ê¸ì •ì )
            if spreads['adverse_selection_component'] < self.thresholds['adverse_selection_high']:
                scores['spreads'] = 0.5
            else:
                scores['spreads'] = -0.5
            
            # 5. Toxicity ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            scores['toxicity'] = -(toxicity['toxicity_score'] - 0.5) * 2
            
            # 6. HFT ì ìˆ˜ (moderate HFTëŠ” ìœ ë™ì„± ì œê³µ, extremeì€ ë¶€ì •ì )
            if 0.3 < hft['hft_activity_score'] < 0.7:
                scores['hft'] = 0.3
            else:
                scores['hft'] = -0.2
            
            # 7. Order Book Pressure ì ìˆ˜
            scores['ob_pressure'] = ob_pressure['net_pressure']
            
            # 8. Resilience ì ìˆ˜
            scores['resilience'] = (resilience['resilience_score'] - 0.5) * 2
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weights = {
                'ofi': 0.20,
                'vpin': 0.15,
                'trade_class': 0.18,
                'spreads': 0.12,
                'toxicity': 0.10,
                'hft': 0.05,
                'ob_pressure': 0.15,
                'resilience': 0.05
            }
            
            microstructure_score = sum(scores[k] * weights[k] for k in scores)
            microstructure_score = np.clip(microstructure_score, -1.0, 1.0)
            
            # ì‹ í˜¸ ìƒì„±
            if microstructure_score > 0.5:
                signal = 'STRONG_BUY'
                trading_recommendation = 'ê°•ë ¥í•œ ë§¤ìˆ˜ ì‹ í˜¸ - ê³µê²©ì  ì§„ì… ê°€ëŠ¥'
            elif microstructure_score > 0.2:
                signal = 'BUY'
                trading_recommendation = 'ë§¤ìˆ˜ ì‹ í˜¸ - ì ì§„ì  ì§„ì… ê¶Œì¥'
            elif microstructure_score < -0.5:
                signal = 'STRONG_SELL'
                trading_recommendation = 'ê°•ë ¥í•œ ë§¤ë„ ì‹ í˜¸ - í¬ì§€ì…˜ ì²­ì‚° ê³ ë ¤'
            elif microstructure_score < -0.2:
                signal = 'SELL'
                trading_recommendation = 'ë§¤ë„ ì‹ í˜¸ - í¬ì§€ì…˜ ì¶•ì†Œ ê¶Œì¥'
            else:
                signal = 'NEUTRAL'
                trading_recommendation = 'ì¤‘ë¦½ - ê´€ë§ ê¶Œì¥'
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_microstructure_confidence(
                ofi, vpin, trade_class, spreads, toxicity, hft, ob_pressure, resilience
            )
            
            return {
                'microstructure_score': microstructure_score,
                'signal': signal,
                'confidence': confidence,
                'components': {
                    'ofi': ofi,
                    'vpin': vpin,
                    'trade_classification': trade_class,
                    'spreads': spreads,
                    'toxicity': toxicity,
                    'hft_activity': hft,
                    'orderbook_pressure': ob_pressure,
                    'depth_resilience': resilience
                },
                'component_scores': scores,
                'trading_recommendation': trading_recommendation,
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Comprehensive microstructure signal error: {e}")
            return {
                'microstructure_score': 0.0,
                'signal': 'NEUTRAL',
                'confidence': 0.5,
                'components': {},
                'trading_recommendation': 'ë°ì´í„° ë¶€ì¡± - ê±°ë˜ ë³´ë¥˜'
            }

    def _calculate_microstructure_confidence(self, ofi, vpin, trade_class, spreads, 
                                           toxicity, hft, ob_pressure, resilience):
        """ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ì‹ í˜¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            confidence_factors = []
            
            # 1. OFI ê°•ë„
            if ofi['imbalance_strength'] in ['STRONG', 'EXTREME']:
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.5)
            
            # 2. VPIN ìˆ˜ì¤€ (ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°)
            confidence_factors.append(1.0 - vpin['vpin'])
            
            # 3. Trade Classification ëª…í™•ì„±
            if abs(trade_class['trade_aggression_index']) > 0.3:
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.5)
            
            # 4. Spread ì•ˆì •ì„±
            if spreads['adverse_selection_component'] < 0.5:
                confidence_factors.append(0.7)
            else:
                confidence_factors.append(0.4)
            
            # 5. ë‚®ì€ ë…ì„± = ë†’ì€ ì‹ ë¢°
            confidence_factors.append(1.0 - toxicity['toxicity_score'])
            
            # 6. Moderate HFT = ìœ ë™ì„± = ë†’ì€ ì‹ ë¢°
            if 0.3 < hft['hft_activity_score'] < 0.7:
                confidence_factors.append(0.7)
            else:
                confidence_factors.append(0.5)
            
            # 7. Order Book Pressure ëª…í™•ì„±
            if abs(ob_pressure['net_pressure']) > 0.3:
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.5)
            
            # 8. ë†’ì€ Resilience = ë†’ì€ ì‹ ë¢°
            confidence_factors.append(resilience['resilience_score'])
            
            # í‰ê·  ì‹ ë¢°ë„
            confidence = np.mean(confidence_factors)
            confidence = np.clip(confidence, 0.0, 1.0)
            
            return confidence

        except Exception as e:
            self.logger.debug(f"Microstructure confidence calculation error: {e}")
            return 0.5

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”Ÿ ì¢…í•© ë¦¬í¬íŠ¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_comprehensive_microstructure_report(self, symbol='BTCUSDT'):
        """
        ë§ˆì¼“ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ì¢…í•© ë¦¬í¬íŠ¸
        
        Returns:
            dict: ëª¨ë“  ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ë¶„ì„ ê²°ê³¼
        """
        try:
            # ì¢…í•© ì‹ í˜¸ ìƒì„±
            comprehensive_signal = self.get_comprehensive_microstructure_signal(symbol)
            
            return {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'comprehensive_signal': comprehensive_signal,
                'historical_stats': {
                    'ofi_history_length': len(self.ofi_history),
                    'vpin_history_length': len(self.vpin_history),
                    'recent_ofi_avg': np.mean([h['ofi'] for h in list(self.ofi_history)[-10:]]) if len(self.ofi_history) > 0 else 0.0,
                    'recent_vpin_avg': np.mean([h['vpin'] for h in list(self.vpin_history)[-10:]]) if len(self.vpin_history) > 0 else 0.5
                }
            }

        except Exception as e:
            self.logger.error(f"Comprehensive microstructure report error: {e}")
            return {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ 4ï¸âƒ£ ë‹¤ì°¨ì› Regime Confidence Scoring (ë¶„ì‚° ê¸°ë°˜) ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MultiDimensionalConfidenceScorer:
    """
    ğŸ¯ ë‹¤ì°¨ì› Regime Confidence Scoring ì‹œìŠ¤í…œ
    - ì§€í‘œ ì¼ì¹˜ë„ ë¶„ì„ (Indicator Agreement)
    - ì‹œê³„ì—´ ì•ˆì •ì„± ë¶„ì„ (Temporal Stability)
    - ë¶„ì‚° ê¸°ë°˜ ì‹ ë¢°ë„ (Variance-Based Confidence)
    - í†µê³„ì  ì‹ ë¢° êµ¬ê°„ (Statistical Confidence Interval)
    - ì•™ìƒë¸” ì‹ ë¢°ë„ (Ensemble Confidence)
    - ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” (Uncertainty Quantification)
    """

    def __init__(self):
        self.logger = get_logger("ConfidenceScorer")

        # ğŸ“Š íˆìŠ¤í† ë¦¬ ë°ì´í„° ì €ì¥
        self.regime_score_history = deque(maxlen=100)  # Regime ì ìˆ˜ íˆìŠ¤í† ë¦¬
        self.indicator_history = deque(maxlen=100)  # ì§€í‘œ íˆìŠ¤í† ë¦¬
        self.confidence_history = deque(maxlen=100)  # ì‹ ë¢°ë„ íˆìŠ¤í† ë¦¬

        # ğŸšï¸ ì‹ ë¢°ë„ ë ˆë²¨ ì„ê³„ê°’
        self.confidence_levels = {
            'very_high': 0.85,
            'high': 0.70,
            'medium': 0.55,
            'low': 0.40,
            'very_low': 0.25
        }

        # ğŸ¯ ì•™ìƒë¸” íŒŒë¼ë¯¸í„°
        self.ensemble_config = {
            'bootstrap_samples': 50,  # Bootstrap ìƒ˜í”Œ ìˆ˜
            'monte_carlo_iterations': 100,  # ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ ë°˜ë³µ
            'confidence_interval': 0.95  # 95% ì‹ ë¢°êµ¬ê°„
        }

        # ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'prediction_accuracy': deque(maxlen=50),
            'false_positive_rate': deque(maxlen=50),
            'false_negative_rate': deque(maxlen=50)
        }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1ï¸âƒ£ ì§€í‘œ ì¼ì¹˜ë„ ë¶„ì„ (Indicator Agreement Analysis)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_indicator_agreement(self, indicators, regime):
        """
        ì—¬ëŸ¬ ì§€í‘œê°€ íŠ¹ì • regimeê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ë¶„ì„

        Returns:
            dict: {
                'agreement_score': float (0.0 ~ 1.0),
                'agreeing_indicators': list,
                'disagreeing_indicators': list,
                'neutral_indicators': list
            }
        """
        try:
            agreeing = []
            disagreeing = []
            neutral = []

            # Regimeì— ëŒ€í•œ ê° ì§€í‘œì˜ ì¼ì¹˜ë„ í‰ê°€

            # 1ï¸âƒ£ Trend ì¼ì¹˜ë„
            trend = indicators.get('trend', 'sideways')
            if 'BULL' in regime:
                if trend in ['uptrend', 'strong_uptrend']:
                    agreeing.append(('trend', 1.0))
                elif trend == 'sideways':
                    neutral.append(('trend', 0.5))
                else:
                    disagreeing.append(('trend', -1.0))

            elif 'BEAR' in regime:
                if trend in ['downtrend', 'strong_downtrend']:
                    agreeing.append(('trend', 1.0))
                elif trend == 'sideways':
                    neutral.append(('trend', 0.5))
                else:
                    disagreeing.append(('trend', -1.0))

            elif 'SIDEWAYS' in regime or regime in ['ACCUMULATION', 'DISTRIBUTION']:
                if trend == 'sideways':
                    agreeing.append(('trend', 1.0))
                else:
                    disagreeing.append(('trend', -0.5))

            # 2ï¸âƒ£ Volatility ì¼ì¹˜ë„
            volatility = indicators.get('volatility', 'medium')
            if 'VOLATILITY' in regime or 'CHOP' in regime:
                if volatility in ['high', 'extreme']:
                    agreeing.append(('volatility', 1.0))
                elif volatility == 'medium':
                    neutral.append(('volatility', 0.5))
                else:
                    disagreeing.append(('volatility', -1.0))

            elif 'CONSOLIDATION' in regime or 'COMPRESSION' in regime:
                if volatility in ['low', 'medium']:
                    agreeing.append(('volatility', 1.0))
                else:
                    disagreeing.append(('volatility', -1.0))

            # 3ï¸âƒ£ Momentum ì¼ì¹˜ë„
            momentum = indicators.get('momentum', 'neutral')
            if 'BULL' in regime:
                if momentum in ['bullish', 'overbought']:
                    agreeing.append(('momentum', 1.0))
                elif momentum == 'neutral':
                    neutral.append(('momentum', 0.5))
                else:
                    disagreeing.append(('momentum', -1.0))

            elif 'BEAR' in regime:
                if momentum in ['bearish', 'oversold']:
                    agreeing.append(('momentum', 1.0))
                elif momentum == 'neutral':
                    neutral.append(('momentum', 0.5))
                else:
                    disagreeing.append(('momentum', -1.0))

            # 4ï¸âƒ£ Volume ì¼ì¹˜ë„
            volume = indicators.get('volume', 'normal')
            if regime in ['ACCUMULATION', 'DISTRIBUTION']:
                if volume in ['high', 'normal']:
                    agreeing.append(('volume', 0.8))
                else:
                    disagreeing.append(('volume', -0.6))

            # 5ï¸âƒ£ Breadth ì¼ì¹˜ë„
            breadth = indicators.get('breadth', 0.5)
            if 'BULL' in regime or regime == 'ACCUMULATION':
                if breadth > 0.6:
                    agreeing.append(('breadth', 1.0))
                elif breadth > 0.4:
                    neutral.append(('breadth', 0.5))
                else:
                    disagreeing.append(('breadth', -1.0))

            elif 'BEAR' in regime or regime == 'DISTRIBUTION':
                if breadth < 0.4:
                    agreeing.append(('breadth', 1.0))
                elif breadth < 0.6:
                    neutral.append(('breadth', 0.5))
                else:
                    disagreeing.append(('breadth', -1.0))

            # 6ï¸âƒ£ ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ì‹ í˜¸ ì¼ì¹˜ë„
            onchain_macro_signals = indicators.get('onchain_macro_signals')
            if onchain_macro_signals:
                merged_signal = onchain_macro_signals['merged']
                merged_score = merged_signal['score']

                if 'BULL' in regime or regime == 'ACCUMULATION':
                    if merged_score > 0.3:
                        agreeing.append(('onchain_macro', merged_score))
                    elif merged_score > -0.3:
                        neutral.append(('onchain_macro', 0.5))
                    else:
                        disagreeing.append(('onchain_macro', merged_score))

                elif 'BEAR' in regime or regime == 'DISTRIBUTION':
                    if merged_score < -0.3:
                        agreeing.append(('onchain_macro', abs(merged_score)))
                    elif merged_score < 0.3:
                        neutral.append(('onchain_macro', 0.5))
                    else:
                        disagreeing.append(('onchain_macro', -merged_score))

            # ğŸ”¥ 7ï¸âƒ£ ìœ ë™ì„± ì‹ í˜¸ ì¼ì¹˜ë„
            liquidity_signals = indicators.get('liquidity_signals')
            if liquidity_signals:
                liquidity_regime = liquidity_signals.get('regime', 'MEDIUM_LIQUIDITY')
                
                # ìœ ë™ì„± ì²´ì œì™€ ì‹œì¥ ì²´ì œì˜ ì¼ì¹˜ë„
                if 'HIGH' in liquidity_regime:
                    # ë†’ì€ ìœ ë™ì„±ì€ ì¼ë°˜ì ìœ¼ë¡œ ê¸ì •ì 
                    agreeing.append(('liquidity', 0.6))
                elif 'LOW' in liquidity_regime or 'VERY_LOW' in liquidity_regime:
                    # ë‚®ì€ ìœ ë™ì„±ì€ ìœ„í—˜ ì‹ í˜¸
                    if 'VOLATILITY' in regime or 'CHOP' in regime:
                        agreeing.append(('liquidity', 0.8))  # ë‚®ì€ ìœ ë™ì„± + ë³€ë™ì„± = ì¼ì¹˜
                    else:
                        disagreeing.append(('liquidity', -0.7))

            # ì¼ì¹˜ë„ ì ìˆ˜ ê³„ì‚°
            total_indicators = len(agreeing) + len(disagreeing) + len(neutral)
            if total_indicators == 0:
                agreement_score = 0.5
            else:
                agree_weight = sum(score for _, score in agreeing)
                disagree_weight = sum(abs(score) for _, score in disagreeing)
                neutral_weight = sum(score for _, score in neutral) * 0.5

                # ì •ê·œí™”ëœ ì¼ì¹˜ë„ ì ìˆ˜
                agreement_score = (agree_weight + neutral_weight) / (
                            agree_weight + disagree_weight + neutral_weight + 1e-6)
                agreement_score = np.clip(agreement_score, 0.0, 1.0)

            return {
                'agreement_score': agreement_score,
                'agreeing_indicators': [name for name, _ in agreeing],
                'disagreeing_indicators': [name for name, _ in disagreeing],
                'neutral_indicators': [name for name, _ in neutral],
                'total_indicators': total_indicators,
                'agree_count': len(agreeing),
                'disagree_count': len(disagreeing),
                'neutral_count': len(neutral)
            }

        except Exception as e:
            self.logger.error(f"Indicator agreement calculation error: {e}")
            return {
                'agreement_score': 0.5,
                'agreeing_indicators': [],
                'disagreeing_indicators': [],
                'neutral_indicators': [],
                'total_indicators': 0,
                'agree_count': 0,
                'disagree_count': 0,
                'neutral_count': 0
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2ï¸âƒ£ ì‹œê³„ì—´ ì•ˆì •ì„± ë¶„ì„ (Temporal Stability Analysis)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_temporal_stability(self, window=20):
        """
        ìµœê·¼ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì•ˆì •ì„± ë¶„ì„

        Args:
            window: ë¶„ì„í•  íˆìŠ¤í† ë¦¬ ìœˆë„ìš° í¬ê¸°

        Returns:
            dict: {
                'stability_score': float (0.0 ~ 1.0),
                'regime_consistency': float,
                'score_volatility': float,
                'trend_direction': str
            }
        """
        try:
            if len(self.regime_score_history) < 5:
                return {
                    'stability_score': 0.7,
                    'regime_consistency': 0.7,
                    'score_volatility': 0.0,
                    'trend_direction': 'stable'
                }

            # ìµœê·¼ ë°ì´í„° ì¶”ì¶œ
            recent_data = list(self.regime_score_history)[-min(window, len(self.regime_score_history)):]
            recent_regimes = [d['regime'] for d in recent_data]
            recent_scores = [d['score'] for d in recent_data]

            # 1ï¸âƒ£ Regime ì¼ê´€ì„± (ê°™ì€ regimeì´ ìœ ì§€ë˜ëŠ” ë¹„ìœ¨)
            if len(recent_regimes) > 0:
                most_common_regime = max(set(recent_regimes), key=recent_regimes.count)
                regime_consistency = recent_regimes.count(most_common_regime) / len(recent_regimes)
            else:
                regime_consistency = 1.0

            # 2ï¸âƒ£ ì ìˆ˜ ë³€ë™ì„± (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )
            if len(recent_scores) > 1:
                score_std = np.std(recent_scores)
                score_mean = np.mean(recent_scores)
                # ë³€ë™ê³„ìˆ˜ (Coefficient of Variation)
                cv = score_std / (abs(score_mean) + 1e-6)
                score_volatility = min(cv, 1.0)
            else:
                score_volatility = 0.0

            # 3ï¸âƒ£ ì¶”ì„¸ ë°©í–¥ì„±
            if len(recent_scores) > 3:
                # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ íŒŒì•…
                x = np.arange(len(recent_scores))
                slope, _ = np.polyfit(x, recent_scores, 1)

                if slope > 0.05:
                    trend_direction = 'strengthening'
                elif slope < -0.05:
                    trend_direction = 'weakening'
                else:
                    trend_direction = 'stable'
            else:
                trend_direction = 'stable'

            # 4ï¸âƒ£ ì¢…í•© ì•ˆì •ì„± ì ìˆ˜
            # ë†’ì€ ì¼ê´€ì„± + ë‚®ì€ ë³€ë™ì„± = ë†’ì€ ì•ˆì •ì„±
            stability_score = (
                    regime_consistency * 0.6 +  # Regime ì¼ê´€ì„± (60%)
                    (1.0 - score_volatility) * 0.4  # ì ìˆ˜ ì•ˆì •ì„± (40%)
            )
            stability_score = np.clip(stability_score, 0.0, 1.0)

            return {
                'stability_score': stability_score,
                'regime_consistency': regime_consistency,
                'score_volatility': score_volatility,
                'trend_direction': trend_direction
            }

        except Exception as e:
            self.logger.error(f"Temporal stability calculation error: {e}")
            return {
                'stability_score': 0.7,
                'regime_consistency': 0.7,
                'score_volatility': 0.0,
                'trend_direction': 'stable'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3ï¸âƒ£ ë¶„ì‚° ê¸°ë°˜ ì‹ ë¢°ë„ (Variance-Based Confidence)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_variance_based_confidence(self, regime_scores_dict):
        """
        ì—¬ëŸ¬ regime ì ìˆ˜ì˜ ë¶„ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°

        Args:
            regime_scores_dict: {'REGIME_NAME': score, ...}

        Returns:
            dict: {
                'variance_confidence': float (0.0 ~ 1.0),
                'score_spread': float,
                'dominant_regime_margin': float,
                'entropy': float
            }
        """
        try:
            if not regime_scores_dict or len(regime_scores_dict) == 0:
                return {
                    'variance_confidence': 0.5,
                    'score_spread': 0.0,
                    'dominant_regime_margin': 0.0,
                    'entropy': 0.0
                }

            scores = np.array(list(regime_scores_dict.values()))
            scores = np.clip(scores, 0.0, 10.0)  # ì´ìƒì¹˜ ì œê±°

            # 1ï¸âƒ£ ì ìˆ˜ ë¶„ì‚° (ë‚®ì„ìˆ˜ë¡ ëª…í™•í•¨)
            score_variance = np.var(scores)
            score_std = np.std(scores)
            score_range = np.max(scores) - np.min(scores)

            # 2ï¸âƒ£ ìµœê³  ì ìˆ˜ì™€ 2ë“± ì ìˆ˜ì˜ ì°¨ì´ (í´ìˆ˜ë¡ ëª…í™•í•¨)
            sorted_scores = np.sort(scores)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ
            if len(sorted_scores) >= 2:
                dominant_margin = sorted_scores[0] - sorted_scores[1]
            else:
                dominant_margin = sorted_scores[0] if len(sorted_scores) > 0 else 0.0

            # 3ï¸âƒ£ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ í™•ì‹¤í•¨)
            # ì ìˆ˜ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
            if np.sum(scores) > 0:
                prob_dist = scores / np.sum(scores)
                score_entropy = entropy(prob_dist + 1e-10)  # ë¡œê·¸(0) ë°©ì§€
                # ì •ê·œí™” (ìµœëŒ€ ì—”íŠ¸ë¡œí”¼ëŠ” log(N))
                max_entropy = np.log(len(scores)) if len(scores) > 1 else 1.0
                normalized_entropy = score_entropy / max_entropy if max_entropy > 0 else 0.0
            else:
                normalized_entropy = 1.0

            # 4ï¸âƒ£ ì¢…í•© ë¶„ì‚° ê¸°ë°˜ ì‹ ë¢°ë„
            # ë†’ì€ ë§ˆì§„ + ë‚®ì€ ë¶„ì‚° + ë‚®ì€ ì—”íŠ¸ë¡œí”¼ = ë†’ì€ ì‹ ë¢°ë„
            variance_confidence = (
                    min(dominant_margin / 2.0, 1.0) * 0.5 +  # ìš°ì„¸ ë§ˆì§„ (50%)
                    (1.0 - min(score_std / 2.0, 1.0)) * 0.3 +  # ë‚®ì€ í‘œì¤€í¸ì°¨ (30%)
                    (1.0 - normalized_entropy) * 0.2  # ë‚®ì€ ì—”íŠ¸ë¡œí”¼ (20%)
            )
            variance_confidence = np.clip(variance_confidence, 0.0, 1.0)

            return {
                'variance_confidence': variance_confidence,
                'score_spread': score_range,
                'dominant_regime_margin': dominant_margin,
                'entropy': normalized_entropy,
                'score_std': score_std
            }

        except Exception as e:
            self.logger.error(f"Variance-based confidence calculation error: {e}")
            return {
                'variance_confidence': 0.5,
                'score_spread': 0.0,
                'dominant_regime_margin': 0.0,
                'entropy': 0.0,
                'score_std': 0.0
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4ï¸âƒ£ í†µê³„ì  ì‹ ë¢° êµ¬ê°„ (Statistical Confidence Interval)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_statistical_confidence_interval(self, window=30):
        """
        Bootstrap ë°©ë²•ì„ ì‚¬ìš©í•œ í†µê³„ì  ì‹ ë¢° êµ¬ê°„ ê³„ì‚°

        Args:
            window: ë¶„ì„í•  íˆìŠ¤í† ë¦¬ ìœˆë„ìš° í¬ê¸°

        Returns:
            dict: {
                'mean_score': float,
                'confidence_interval': tuple (lower, upper),
                'interval_width': float,
                'statistical_confidence': float (0.0 ~ 1.0)
            }
        """
        try:
            if len(self.regime_score_history) < 10:
                return {
                    'mean_score': 0.0,
                    'confidence_interval': (0.0, 0.0),
                    'interval_width': 0.0,
                    'statistical_confidence': 0.6
                }

            # ìµœê·¼ ë°ì´í„° ì¶”ì¶œ
            recent_scores = [d['score'] for d in
                             list(self.regime_score_history)[-min(window, len(self.regime_score_history)):]]
            recent_scores = np.array(recent_scores)

            # Bootstrap ìƒ˜í”Œë§
            bootstrap_means = []
            n_samples = len(recent_scores)

            for _ in range(self.ensemble_config['bootstrap_samples']):
                # ë³µì› ì¶”ì¶œ
                bootstrap_sample = np.random.choice(recent_scores, size=n_samples, replace=True)
                bootstrap_means.append(np.mean(bootstrap_sample))

            bootstrap_means = np.array(bootstrap_means)

            # ì‹ ë¢° êµ¬ê°„ ê³„ì‚° (95%)
            alpha = 1 - self.ensemble_config['confidence_interval']
            lower_percentile = (alpha / 2) * 100
            upper_percentile = (1 - alpha / 2) * 100

            ci_lower = np.percentile(bootstrap_means, lower_percentile)
            ci_upper = np.percentile(bootstrap_means, upper_percentile)
            ci_width = ci_upper - ci_lower

            mean_score = np.mean(recent_scores)

            # ì‹ ë¢°ë„ ê³„ì‚° (êµ¬ê°„ì´ ì¢ì„ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„)
            # êµ¬ê°„ í­ì„ ì ìˆ˜ ë²”ìœ„(-1 ~ 1)ë¡œ ì •ê·œí™”
            normalized_width = ci_width / 2.0  # ìµœëŒ€ í­ = 2.0
            statistical_confidence = 1.0 - min(normalized_width, 1.0)
            statistical_confidence = np.clip(statistical_confidence, 0.0, 1.0)

            return {
                'mean_score': float(mean_score),
                'confidence_interval': (float(ci_lower), float(ci_upper)),
                'interval_width': float(ci_width),
                'statistical_confidence': float(statistical_confidence)
            }

        except Exception as e:
            self.logger.error(f"Statistical confidence interval calculation error: {e}")
            return {
                'mean_score': 0.0,
                'confidence_interval': (0.0, 0.0),
                'interval_width': 0.0,
                'statistical_confidence': 0.6
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5ï¸âƒ£ ì•™ìƒë¸” ì‹ ë¢°ë„ (Ensemble Confidence)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_ensemble_confidence(self, indicators, regime_scores_dict):
        """
        ë‹¤ì¤‘ ì‹œê°„í”„ë ˆì„ ë° ë°©ë²•ë¡ ì„ ê²°í•©í•œ ì•™ìƒë¸” ì‹ ë¢°ë„

        Returns:
            dict: {
                'ensemble_confidence': float (0.0 ~ 1.0),
                'method_agreement': float,
                'robust_score': float
            }
        """
        try:
            # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê³„ì‚°ëœ ì‹ ë¢°ë„ ìˆ˜ì§‘
            confidence_scores = []

            # 1ï¸âƒ£ ì§€í‘œ ì¼ì¹˜ë„ ê¸°ë°˜
            agreement_result = self.calculate_indicator_agreement(
                indicators,
                max(regime_scores_dict, key=regime_scores_dict.get)
            )
            confidence_scores.append(agreement_result['agreement_score'])

            # 2ï¸âƒ£ ì‹œê³„ì—´ ì•ˆì •ì„± ê¸°ë°˜
            stability_result = self.calculate_temporal_stability()
            confidence_scores.append(stability_result['stability_score'])

            # 3ï¸âƒ£ ë¶„ì‚° ê¸°ë°˜
            variance_result = self.calculate_variance_based_confidence(regime_scores_dict)
            confidence_scores.append(variance_result['variance_confidence'])

            # 4ï¸âƒ£ í†µê³„ì  ì‹ ë¢° êµ¬ê°„ ê¸°ë°˜
            statistical_result = self.calculate_statistical_confidence_interval()
            confidence_scores.append(statistical_result['statistical_confidence'])

            # ë°©ë²• ê°„ ì¼ì¹˜ë„ ê³„ì‚°
            if len(confidence_scores) > 1:
                method_std = np.std(confidence_scores)
                method_agreement = 1.0 - min(method_std / 0.5, 1.0)  # í‘œì¤€í¸ì°¨ê°€ 0.5 ì´í•˜ë©´ ë†’ì€ ì¼ì¹˜ë„
            else:
                method_agreement = 1.0

            # ì•™ìƒë¸” ì‹ ë¢°ë„ = ê°€ì¤‘ í‰ê· 
            weights = [0.30, 0.25, 0.25, 0.20]  # ê° ë°©ë²•ì˜ ê°€ì¤‘ì¹˜
            ensemble_confidence = np.average(confidence_scores, weights=weights)

            # ë°©ë²• ê°„ ì¼ì¹˜ë„ê°€ ë‚®ìœ¼ë©´ í˜ë„í‹°
            if method_agreement < 0.7:
                ensemble_confidence *= 0.9

            # ë¡œë²„ìŠ¤íŠ¸ ì ìˆ˜ = ì¤‘ì•™ê°’ (ì´ìƒì¹˜ì— ê°•ê±´)
            robust_score = np.median(confidence_scores)

            ensemble_confidence = np.clip(ensemble_confidence, 0.0, 1.0)

            return {
                'ensemble_confidence': float(ensemble_confidence),
                'method_agreement': float(method_agreement),
                'robust_score': float(robust_score),
                'individual_scores': {
                    'agreement': confidence_scores[0],
                    'stability': confidence_scores[1],
                    'variance': confidence_scores[2],
                    'statistical': confidence_scores[3]
                }
            }

        except Exception as e:
            self.logger.error(f"Ensemble confidence calculation error: {e}")
            return {
                'ensemble_confidence': 0.6,
                'method_agreement': 0.7,
                'robust_score': 0.6,
                'individual_scores': {}
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6ï¸âƒ£ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” (Uncertainty Quantification)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def quantify_uncertainty(self, regime, regime_scores_dict, indicators):
        """
        ë² ì´ì§€ì•ˆ ë° ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ë²•ì„ ì‚¬ìš©í•œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”

        Returns:
            dict: {
                'uncertainty_score': float (0.0 ~ 1.0, ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
                'prediction_interval': tuple (lower, upper),
                'risk_level': str
            }
        """
        try:
            uncertainty_factors = []

            # 1ï¸âƒ£ ì ìˆ˜ ë¶„ì‚° ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
            if regime_scores_dict:
                scores = list(regime_scores_dict.values())
                score_std = np.std(scores)
                normalized_std = min(score_std / 2.0, 1.0)
                uncertainty_factors.append(normalized_std)

            # 2ï¸âƒ£ ì§€í‘œ ë¶ˆì¼ì¹˜ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
            agreement_result = self.calculate_indicator_agreement(indicators, regime)
            disagreement_ratio = agreement_result['disagree_count'] / max(agreement_result['total_indicators'], 1)
            uncertainty_factors.append(disagreement_ratio)

            # 3ï¸âƒ£ ì‹œê³„ì—´ ë¶ˆì•ˆì •ì„± ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
            stability_result = self.calculate_temporal_stability()
            instability = 1.0 - stability_result['stability_score']
            uncertainty_factors.append(instability)

            # 4ï¸âƒ£ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
            variance_result = self.calculate_variance_based_confidence(regime_scores_dict)
            entropy_uncertainty = variance_result['entropy']
            uncertainty_factors.append(entropy_uncertainty)

            # ì¢…í•© ë¶ˆí™•ì‹¤ì„± ì ìˆ˜
            uncertainty_score = np.mean(uncertainty_factors)
            uncertainty_score = np.clip(uncertainty_score, 0.0, 1.0)

            # ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì˜ˆì¸¡ êµ¬ê°„ ì¶”ì •
            if len(self.regime_score_history) >= 10:
                recent_scores = [d['score'] for d in list(self.regime_score_history)[-20:]]
                mean_score = np.mean(recent_scores)
                std_score = np.std(recent_scores)

                # ì •ê·œë¶„í¬ ê°€ì •í•˜ì— ì˜ˆì¸¡ êµ¬ê°„
                z_score = 1.96  # 95% ì‹ ë¢°êµ¬ê°„
                margin = z_score * std_score
                prediction_interval = (mean_score - margin, mean_score + margin)
            else:
                prediction_interval = (-1.0, 1.0)

            # ë¦¬ìŠ¤í¬ ë ˆë²¨ ê²°ì •
            if uncertainty_score < 0.3:
                risk_level = 'LOW'
            elif uncertainty_score < 0.5:
                risk_level = 'MEDIUM'
            elif uncertainty_score < 0.7:
                risk_level = 'HIGH'
            else:
                risk_level = 'VERY_HIGH'

            return {
                'uncertainty_score': float(uncertainty_score),
                'prediction_interval': prediction_interval,
                'risk_level': risk_level,
                'uncertainty_components': {
                    'score_variance': uncertainty_factors[0] if len(uncertainty_factors) > 0 else 0.0,
                    'indicator_disagreement': uncertainty_factors[1] if len(uncertainty_factors) > 1 else 0.0,
                    'temporal_instability': uncertainty_factors[2] if len(uncertainty_factors) > 2 else 0.0,
                    'entropy': uncertainty_factors[3] if len(uncertainty_factors) > 3 else 0.0
                }
            }

        except Exception as e:
            self.logger.error(f"Uncertainty quantification error: {e}")
            return {
                'uncertainty_score': 0.5,
                'prediction_interval': (-1.0, 1.0),
                'risk_level': 'MEDIUM',
                'uncertainty_components': {}
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7ï¸âƒ£ ì¢…í•© ë‹¤ì°¨ì› ì‹ ë¢°ë„ ê³„ì‚° (Comprehensive Multi-Dimensional Confidence)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_comprehensive_confidence(self, regime, regime_scores_dict, indicators):
        """
        ëª¨ë“  ì°¨ì›ì„ í†µí•©í•œ ì¢…í•© ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°

        Returns:
            dict: {
                'overall_confidence': float (0.0 ~ 1.0),
                'confidence_level': str,
                'detailed_scores': dict,
                'risk_assessment': dict
            }
        """
        try:
            # 1ï¸âƒ£ ì•™ìƒë¸” ì‹ ë¢°ë„
            ensemble_result = self.calculate_ensemble_confidence(indicators, regime_scores_dict)

            # 2ï¸âƒ£ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
            uncertainty_result = self.quantify_uncertainty(regime, regime_scores_dict, indicators)

            # 3ï¸âƒ£ ì§€í‘œ ì¼ì¹˜ë„
            agreement_result = self.calculate_indicator_agreement(indicators, regime)

            # 4ï¸âƒ£ ì‹œê³„ì—´ ì•ˆì •ì„±
            stability_result = self.calculate_temporal_stability()

            # 5ï¸âƒ£ ë¶„ì‚° ê¸°ë°˜ ì‹ ë¢°ë„
            variance_result = self.calculate_variance_based_confidence(regime_scores_dict)

            # 6ï¸âƒ£ í†µê³„ì  ì‹ ë¢°ë„
            statistical_result = self.calculate_statistical_confidence_interval()

            # ì¢…í•© ì‹ ë¢°ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            overall_confidence = (
                    ensemble_result['ensemble_confidence'] * 0.35 +  # ì•™ìƒë¸” (35%)
                    (1.0 - uncertainty_result['uncertainty_score']) * 0.25 +  # ë¶ˆí™•ì‹¤ì„± (25%, ì—­ìˆ˜)
                    agreement_result['agreement_score'] * 0.15 +  # ì§€í‘œ ì¼ì¹˜ë„ (15%)
                    stability_result['stability_score'] * 0.15 +  # ì‹œê³„ì—´ ì•ˆì •ì„± (15%)
                    variance_result['variance_confidence'] * 0.10  # ë¶„ì‚° ê¸°ë°˜ (10%)
            )
            overall_confidence = np.clip(overall_confidence, 0.0, 1.0)

            # ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •
            if overall_confidence >= self.confidence_levels['very_high']:
                confidence_level = 'VERY_HIGH'
            elif overall_confidence >= self.confidence_levels['high']:
                confidence_level = 'HIGH'
            elif overall_confidence >= self.confidence_levels['medium']:
                confidence_level = 'MEDIUM'
            elif overall_confidence >= self.confidence_levels['low']:
                confidence_level = 'LOW'
            else:
                confidence_level = 'VERY_LOW'

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.confidence_history.append({
                'timestamp': datetime.now(),
                'regime': regime,
                'confidence': overall_confidence,
                'level': confidence_level
            })

            # ìƒì„¸ ê²°ê³¼ ë°˜í™˜
            return {
                'overall_confidence': float(overall_confidence),
                'confidence_level': confidence_level,
                'confidence_percentage': float(overall_confidence * 100),
                'detailed_scores': {
                    'ensemble': ensemble_result,
                    'uncertainty': uncertainty_result,
                    'agreement': agreement_result,
                    'stability': stability_result,
                    'variance': variance_result,
                    'statistical': statistical_result
                },
                'risk_assessment': {
                    'risk_level': uncertainty_result['risk_level'],
                    'uncertainty_score': uncertainty_result['uncertainty_score'],
                    'prediction_interval': uncertainty_result['prediction_interval']
                },
                'key_insights': self._generate_confidence_insights(
                    overall_confidence,
                    ensemble_result,
                    uncertainty_result,
                    agreement_result
                )
            }

        except Exception as e:
            self.logger.error(f"Comprehensive confidence calculation error: {e}")
            return {
                'overall_confidence': 0.6,
                'confidence_level': 'MEDIUM',
                'confidence_percentage': 60.0,
                'detailed_scores': {},
                'risk_assessment': {},
                'key_insights': []
            }

    def _generate_confidence_insights(self, overall_confidence, ensemble_result,
                                      uncertainty_result, agreement_result):
        """ì‹ ë¢°ë„ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        try:
            # 1ï¸âƒ£ ì „ì²´ ì‹ ë¢°ë„ í‰ê°€
            if overall_confidence >= 0.85:
                insights.append("ğŸŸ¢ ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„ - ê°•ë ¥í•œ ì‹ í˜¸")
            elif overall_confidence >= 0.70:
                insights.append("ğŸŸ¡ ë†’ì€ ì‹ ë¢°ë„ - ì‹ ë¢° ê°€ëŠ¥í•œ ì‹ í˜¸")
            elif overall_confidence >= 0.55:
                insights.append("ğŸŸ  ì¤‘ê°„ ì‹ ë¢°ë„ - ì£¼ì˜ í•„ìš”")
            else:
                insights.append("ğŸ”´ ë‚®ì€ ì‹ ë¢°ë„ - ë¶ˆí™•ì‹¤í•œ ì‹ í˜¸")

            # 2ï¸âƒ£ ë°©ë²• ê°„ ì¼ì¹˜ë„
            if ensemble_result['method_agreement'] >= 0.8:
                insights.append("âœ… ëª¨ë“  ë¶„ì„ ë°©ë²•ì´ ì¼ì¹˜")
            elif ensemble_result['method_agreement'] < 0.6:
                insights.append("âš ï¸ ë¶„ì„ ë°©ë²• ê°„ ë¶ˆì¼ì¹˜ ì¡´ì¬")

            # 3ï¸âƒ£ ë¶ˆí™•ì‹¤ì„± í‰ê°€
            if uncertainty_result['risk_level'] == 'LOW':
                insights.append("ğŸ’ ë‚®ì€ ë¶ˆí™•ì‹¤ì„± - ì•ˆì •ì  ì˜ˆì¸¡")
            elif uncertainty_result['risk_level'] in ['HIGH', 'VERY_HIGH']:
                insights.append("âš¡ ë†’ì€ ë¶ˆí™•ì‹¤ì„± - ë³€ë™ì„± ì£¼ì˜")

            # 4ï¸âƒ£ ì§€í‘œ ì¼ì¹˜ë„
            agree_ratio = agreement_result['agree_count'] / max(agreement_result['total_indicators'], 1)
            if agree_ratio >= 0.75:
                insights.append("ğŸ¯ ì§€í‘œ ê°•í•œ ì¼ì¹˜")
            elif agreement_result['disagree_count'] >= agreement_result['agree_count']:
                insights.append("ğŸ”€ ì§€í‘œ ê°„ ë¶ˆì¼ì¹˜")

        except Exception as e:
            self.logger.debug(f"Insights generation error: {e}")

        return insights

    def update_history(self, regime, score, indicators):
        """íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        try:
            self.regime_score_history.append({
                'timestamp': datetime.now(),
                'regime': regime,
                'score': score
            })

            self.indicator_history.append({
                'timestamp': datetime.now(),
                'indicators': indicators.copy()
            })
        except Exception as e:
            self.logger.debug(f"History update error: {e}")

    def get_confidence_report(self):
        """ì‹ ë¢°ë„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            if len(self.confidence_history) == 0:
                return {}

            recent_confidences = [c['confidence'] for c in list(self.confidence_history)[-20:]]

            return {
                'current_confidence': self.confidence_history[-1]['confidence'] if len(
                    self.confidence_history) > 0 else 0.0,
                'average_confidence': np.mean(recent_confidences),
                'confidence_trend': 'increasing' if len(recent_confidences) > 1 and recent_confidences[-1] >
                                                    recent_confidences[0] else 'stable',
                'confidence_volatility': np.std(recent_confidences),
                'history_length': len(self.confidence_history)
            }
        except Exception as e:
            self.logger.error(f"Confidence report generation error: {e}")
            return {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ 5ï¸âƒ£ ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤ (Multi-Timeframe Consensus) ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MultiTimeframeConsensusEngine:
    """
    ğŸ¯ ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤ ì—”ì§„
    - ì—¬ëŸ¬ íƒ€ì„í”„ë ˆì„ì—ì„œ ë™ì‹œì— regime ë¶„ì„
    - íƒ€ì„í”„ë ˆì„ ê°„ ì¼ì¹˜ë„ ê³„ì‚°
    - ê³„ì¸µì  ì»¨ì„¼ì„œìŠ¤ (ì¥ê¸° > ë‹¨ê¸° ê°€ì¤‘ì¹˜)
    - íƒ€ì„í”„ë ˆì„ ê°„ ëª¨ìˆœ ê°ì§€ ë° í•´ê²°
    - ë‹¤ì¤‘ í•´ìƒë„ regime ë§µ ìƒì„±
    - Regime ì§„í™” ì˜ˆì¸¡
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("MTFConsensus")

        # ğŸ“Š ë¶„ì„í•  íƒ€ì„í”„ë ˆì„ ì •ì˜ (ì§§ì€ ê²ƒë¶€í„° ê¸´ ê²ƒê¹Œì§€)
        self.timeframes = ['5m', '15m', '1h', '4h', '1d']

        # ğŸ¯ íƒ€ì„í”„ë ˆì„ë³„ ê°€ì¤‘ì¹˜ (ì¥ê¸° íƒ€ì„í”„ë ˆì„ì´ ë” ì¤‘ìš”)
        self.timeframe_weights = {
            '5m': 0.10,   # ë‹¨ê¸° ë…¸ì´ì¦ˆ í•„í„°ë§
            '15m': 0.15,  # ë‹¨ê¸° ì¶”ì„¸
            '1h': 0.20,   # ì¤‘ê¸° ì¶”ì„¸
            '4h': 0.25,   # ì¥ê¸° ì¶”ì„¸
            '1d': 0.30    # ë§¤í¬ë¡œ ì¶”ì„¸ (ê°€ì¥ ì¤‘ìš”)
        }

        # ğŸ“¦ ìºì‹±
        self._cache = {}
        self._cache_ttl = 180  # 3ë¶„ ìºì‹œ

        # ğŸ“ˆ íˆìŠ¤í† ë¦¬
        self.consensus_history = deque(maxlen=100)
        self.divergence_history = deque(maxlen=50)
        self.alignment_history = deque(maxlen=50)

        # ğŸšï¸ ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'strong_consensus': 0.80,      # ê°•í•œ ì»¨ì„¼ì„œìŠ¤
            'moderate_consensus': 0.60,    # ì¤‘ê°„ ì»¨ì„¼ì„œìŠ¤
            'weak_consensus': 0.40,        # ì•½í•œ ì»¨ì„¼ì„œìŠ¤
            'divergence_critical': 0.30,   # ì‹¬ê°í•œ ë¶„ì‚°
            'alignment_excellent': 0.85,   # ìš°ìˆ˜í•œ ì •ë ¬
            'alignment_good': 0.70,        # ì¢‹ì€ ì •ë ¬
            'alignment_poor': 0.50         # ë‚®ì€ ì •ë ¬
        }

        # ğŸ”„ íƒ€ì„í”„ë ˆì„ ê°„ ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
        self.timeframe_relationships = self._build_timeframe_relationships()

        # ğŸ“Š íƒ€ì„í”„ë ˆì„ë³„ regime ìºì‹œ
        self.timeframe_regimes = {}

        # ğŸ¯ ê³„ì¸µì  êµ¬ì¡° (ë¶€ëª¨-ìì‹ ê´€ê³„)
        self.timeframe_hierarchy = {
            '1d': ['4h'],
            '4h': ['1h'],
            '1h': ['15m'],
            '15m': ['5m'],
            '5m': []
        }

    def _build_timeframe_relationships(self):
        """íƒ€ì„í”„ë ˆì„ ê°„ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì¶•"""
        try:
            # íƒ€ì„í”„ë ˆì„ ê°„ ì˜ˆìƒ ìƒê´€ê³„ìˆ˜ (ì¥ê¸° > ë‹¨ê¸°)
            relationships = {}
            
            for i, tf1 in enumerate(self.timeframes):
                relationships[tf1] = {}
                for j, tf2 in enumerate(self.timeframes):
                    if i == j:
                        relationships[tf1][tf2] = 1.0  # ìê¸° ìì‹ 
                    else:
                        # ê±°ë¦¬ì— ë”°ë¼ ìƒê´€ê³„ìˆ˜ ê°ì†Œ
                        distance = abs(i - j)
                        correlation = max(0.5, 1.0 - (distance * 0.15))
                        relationships[tf1][tf2] = correlation

            return relationships

        except Exception as e:
            self.logger.error(f"Relationship matrix building error: {e}")
            return {}

    def _get_cached_data(self, key):
        """ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        """ë°ì´í„° ìºì‹±"""
        self._cache[key] = (data, datetime.now().timestamp())

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1ï¸âƒ£ ëª¨ë“  íƒ€ì„í”„ë ˆì„ ë¶„ì„
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_all_timeframes(self, regime_analyzer):
        """
        ëª¨ë“  íƒ€ì„í”„ë ˆì„ì—ì„œ regime ë¶„ì„ ìˆ˜í–‰

        Args:
            regime_analyzer: MarketRegimeAnalyzer ì¸ìŠ¤í„´ìŠ¤

        Returns:
            dict: {timeframe: {'regime': str, 'score': float, 'confidence': float, 'indicators': dict}}
        """
        try:
            results = {}

            for timeframe in self.timeframes:
                cached = self._get_cached_data(f'timeframe_analysis_{timeframe}')
                if cached:
                    results[timeframe] = cached
                    continue

                try:
                    # ê° íƒ€ì„í”„ë ˆì„ì—ì„œ regime ë¶„ì„
                    # (ì‹¤ì œë¡œëŠ” íƒ€ì„í”„ë ˆì„ë³„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨)
                    # ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”ëœ ë²„ì „ìœ¼ë¡œ êµ¬í˜„
                    
                    # Trend ë¶„ì„
                    trend = regime_analyzer._get_macro_trend(timeframe)
                    
                    # Volatility ë¶„ì„
                    volatility = regime_analyzer._get_market_volatility(timeframe)
                    
                    # Volume ë¶„ì„
                    volume_profile = regime_analyzer._analyze_volume_profile(timeframe=timeframe)
                    
                    # Momentum ë¶„ì„
                    momentum = regime_analyzer._calculate_market_momentum(timeframe)
                    
                    # Breadth ë¶„ì„
                    market_breadth = regime_analyzer._get_market_breadth(timeframe)

                    # ê°„ì†Œí™”ëœ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
                    indicators = {
                        'trend': trend,
                        'volatility': volatility,
                        'volume': volume_profile,
                        'momentum': momentum,
                        'breadth': market_breadth
                    }

                    # Regime ê²°ì • (ê°„ì†Œí™”)
                    regime, score = self._determine_regime_for_timeframe(indicators)
                    
                    # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ì†Œí™”)
                    confidence = self._calculate_timeframe_confidence(indicators, timeframe)

                    result = {
                        'regime': regime,
                        'score': score,
                        'confidence': confidence,
                        'indicators': indicators,
                        'timeframe': timeframe,
                        'timestamp': datetime.now()
                    }

                    results[timeframe] = result
                    self._set_cached_data(f'timeframe_analysis_{timeframe}', result)
                    self.timeframe_regimes[timeframe] = result

                except Exception as e:
                    self.logger.debug(f"Timeframe {timeframe} analysis error: {e}")
                    # ì—ëŸ¬ ì‹œ ì¤‘ë¦½ ìƒíƒœ ë°˜í™˜
                    results[timeframe] = {
                        'regime': 'UNCERTAIN',
                        'score': 0.0,
                        'confidence': 0.5,
                        'indicators': {},
                        'timeframe': timeframe,
                        'timestamp': datetime.now()
                    }

            return results

        except Exception as e:
            self.logger.error(f"All timeframes analysis error: {e}")
            return {}

    def _determine_regime_for_timeframe(self, indicators):
        """
        íŠ¹ì • íƒ€ì„í”„ë ˆì„ì˜ ì§€í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ regime ê²°ì •

        Returns:
            tuple: (regime_name, score)
        """
        try:
            trend = indicators.get('trend', 'sideways')
            volatility = indicators.get('volatility', 'medium')
            momentum = indicators.get('momentum', 'neutral')
            breadth = indicators.get('breadth', 0.5)

            # ê°„ì†Œí™”ëœ regime ê²°ì • ë¡œì§
            if trend in ['uptrend', 'strong_uptrend']:
                if volatility in ['high', 'extreme']:
                    return 'BULL_VOLATILITY', 0.8
                else:
                    return 'BULL_CONSOLIDATION', 0.7

            elif trend in ['downtrend', 'strong_downtrend']:
                if volatility in ['high', 'extreme']:
                    return 'BEAR_VOLATILITY', 0.8
                else:
                    return 'BEAR_CONSOLIDATION', 0.7

            elif trend == 'sideways':
                if volatility == 'low':
                    return 'SIDEWAYS_COMPRESSION', 0.6
                else:
                    return 'SIDEWAYS_CHOP', 0.6

            return 'UNCERTAIN', 0.5

        except Exception as e:
            self.logger.debug(f"Regime determination error: {e}")
            return 'UNCERTAIN', 0.5

    def _calculate_timeframe_confidence(self, indicators, timeframe):
        """
        íƒ€ì„í”„ë ˆì„ë³„ ì‹ ë¢°ë„ ê³„ì‚°

        Returns:
            float: ì‹ ë¢°ë„ (0.0 ~ 1.0)
        """
        try:
            confidence = 0.7  # ê¸°ë³¸ ì‹ ë¢°ë„

            # ì§€í‘œ ëª…í™•ì„±ì— ë”°ë¼ ì‹ ë¢°ë„ ì¡°ì •
            trend = indicators.get('trend', 'sideways')
            if trend in ['strong_uptrend', 'strong_downtrend']:
                confidence += 0.15
            elif trend == 'sideways':
                confidence -= 0.10

            volatility = indicators.get('volatility', 'medium')
            if volatility in ['extreme', 'low']:
                confidence += 0.10

            # íƒ€ì„í”„ë ˆì„ë³„ ê°€ì¤‘ì¹˜ (ì¥ê¸° íƒ€ì„í”„ë ˆì„ì´ ë” ì‹ ë¢°)
            tf_weight = self.timeframe_weights.get(timeframe, 0.5)
            confidence *= (0.8 + tf_weight * 0.4)  # 0.8 ~ 1.2 ë²”ìœ„

            return np.clip(confidence, 0.0, 1.0)

        except Exception as e:
            self.logger.debug(f"Timeframe confidence calculation error: {e}")
            return 0.7

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2ï¸âƒ£ íƒ€ì„í”„ë ˆì„ ê°„ ì»¨ì„¼ì„œìŠ¤ ê³„ì‚°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_timeframe_consensus(self, timeframe_results):
        """
        ì—¬ëŸ¬ íƒ€ì„í”„ë ˆì„ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì»¨ì„¼ì„œìŠ¤ ê³„ì‚°

        Args:
            timeframe_results: {timeframe: result_dict}

        Returns:
            dict: {
                'consensus_regime': str,
                'consensus_score': float,
                'consensus_confidence': float,
                'alignment_score': float,
                'participating_timeframes': list
            }
        """
        try:
            if not timeframe_results:
                return {
                    'consensus_regime': 'UNCERTAIN',
                    'consensus_score': 0.0,
                    'consensus_confidence': 0.5,
                    'alignment_score': 0.5,
                    'participating_timeframes': []
                }

            # ê° regimeì˜ ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°
            regime_scores = {}
            total_weight = 0.0

            for timeframe, result in timeframe_results.items():
                regime = result['regime']
                score = result['score']
                confidence = result['confidence']
                
                # íƒ€ì„í”„ë ˆì„ ê°€ì¤‘ì¹˜
                tf_weight = self.timeframe_weights.get(timeframe, 0.1)
                
                # ê°€ì¤‘ ì ìˆ˜ = ì›ì ìˆ˜ Ã— ì‹ ë¢°ë„ Ã— íƒ€ì„í”„ë ˆì„ ê°€ì¤‘ì¹˜
                weighted_score = score * confidence * tf_weight
                
                if regime not in regime_scores:
                    regime_scores[regime] = 0.0
                
                regime_scores[regime] += weighted_score
                total_weight += tf_weight

            # ì •ê·œí™”
            if total_weight > 0:
                regime_scores = {k: v / total_weight for k, v in regime_scores.items()}

            # ìµœê³  ì ìˆ˜ regime ì„ íƒ
            if regime_scores:
                consensus_regime = max(regime_scores, key=regime_scores.get)
                consensus_score = regime_scores[consensus_regime]
            else:
                consensus_regime = 'UNCERTAIN'
                consensus_score = 0.0

            # ì •ë ¬ë„ ê³„ì‚°
            alignment_score = self.calculate_alignment_score(timeframe_results)

            # ì»¨ì„¼ì„œìŠ¤ ì‹ ë¢°ë„ ê³„ì‚°
            consensus_confidence = self._calculate_consensus_confidence(
                timeframe_results, 
                consensus_regime,
                alignment_score
            )

            result = {
                'consensus_regime': consensus_regime,
                'consensus_score': consensus_score,
                'consensus_confidence': consensus_confidence,
                'alignment_score': alignment_score,
                'participating_timeframes': list(timeframe_results.keys()),
                'regime_scores': regime_scores,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.consensus_history.append(result)

            return result

        except Exception as e:
            self.logger.error(f"Consensus calculation error: {e}")
            return {
                'consensus_regime': 'UNCERTAIN',
                'consensus_score': 0.0,
                'consensus_confidence': 0.5,
                'alignment_score': 0.5,
                'participating_timeframes': []
            }

    def _calculate_consensus_confidence(self, timeframe_results, consensus_regime, alignment_score):
        """
        ì»¨ì„¼ì„œìŠ¤ ì‹ ë¢°ë„ ê³„ì‚°

        Returns:
            float: ì‹ ë¢°ë„ (0.0 ~ 1.0)
        """
        try:
            # 1ï¸âƒ£ ì¼ì¹˜í•˜ëŠ” íƒ€ì„í”„ë ˆì„ ë¹„ìœ¨
            agreeing_count = sum(1 for result in timeframe_results.values() 
                                if result['regime'] == consensus_regime)
            agreement_ratio = agreeing_count / len(timeframe_results) if timeframe_results else 0.5

            # 2ï¸âƒ£ í‰ê·  ì‹ ë¢°ë„
            avg_confidence = np.mean([result['confidence'] for result in timeframe_results.values()]) if timeframe_results else 0.5

            # 3ï¸âƒ£ ì •ë ¬ë„
            alignment_factor = alignment_score

            # 4ï¸âƒ£ ì ìˆ˜ ë¶„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            scores = [result['score'] for result in timeframe_results.values()]
            score_std = np.std(scores) if len(scores) > 1 else 0.0
            score_consistency = 1.0 - min(score_std, 1.0)

            # ì¢…í•© ì‹ ë¢°ë„
            consensus_confidence = (
                agreement_ratio * 0.35 +
                avg_confidence * 0.25 +
                alignment_factor * 0.25 +
                score_consistency * 0.15
            )

            return np.clip(consensus_confidence, 0.0, 1.0)

        except Exception as e:
            self.logger.debug(f"Consensus confidence calculation error: {e}")
            return 0.5

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3ï¸âƒ£ íƒ€ì„í”„ë ˆì„ ê°„ ì •ë ¬ë„ ê³„ì‚°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def calculate_alignment_score(self, timeframe_results):
        """
        íƒ€ì„í”„ë ˆì„ ê°„ ì •ë ¬ë„ ì ìˆ˜ ê³„ì‚°
        ëª¨ë“  íƒ€ì„í”„ë ˆì„ì´ ê°™ì€ ë°©í–¥ì„ ê°€ë¦¬í‚¤ëŠ” ì •ë„

        Returns:
            float: ì •ë ¬ë„ (0.0 ~ 1.0)
        """
        try:
            if len(timeframe_results) < 2:
                return 1.0

            # regimeì„ ë°©í–¥ì„±ìœ¼ë¡œ ë³€í™˜
            regime_directions = {}
            for timeframe, result in timeframe_results.items():
                regime = result['regime']
                
                if 'BULL' in regime or regime == 'ACCUMULATION':
                    direction = 1  # ìƒìŠ¹
                elif 'BEAR' in regime or regime == 'DISTRIBUTION':
                    direction = -1  # í•˜ë½
                else:
                    direction = 0  # ì¤‘ë¦½
                
                regime_directions[timeframe] = direction

            # ë°©í–¥ ì¼ì¹˜ë„ ê³„ì‚°
            directions = list(regime_directions.values())
            
            # ëª¨ë‘ ê°™ì€ ë°©í–¥ì´ë©´ 1.0
            if len(set(directions)) == 1:
                alignment = 1.0
            else:
                # ë‹¤ìˆ˜ ë°©í–¥ ì°¾ê¸°
                most_common_direction = max(set(directions), key=directions.count)
                alignment_count = directions.count(most_common_direction)
                alignment = alignment_count / len(directions)

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.alignment_history.append({
                'timestamp': datetime.now(),
                'alignment_score': alignment,
                'directions': regime_directions
            })

            return alignment

        except Exception as e:
            self.logger.debug(f"Alignment score calculation error: {e}")
            return 0.5

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4ï¸âƒ£ íƒ€ì„í”„ë ˆì„ ê°„ ë¶„ì‚° ê°ì§€
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def detect_timeframe_divergence(self, timeframe_results):
        """
        íƒ€ì„í”„ë ˆì„ ê°„ ë¶„ì‚°(Divergence) ê°ì§€

        Returns:
            dict: {
                'divergence_detected': bool,
                'divergence_score': float (0.0 ~ 1.0),
                'diverging_pairs': list,
                'dominant_timeframe': str
            }
        """
        try:
            diverging_pairs = []
            divergence_scores = []

            # ëª¨ë“  íƒ€ì„í”„ë ˆì„ ìŒ ë¹„êµ
            timeframes_list = list(timeframe_results.keys())
            
            for i, tf1 in enumerate(timeframes_list):
                for tf2 in timeframes_list[i+1:]:
                    result1 = timeframe_results[tf1]
                    result2 = timeframe_results[tf2]

                    # Regime ë¹„êµ
                    regime1 = result1['regime']
                    regime2 = result2['regime']

                    # ë°©í–¥ì„± ë¹„êµ
                    dir1 = 1 if 'BULL' in regime1 or regime1 == 'ACCUMULATION' else (-1 if 'BEAR' in regime1 or regime1 == 'DISTRIBUTION' else 0)
                    dir2 = 1 if 'BULL' in regime2 or regime2 == 'ACCUMULATION' else (-1 if 'BEAR' in regime2 or regime2 == 'DISTRIBUTION' else 0)

                    # ë¶„ì‚° ì ìˆ˜ ê³„ì‚°
                    if dir1 * dir2 < 0:  # ë°˜ëŒ€ ë°©í–¥
                        divergence = 1.0
                        diverging_pairs.append((tf1, tf2, regime1, regime2))
                    elif dir1 == 0 or dir2 == 0:  # í•œìª½ì´ ì¤‘ë¦½
                        divergence = 0.5
                    else:  # ê°™ì€ ë°©í–¥
                        divergence = 0.0

                    divergence_scores.append(divergence)

            # ì „ì²´ ë¶„ì‚° ì ìˆ˜
            avg_divergence = np.mean(divergence_scores) if divergence_scores else 0.0

            # ë¶„ì‚° ê°ì§€ ì—¬ë¶€
            divergence_detected = avg_divergence > self.thresholds['divergence_critical']

            # ì§€ë°°ì  íƒ€ì„í”„ë ˆì„ ì°¾ê¸° (ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ì˜ íƒ€ì„í”„ë ˆì„)
            dominant_timeframe = max(
                timeframe_results.keys(),
                key=lambda tf: self.timeframe_weights.get(tf, 0)
            )

            result = {
                'divergence_detected': divergence_detected,
                'divergence_score': avg_divergence,
                'diverging_pairs': diverging_pairs,
                'dominant_timeframe': dominant_timeframe,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.divergence_history.append(result)

            return result

        except Exception as e:
            self.logger.error(f"Divergence detection error: {e}")
            return {
                'divergence_detected': False,
                'divergence_score': 0.0,
                'diverging_pairs': [],
                'dominant_timeframe': '1d'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5ï¸âƒ£ ì¶©ëŒ í•´ê²°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def resolve_conflicts(self, timeframe_results, divergence_info):
        """
        íƒ€ì„í”„ë ˆì„ ê°„ ì¶©ëŒ í•´ê²°
        ì¥ê¸° íƒ€ì„í”„ë ˆì„ ìš°ì„ , ì‹ ë¢°ë„ ê³ ë ¤

        Returns:
            dict: {
                'resolved_regime': str,
                'resolution_method': str,
                'confidence': float
            }
        """
        try:
            # 1ï¸âƒ£ ë¶„ì‚°ì´ ì—†ìœ¼ë©´ ì»¨ì„¼ì„œìŠ¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if not divergence_info['divergence_detected']:
                consensus = self.calculate_timeframe_consensus(timeframe_results)
                return {
                    'resolved_regime': consensus['consensus_regime'],
                    'resolution_method': 'consensus',
                    'confidence': consensus['consensus_confidence']
                }

            # 2ï¸âƒ£ ë¶„ì‚°ì´ ìˆìœ¼ë©´ ê³„ì¸µì  í•´ê²°
            # ì§€ë°°ì  íƒ€ì„í”„ë ˆì„(ê°€ì¥ ê¸´ íƒ€ì„í”„ë ˆì„) ìš°ì„ 
            dominant_tf = divergence_info['dominant_timeframe']
            dominant_result = timeframe_results[dominant_tf]

            # 3ï¸âƒ£ ì§€ë°°ì  íƒ€ì„í”„ë ˆì„ì˜ ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì±„íƒ
            if dominant_result['confidence'] > 0.7:
                return {
                    'resolved_regime': dominant_result['regime'],
                    'resolution_method': 'dominant_timeframe',
                    'confidence': dominant_result['confidence'],
                    'dominant_timeframe': dominant_tf
                }

            # 4ï¸âƒ£ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ê°€ì¤‘ í‰ê· 
            weighted_regimes = {}
            total_weight = 0.0

            for timeframe, result in timeframe_results.items():
                regime = result['regime']
                confidence = result['confidence']
                tf_weight = self.timeframe_weights.get(timeframe, 0.1)
                
                weight = confidence * tf_weight
                
                if regime not in weighted_regimes:
                    weighted_regimes[regime] = 0.0
                
                weighted_regimes[regime] += weight
                total_weight += weight

            # ì •ê·œí™”
            if total_weight > 0:
                weighted_regimes = {k: v / total_weight for k, v in weighted_regimes.items()}

            # ìµœê³  ê°€ì¤‘ì¹˜ regime ì„ íƒ
            resolved_regime = max(weighted_regimes, key=weighted_regimes.get)
            resolved_confidence = weighted_regimes[resolved_regime]

            return {
                'resolved_regime': resolved_regime,
                'resolution_method': 'weighted_average',
                'confidence': resolved_confidence
            }

        except Exception as e:
            self.logger.error(f"Conflict resolution error: {e}")
            return {
                'resolved_regime': 'UNCERTAIN',
                'resolution_method': 'error_fallback',
                'confidence': 0.5
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6ï¸âƒ£ ë‹¤ì¤‘ í•´ìƒë„ Regime ë§µ ìƒì„±
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def generate_multi_resolution_map(self, timeframe_results):
        """
        ë‹¤ì¤‘ í•´ìƒë„ regime ë§µ ìƒì„±
        ê° íƒ€ì„í”„ë ˆì„ì˜ regimeê³¼ ê·¸ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„

        Returns:
            dict: íƒ€ì„í”„ë ˆì„ë³„ regime ë§µê³¼ ê³„ì¸µ êµ¬ì¡°
        """
        try:
            resolution_map = {
                'timeframe_regimes': {},
                'hierarchical_structure': {},
                'transition_points': [],
                'key_insights': []
            }

            # 1ï¸âƒ£ ê° íƒ€ì„í”„ë ˆì„ì˜ regime ê¸°ë¡
            for timeframe, result in timeframe_results.items():
                resolution_map['timeframe_regimes'][timeframe] = {
                    'regime': result['regime'],
                    'score': result['score'],
                    'confidence': result['confidence'],
                    'indicators': result['indicators']
                }

            # 2ï¸âƒ£ ê³„ì¸µ êµ¬ì¡° ë¶„ì„
            for parent_tf, children_tf in self.timeframe_hierarchy.items():
                if parent_tf in timeframe_results:
                    parent_regime = timeframe_results[parent_tf]['regime']
                    
                    children_regimes = []
                    for child_tf in children_tf:
                        if child_tf in timeframe_results:
                            child_regime = timeframe_results[child_tf]['regime']
                            children_regimes.append({
                                'timeframe': child_tf,
                                'regime': child_regime,
                                'matches_parent': child_regime == parent_regime
                            })
                    
                    resolution_map['hierarchical_structure'][parent_tf] = {
                        'parent_regime': parent_regime,
                        'children': children_regimes
                    }

            # 3ï¸âƒ£ ì „í™˜ì  ê°ì§€
            # íƒ€ì„í”„ë ˆì„ ê°„ regimeì´ ë°”ë€ŒëŠ” ì§€ì  ì°¾ê¸°
            for i, timeframe in enumerate(self.timeframes[:-1]):
                current_tf = timeframe
                next_tf = self.timeframes[i + 1]
                
                if current_tf in timeframe_results and next_tf in timeframe_results:
                    current_regime = timeframe_results[current_tf]['regime']
                    next_regime = timeframe_results[next_tf]['regime']
                    
                    if current_regime != next_regime:
                        resolution_map['transition_points'].append({
                            'from_timeframe': current_tf,
                            'to_timeframe': next_tf,
                            'from_regime': current_regime,
                            'to_regime': next_regime
                        })

            # 4ï¸âƒ£ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±
            resolution_map['key_insights'] = self._generate_resolution_insights(
                timeframe_results,
                resolution_map
            )

            return resolution_map

        except Exception as e:
            self.logger.error(f"Multi-resolution map generation error: {e}")
            return {}

    def _generate_resolution_insights(self, timeframe_results, resolution_map):
        """í•´ìƒë„ ë§µ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        try:
            # 1ï¸âƒ£ ì „ì²´ ë°©í–¥ì„± í™•ì¸
            regimes = [result['regime'] for result in timeframe_results.values()]
            unique_regimes = set(regimes)
            
            if len(unique_regimes) == 1:
                insights.append(f"âœ… ëª¨ë“  íƒ€ì„í”„ë ˆì„ì´ {regimes[0]} ìƒíƒœë¡œ ì¼ì¹˜")
            elif len(unique_regimes) > 3:
                insights.append("âš ï¸ íƒ€ì„í”„ë ˆì„ ê°„ ë†’ì€ ë¶„ì‚° - ë¶ˆí™•ì‹¤í•œ ì‹œì¥")

            # 2ï¸âƒ£ ì¥ê¸° vs ë‹¨ê¸° ë¹„êµ
            if '1d' in timeframe_results and '5m' in timeframe_results:
                long_term = timeframe_results['1d']['regime']
                short_term = timeframe_results['5m']['regime']
                
                if long_term != short_term:
                    insights.append(f"ğŸ”„ ì¥ë‹¨ê¸° ë¶ˆì¼ì¹˜: ì¥ê¸°({long_term}) vs ë‹¨ê¸°({short_term})")

            # 3ï¸âƒ£ ì „í™˜ì  ë¶„ì„
            if len(resolution_map['transition_points']) > 0:
                insights.append(f"ğŸ“ {len(resolution_map['transition_points'])}ê°œì˜ regime ì „í™˜ì  ê°ì§€")

        except Exception as e:
            self.logger.debug(f"Resolution insights generation error: {e}")

        return insights

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7ï¸âƒ£ Regime ì§„í™” ì˜ˆì¸¡
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def predict_regime_evolution(self, timeframe_results):
        """
        íƒ€ì„í”„ë ˆì„ ê°„ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ regime ì§„í™” ì˜ˆì¸¡

        Returns:
            dict: {
                'predicted_regime': str,
                'prediction_confidence': float,
                'evolution_direction': str,
                'time_horizon': str
            }
        """
        try:
            # 1ï¸âƒ£ ë‹¨ê¸° â†’ ì¥ê¸° ì¶”ì„¸ ì „íŒŒ ë¶„ì„
            # ë‹¨ê¸° íƒ€ì„í”„ë ˆì„ì˜ ë³€í™”ê°€ ì¥ê¸°ë¡œ í™•ì‚°ë˜ëŠ”ì§€ í™•ì¸
            
            short_term_regimes = []
            long_term_regimes = []

            for timeframe in ['5m', '15m']:
                if timeframe in timeframe_results:
                    short_term_regimes.append(timeframe_results[timeframe]['regime'])

            for timeframe in ['4h', '1d']:
                if timeframe in timeframe_results:
                    long_term_regimes.append(timeframe_results[timeframe]['regime'])

            # 2ï¸âƒ£ íˆìŠ¤í† ë¦¬ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­
            if len(self.consensus_history) >= 3:
                recent_consensus = [c['consensus_regime'] for c in list(self.consensus_history)[-3:]]
                
                # ì¶”ì„¸ íŒŒì•…
                if len(set(recent_consensus)) == 1:
                    evolution_direction = 'stable'
                elif recent_consensus[-1] != recent_consensus[0]:
                    evolution_direction = 'transitioning'
                else:
                    evolution_direction = 'oscillating'
            else:
                evolution_direction = 'unknown'

            # 3ï¸âƒ£ ì˜ˆì¸¡ ë¡œì§
            predicted_regime = 'UNCERTAIN'
            prediction_confidence = 0.5

            # ë‹¨ê¸°ì™€ ì¥ê¸°ê°€ ì¼ì¹˜í•˜ë©´ ìœ ì§€ ì˜ˆì¸¡
            if short_term_regimes and long_term_regimes:
                if short_term_regimes[0] == long_term_regimes[0]:
                    predicted_regime = short_term_regimes[0]
                    prediction_confidence = 0.75
                else:
                    # ì¥ê¸° ì¶”ì„¸ë¥¼ ë”°ë¥¼ ê°€ëŠ¥ì„± ë†’ìŒ
                    predicted_regime = long_term_regimes[0]
                    prediction_confidence = 0.60

            # 4ï¸âƒ£ ì‹œê°„ ë²”ìœ„ ì¶”ì •
            if prediction_confidence > 0.7:
                time_horizon = 'short_term'  # 1-4ì‹œê°„
            elif prediction_confidence > 0.5:
                time_horizon = 'medium_term'  # 4-24ì‹œê°„
            else:
                time_horizon = 'long_term'  # 24ì‹œê°„ ì´ìƒ

            return {
                'predicted_regime': predicted_regime,
                'prediction_confidence': prediction_confidence,
                'evolution_direction': evolution_direction,
                'time_horizon': time_horizon,
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Regime evolution prediction error: {e}")
            return {
                'predicted_regime': 'UNCERTAIN',
                'prediction_confidence': 0.5,
                'evolution_direction': 'unknown',
                'time_horizon': 'unknown'
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 8ï¸âƒ£ ì¢…í•© ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_consensus_report(self):
        """
        ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤ ì¢…í•© ë¦¬í¬íŠ¸

        Returns:
            dict: ì „ì²´ ì»¨ì„¼ì„œìŠ¤ ë¶„ì„ ê²°ê³¼
        """
        try:
            if len(self.consensus_history) == 0:
                return {}

            latest_consensus = self.consensus_history[-1]

            report = {
                'timestamp': datetime.now().isoformat(),
                'current_consensus': latest_consensus,
                'alignment_metrics': {
                    'current_alignment': self.alignment_history[-1] if len(self.alignment_history) > 0 else None,
                    'average_alignment': np.mean([a['alignment_score'] for a in self.alignment_history]) if len(self.alignment_history) > 0 else 0.5
                },
                'divergence_metrics': {
                    'current_divergence': self.divergence_history[-1] if len(self.divergence_history) > 0 else None,
                    'divergence_frequency': sum(1 for d in self.divergence_history if d['divergence_detected']) / max(len(self.divergence_history), 1)
                },
                'timeframe_regimes': self.timeframe_regimes,
                'consensus_trend': self._analyze_consensus_trend()
            }

            return report

        except Exception as e:
            self.logger.error(f"Consensus report generation error: {e}")
            return {}

    def _analyze_consensus_trend(self):
        """ì»¨ì„¼ì„œìŠ¤ ì¶”ì„¸ ë¶„ì„"""
        try:
            if len(self.consensus_history) < 3:
                return 'insufficient_data'

            recent = [c['consensus_regime'] for c in list(self.consensus_history)[-5:]]
            
            if len(set(recent)) == 1:
                return 'stable'
            elif recent[-1] != recent[0]:
                return 'changing'
            else:
                return 'volatile'

        except Exception as e:
            self.logger.debug(f"Consensus trend analysis error: {e}")
            return 'unknown'


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MarketRegimeAnalyzer í´ë˜ìŠ¤ (ì „ì²´ ì‹œìŠ¤í…œ í†µí•© - ìœ ë™ì„± í¬í•¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MarketRegimeAnalyzer:
    """
    ì—¬ëŸ¬ ìš”ì¸ì„ ì¢…í•©í•˜ì—¬ í˜„ì¬ ì‹œì¥ ì²´ì œ(Market Regime)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    ê³ ë„í™” ë²„ì „ - ë‹¤ì¤‘ ì§€í‘œ ë° ì‹¬ì¸µ ë¶„ì„ + ì‹¤ì‹œê°„ ì ì‘í˜• ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ + ìƒíƒœ ì§€ì†ì„± ì¶”ì  + ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ë°ì´í„° ìœµí•© + ë‹¤ì°¨ì› ì‹ ë¢°ë„ ìŠ¤ì½”ì–´ë§ + ğŸ”¥ğŸ”¥ ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤ + ğŸ”¥ğŸ”¥ ìœ ë™ì„± ìƒíƒœ ì¶”ì • ğŸ”¥ğŸ”¥
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("MarketRegime")

        # ìºì‹± ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€ (ì„±ëŠ¥ í–¥ìƒ)
        self._cache = {}
        self._cache_ttl = 60  # 60ì´ˆ ìºì‹œ

        # ë¶„ì„ì— ì‚¬ìš©í•  ì£¼ìš” ì•ŒíŠ¸ì½”ì¸ ë¦¬ìŠ¤íŠ¸ í™•ì¥
        self.major_alts = ['ETHUSDT', 'SOLUSDT', 'BNBUSDT', 'ADAUSDT', 'XRPUSDT']

        # ğŸ”¥ğŸ”¥ğŸ”¥ ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™” ğŸ”¥ğŸ”¥ğŸ”¥
        self.onchain_manager = OnChainDataManager()
        self.macro_manager = MacroDataManager(market_data_manager)

        # ğŸ”¥ğŸ”¥ğŸ”¥ 4ï¸âƒ£ ë‹¤ì°¨ì› ì‹ ë¢°ë„ ìŠ¤ì½”ì–´ëŸ¬ ì´ˆê¸°í™” ğŸ”¥ğŸ”¥ğŸ”¥
        self.confidence_scorer = MultiDimensionalConfidenceScorer()

        # ğŸ”¥ğŸ”¥ğŸ”¥ 5ï¸âƒ£ ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤ ì—”ì§„ ì´ˆê¸°í™” ğŸ”¥ğŸ”¥ğŸ”¥
        self.mtf_consensus = MultiTimeframeConsensusEngine(market_data_manager)

        # ğŸ”¥ğŸ”¥ğŸ”¥ 6ï¸âƒ£ ìœ ë™ì„± ìƒíƒœ íƒì§€ê¸° ì´ˆê¸°í™” ğŸ”¥ğŸ”¥ğŸ”¥
        self.liquidity_detector = LiquidityRegimeDetector(market_data_manager)

        # ğŸ¯ ê¸°ë³¸ ì‹œì¥ ì²´ì œë³„ ê°€ì¤‘ì¹˜ ì„¤ì • (Baseline)
        self.base_regime_weights = {
            'trend': 0.20,  # ì¶”ì„¸ (ìœ ë™ì„±ìœ¼ë¡œ ì¸í•´ ë¹„ì¤‘ ì¡°ì •)
            'volatility': 0.18,  # ë³€ë™ì„±
            'volume': 0.12,  # ê±°ë˜ëŸ‰
            'momentum': 0.10,  # ëª¨ë©˜í…€
            'sentiment': 0.06,  # ì‹œì¥ ì‹¬ë¦¬
            'onchain': 0.10,  # ğŸ”¥ ì˜¨ì²´ì¸ ì‹ í˜¸
            'macro': 0.07,  # ğŸ”¥ ë§¤í¬ë¡œ ì‹ í˜¸
            'liquidity': 0.17  # ğŸ”¥ğŸ”¥ ìœ ë™ì„± (ìƒˆë¡œ ì¶”ê°€, ì¤‘ìš”ë„ ë†’ìŒ)
        }

        # ğŸ”¥ ì‹¤ì‹œê°„ ì ì‘í˜• ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ
        self.adaptive_weights = self.base_regime_weights.copy()

        # ğŸ¯ ì‹œì¥ ìƒíƒœë³„ ìµœì  ê°€ì¤‘ì¹˜ í”„ë¡œíŒŒì¼ (ìœ ë™ì„± í¬í•¨)
        self.weight_profiles = {
            'high_volatility': {
                'trend': 0.12,
                'volatility': 0.30,
                'volume': 0.10,
                'momentum': 0.15,
                'sentiment': 0.04,
                'onchain': 0.08,
                'macro': 0.05,
                'liquidity': 0.16
            },
            'strong_trend': {
                'trend': 0.30,
                'volatility': 0.10,
                'volume': 0.10,
                'momentum': 0.15,
                'sentiment': 0.04,
                'onchain': 0.08,
                'macro': 0.06,
                'liquidity': 0.17
            },
            'sideways_market': {
                'trend': 0.08,
                'volatility': 0.15,
                'volume': 0.20,
                'momentum': 0.10,
                'sentiment': 0.08,
                'onchain': 0.12,
                'macro': 0.07,
                'liquidity': 0.20
            },
            'volume_spike': {
                'trend': 0.15,
                'volatility': 0.12,
                'volume': 0.25,
                'momentum': 0.10,
                'sentiment': 0.04,
                'onchain': 0.08,
                'macro': 0.06,
                'liquidity': 0.20
            },
            'momentum_driven': {
                'trend': 0.20,
                'volatility': 0.10,
                'volume': 0.10,
                'momentum': 0.25,
                'sentiment': 0.04,
                'onchain': 0.06,
                'macro': 0.06,
                'liquidity': 0.19
            },
            'sentiment_extreme': {
                'trend': 0.15,
                'volatility': 0.12,
                'volume': 0.10,
                'momentum': 0.12,
                'sentiment': 0.12,
                'onchain': 0.12,
                'macro': 0.06,
                'liquidity': 0.21
            },
            'onchain_dominant': {
                'trend': 0.12,
                'volatility': 0.08,
                'volume': 0.08,
                'momentum': 0.08,
                'sentiment': 0.04,
                'onchain': 0.30,
                'macro': 0.12,
                'liquidity': 0.18
            },
            'macro_dominant': {
                'trend': 0.12,
                'volatility': 0.08,
                'volume': 0.08,
                'momentum': 0.08,
                'sentiment': 0.04,
                'onchain': 0.12,
                'macro': 0.30,
                'liquidity': 0.18
            },
            'low_liquidity': {  # ğŸ”¥ğŸ”¥ ìƒˆë¡œìš´ í”„ë¡œíŒŒì¼
                'trend': 0.10,
                'volatility': 0.18,
                'volume': 0.08,
                'momentum': 0.08,
                'sentiment': 0.06,
                'onchain': 0.08,
                'macro': 0.06,
                'liquidity': 0.36  # ìœ ë™ì„±ì´ ì§€ë°°ì 
            }
        }

        # ğŸ“Š ì§€í‘œ ì‹ ë¢°ë„ ì¶”ì  (Historical Performance)
        self.indicator_reliability = {
            'trend': deque(maxlen=50),
            'volatility': deque(maxlen=50),
            'volume': deque(maxlen=50),
            'momentum': deque(maxlen=50),
            'sentiment': deque(maxlen=50),
            'onchain': deque(maxlen=50),
            'macro': deque(maxlen=50),
            'liquidity': deque(maxlen=50)  # ğŸ”¥ğŸ”¥ ìœ ë™ì„± ì¶”ê°€
        }

        # ğŸ“ˆ ì˜ˆì¸¡ ì„±ëŠ¥ ì¶”ì 
        self.prediction_history = deque(maxlen=100)

        # âš¡ ê°€ì¤‘ì¹˜ ì¡°ì • ì†ë„ (0.0 ~ 1.0, ë†’ì„ìˆ˜ë¡ ë¹ ë¥´ê²Œ ì ì‘)
        self.adaptation_speed = 0.3

        # ğŸšï¸ ì§€í‘œë³„ ì‹ ë¢°ë„ ì„ê³„ê°’
        self.reliability_thresholds = {
            'high': 0.75,
            'medium': 0.50,
            'low': 0.30
        }

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”¥ğŸ”¥ğŸ”¥ 2ï¸âƒ£ ìƒíƒœ ì§€ì†ì„± ì¶”ì  (Regime Transition Stability) ğŸ”¥ğŸ”¥ğŸ”¥
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # ğŸ“Œ í˜„ì¬ regime ìƒíƒœ ì¶”ì 
        self.current_regime = None
        self.current_regime_start_time = None
        self.current_regime_duration = timedelta(0)

        # ğŸ“Š Regime ì „í™˜ íˆìŠ¤í† ë¦¬ (ìµœê·¼ 100ê°œ)
        self.regime_history = deque(maxlen=100)

        # ğŸ¯ Regime ì „í™˜ íŒŒë¼ë¯¸í„°
        self.transition_config = {
            'min_duration_seconds': 300,  # ìµœì†Œ regime ì§€ì† ì‹œê°„ (5ë¶„)
            'base_threshold': 0.15,  # ê¸°ë³¸ ì „í™˜ ì„ê³„ê°’ (15% ì°¨ì´ í•„ìš”)
            'max_threshold': 0.40,  # ìµœëŒ€ ì „í™˜ ì„ê³„ê°’
            'stability_window': 20,  # ì•ˆì •ì„± í‰ê°€ ìœˆë„ìš° (ìµœê·¼ Nê°œ)
            'flip_penalty_duration': 600,  # ë¹ˆë²ˆí•œ ì „í™˜ í˜ë„í‹° ê¸°ê°„ (10ë¶„)
            'confidence_boost_stable': 1.2,  # ì•ˆì •ì ì¸ regime ì‹ ë¢°ë„ ë¶€ìŠ¤íŠ¸
            'confidence_penalty_unstable': 0.8  # ë¶ˆì•ˆì • regime ì‹ ë¢°ë„ í˜ë„í‹°
        }

        # ğŸ“ˆ Regime ê°•ë„ ì¶”ì 
        self.regime_strength_history = deque(maxlen=50)
        self.current_regime_strength = 0.0

        # ğŸ”„ ì „í™˜ ì„ê³„ê°’ ë™ì  ì¡°ì •
        self.dynamic_threshold = self.transition_config['base_threshold']

        # âš ï¸ ì „í™˜ ì¹´ìš´í„° (ë¹ˆë²ˆí•œ ì „í™˜ ê°ì§€ìš©)
        self.recent_transitions = deque(maxlen=10)
        self.flip_count = 0
        self.last_flip_time = None

        # ğŸšï¸ Stability Score
        self.stability_score = 1.0  # 1.0 = ë§¤ìš° ì•ˆì •ì , 0.0 = ë§¤ìš° ë¶ˆì•ˆì •

        # ğŸ“Š Regimeë³„ í†µê³„ ì¶”ì 
        self.regime_statistics = {}
        self._initialize_regime_statistics()

        # ğŸ”® ì „í™˜ ì˜ˆì¸¡ ë©”ì»¤ë‹ˆì¦˜
        self.transition_warning = None
        self.transition_probability = 0.0

        # ğŸ”¥ğŸ”¥ğŸ”¥ ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ë°ì´í„° í†µí•© ì„¤ì • ğŸ”¥ğŸ”¥ğŸ”¥
        self.onchain_macro_config = {
            'onchain_weight': 0.40,  # ì˜¨ì²´ì¸ ì‹ í˜¸ì˜ ì „ì²´ ê°€ì¤‘ì¹˜
            'macro_weight': 0.30,  # ë§¤í¬ë¡œ ì‹ í˜¸ì˜ ì „ì²´ ê°€ì¤‘ì¹˜
            'traditional_weight': 0.30,  # ê¸°ì¡´ ì§€í‘œì˜ ì „ì²´ ê°€ì¤‘ì¹˜
            'signal_merge_method': 'weighted_average',  # weighted_average, max, consensus
            'contradiction_threshold': 0.5,  # ì‹ í˜¸ ëª¨ìˆœ ê°ì§€ ì„ê³„ê°’
            'min_confidence_threshold': 0.3  # ìµœì†Œ ì‹ ë¢°ë„
        }

        # ğŸ“Š ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ì‹ í˜¸ íˆìŠ¤í† ë¦¬
        self.onchain_signal_history = deque(maxlen=50)
        self.macro_signal_history = deque(maxlen=50)
        self.signal_correlation_history = deque(maxlen=50)

    def _initialize_regime_statistics(self):
        """Regimeë³„ í†µê³„ ì´ˆê¸°í™”"""
        regime_types = [
            'BULL_VOLATILITY', 'BULL_CONSOLIDATION', 'BULL_EXHAUSTION',
            'BEAR_VOLATILITY', 'BEAR_CONSOLIDATION', 'BEAR_CAPITULATION',
            'SIDEWAYS_COMPRESSION', 'SIDEWAYS_CHOP',
            'ACCUMULATION', 'DISTRIBUTION', 'UNCERTAIN'
        ]

        for regime in regime_types:
            self.regime_statistics[regime] = {
                'total_count': 0,
                'total_duration': timedelta(0),
                'avg_duration': timedelta(0),
                'avg_strength': 0.0,
                'transition_from': {},  # ì–´ë–¤ regimeì—ì„œ ì „í™˜ë˜ì–´ ì™”ëŠ”ì§€
                'transition_to': {},  # ì–´ë–¤ regimeìœ¼ë¡œ ì „í™˜ë˜ì—ˆëŠ”ì§€
                'success_rate': 0.0  # ì˜ˆì¸¡ ì„±ê³µë¥ 
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”¥ğŸ”¥ğŸ”¥ ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ë°ì´í„° ìœµí•© í•µì‹¬ ë©”ì„œë“œ ğŸ”¥ğŸ”¥ğŸ”¥
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _get_onchain_macro_signals(self):
        """
        ì˜¨ì²´ì¸ ë° ë§¤í¬ë¡œ ì‹ í˜¸ ìˆ˜ì§‘
        Returns: {'onchain': dict, 'macro': dict, 'merged': dict}
        """
        try:
            # ì˜¨ì²´ì¸ ì‹ í˜¸ ìˆ˜ì§‘
            onchain_signal = self.onchain_manager.get_comprehensive_onchain_signal()

            # ë§¤í¬ë¡œ ì‹ í˜¸ ìˆ˜ì§‘
            macro_signal = self.macro_manager.get_comprehensive_macro_signal()

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.onchain_signal_history.append({
                'timestamp': datetime.now(),
                'signal': onchain_signal['signal'],
                'score': onchain_signal['score']
            })

            self.macro_signal_history.append({
                'timestamp': datetime.now(),
                'signal': macro_signal['signal'],
                'score': macro_signal['score']
            })

            # ì‹ í˜¸ ë³‘í•©
            merged_signal = self._merge_onchain_macro_signals(onchain_signal, macro_signal)

            return {
                'onchain': onchain_signal,
                'macro': macro_signal,
                'merged': merged_signal
            }

        except Exception as e:
            self.logger.error(f"Onchain/Macro signal collection error: {e}")
            return {
                'onchain': {'score': 0.0, 'signal': 'NEUTRAL', 'details': {}, 'component_scores': {}},
                'macro': {'score': 0.0, 'signal': 'NEUTRAL', 'details': {}, 'component_scores': {}},
                'merged': {'score': 0.0, 'signal': 'NEUTRAL', 'confidence': 0.5}
            }

    def _merge_onchain_macro_signals(self, onchain_signal, macro_signal):
        """
        ì˜¨ì²´ì¸ ë° ë§¤í¬ë¡œ ì‹ í˜¸ë¥¼ ë³‘í•©í•˜ì—¬ í†µí•© ì‹ í˜¸ ìƒì„±
        """
        try:
            method = self.onchain_macro_config['signal_merge_method']
            onchain_weight = self.onchain_macro_config['onchain_weight']
            macro_weight = self.onchain_macro_config['macro_weight']

            # ê°€ì¤‘ í‰ê·  ë°©ì‹
            if method == 'weighted_average':
                merged_score = (
                                       onchain_signal['score'] * onchain_weight +
                                       macro_signal['score'] * macro_weight
                               ) / (onchain_weight + macro_weight)

            # ìµœëŒ€ê°’ ë°©ì‹ (ë” ê°•í•œ ì‹ í˜¸ ì±„íƒ)
            elif method == 'max':
                if abs(onchain_signal['score']) > abs(macro_signal['score']):
                    merged_score = onchain_signal['score']
                else:
                    merged_score = macro_signal['score']

            # í•©ì˜ ë°©ì‹ (ë‘ ì‹ í˜¸ê°€ ì¼ì¹˜í•  ë•Œë§Œ ê°•í•œ ì‹ í˜¸)
            elif method == 'consensus':
                if (onchain_signal['score'] > 0 and macro_signal['score'] > 0) or \
                        (onchain_signal['score'] < 0 and macro_signal['score'] < 0):
                    # ê°™ì€ ë°©í–¥ì´ë©´ í‰ê· 
                    merged_score = (onchain_signal['score'] + macro_signal['score']) / 2
                else:
                    # ë‹¤ë¥¸ ë°©í–¥ì´ë©´ ì•½í™”
                    merged_score = (onchain_signal['score'] + macro_signal['score']) / 4

            else:
                merged_score = (onchain_signal['score'] + macro_signal['score']) / 2

            # ì‹ í˜¸ ëª¨ìˆœ ê°ì§€
            contradiction_level = self._detect_signal_contradiction(
                onchain_signal, macro_signal
            )

            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_merged_confidence(
                onchain_signal, macro_signal, contradiction_level
            )

            # ìµœì¢… ì‹ í˜¸ ìƒì„±
            if merged_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif merged_score > 0.2:
                signal = 'BULLISH'
            elif merged_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif merged_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            return {
                'score': merged_score,
                'signal': signal,
                'confidence': confidence,
                'contradiction_level': contradiction_level,
                'onchain_contribution': onchain_signal['score'] * onchain_weight,
                'macro_contribution': macro_signal['score'] * macro_weight
            }

        except Exception as e:
            self.logger.error(f"Signal merge error: {e}")
            return {
                'score': 0.0,
                'signal': 'NEUTRAL',
                'confidence': 0.5,
                'contradiction_level': 0.0,
                'onchain_contribution': 0.0,
                'macro_contribution': 0.0
            }

    def _detect_signal_contradiction(self, onchain_signal, macro_signal):
        """
        ì˜¨ì²´ì¸ê³¼ ë§¤í¬ë¡œ ì‹ í˜¸ ê°„ ëª¨ìˆœ ê°ì§€
        Returns: float (0.0 ~ 1.0, ë†’ì„ìˆ˜ë¡ ëª¨ìˆœ ì‹¬ê°)
        """
        try:
            onchain_score = onchain_signal['score']
            macro_score = macro_signal['score']

            # ë°©í–¥ ì°¨ì´ ê³„ì‚°
            if (onchain_score > 0 and macro_score < 0) or \
                    (onchain_score < 0 and macro_score > 0):
                # ë°˜ëŒ€ ë°©í–¥
                contradiction = min(abs(onchain_score - macro_score), 2.0) / 2.0
            else:
                # ê°™ì€ ë°©í–¥ì´ì§€ë§Œ ê°•ë„ ì°¨ì´
                contradiction = abs(abs(onchain_score) - abs(macro_score)) / 2.0

            return np.clip(contradiction, 0.0, 1.0)

        except Exception as e:
            self.logger.debug(f"Contradiction detection error: {e}")
            return 0.0

    def _calculate_merged_confidence(self, onchain_signal, macro_signal, contradiction_level):
        """
        ë³‘í•©ëœ ì‹ í˜¸ì˜ ì‹ ë¢°ë„ ê³„ì‚°
        """
        try:
            # ê¸°ë³¸ ì‹ ë¢°ë„: ë‘ ì‹ í˜¸ì˜ ê°•ë„ í‰ê· 
            base_confidence = (abs(onchain_signal['score']) + abs(macro_signal['score'])) / 2

            # ëª¨ìˆœ í˜ë„í‹°
            contradiction_penalty = contradiction_level * 0.3

            # ì¼ì¹˜ ë³´ë„ˆìŠ¤ (ë‘ ì‹ í˜¸ê°€ ê°™ì€ ë°©í–¥ì´ë©´)
            if (onchain_signal['score'] > 0 and macro_signal['score'] > 0) or \
                    (onchain_signal['score'] < 0 and macro_signal['score'] < 0):
                alignment_bonus = 0.2
            else:
                alignment_bonus = 0.0

            # ìµœì¢… ì‹ ë¢°ë„
            confidence = base_confidence + alignment_bonus - contradiction_penalty

            # íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì¡°ì •
            if len(self.onchain_signal_history) >= 5 and len(self.macro_signal_history) >= 5:
                recent_onchain = [s['score'] for s in list(self.onchain_signal_history)[-5:]]
                recent_macro = [s['score'] for s in list(self.macro_signal_history)[-5:]]

                # ì¼ê´€ì„± ì²´í¬
                onchain_std = np.std(recent_onchain)
                macro_std = np.std(recent_macro)

                consistency_bonus = (2.0 - onchain_std - macro_std) / 20  # ìµœëŒ€ 0.1
                confidence += consistency_bonus

            return np.clip(confidence, 0.0, 1.0)

        except Exception as e:
            self.logger.debug(f"Merged confidence calculation error: {e}")
            return 0.5

    def _integrate_onchain_macro_to_regime(self, base_regime_score, merged_signal):
        """
        ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ì‹ í˜¸ë¥¼ ê¸°ì¡´ regime ì ìˆ˜ì— í†µí•©
        """
        try:
            traditional_weight = self.onchain_macro_config['traditional_weight']
            onchain_macro_weight = 1.0 - traditional_weight

            # ë³‘í•© ì‹ í˜¸ì˜ ê¸°ì—¬ë„
            signal_contribution = merged_signal['score'] * merged_signal['confidence']

            # í†µí•© ì ìˆ˜ ê³„ì‚°
            integrated_score = (
                    base_regime_score * traditional_weight +
                    signal_contribution * onchain_macro_weight
            )

            # ì‹ í˜¸ ê°•ë„ì— ë”°ë¥¸ ì¡°ì •
            if abs(merged_signal['score']) > 0.7:
                # ê°•í•œ ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ì‹ í˜¸ëŠ” ë” í° ì˜í–¥
                integrated_score *= 1.1

            # ëª¨ìˆœì´ ì‹¬í•˜ë©´ ì‹ ë¢°ë„ ê°ì†Œ
            if merged_signal['contradiction_level'] > self.onchain_macro_config['contradiction_threshold']:
                integrated_score *= 0.9

            return np.clip(integrated_score, -1.0, 1.0)

        except Exception as e:
            self.logger.error(f"Regime integration error: {e}")
            return base_regime_score

    def _adjust_regime_based_on_onchain_macro(self, base_regime, merged_signal):
        """
        ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ì‹ í˜¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ regime ì¡°ì •
        íŠ¹ì • ì¡°ê±´ì—ì„œëŠ” regimeì„ overrideí•  ìˆ˜ ìˆìŒ
        """
        try:
            # ë§¤ìš° ê°•í•œ ì‹ í˜¸ëŠ” regime override ê°€ëŠ¥
            if abs(merged_signal['score']) > 0.8 and merged_signal['confidence'] > 0.7:

                if merged_signal['signal'] == 'STRONG_BULLISH':
                    # ì˜¨ì²´ì¸/ë§¤í¬ë¡œê°€ ê°•í•œ ìƒìŠ¹ì„ ë‚˜íƒ€ë‚´ë©´
                    if 'BEAR' in base_regime or base_regime == 'UNCERTAIN':
                        self.logger.info(
                            f"ğŸ”¥ ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ì‹ í˜¸ë¡œ Regime Override: "
                            f"{base_regime} -> ACCUMULATION (ì‹ í˜¸: {merged_signal['signal']})"
                        )
                        return 'ACCUMULATION'

                elif merged_signal['signal'] == 'STRONG_BEARISH':
                    # ì˜¨ì²´ì¸/ë§¤í¬ë¡œê°€ ê°•í•œ í•˜ë½ì„ ë‚˜íƒ€ë‚´ë©´
                    if 'BULL' in base_regime or base_regime == 'UNCERTAIN':
                        self.logger.info(
                            f"ğŸ”¥ ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ì‹ í˜¸ë¡œ Regime Override: "
                            f"{base_regime} -> DISTRIBUTION (ì‹ í˜¸: {merged_signal['signal']})"
                        )
                        return 'DISTRIBUTION'

            return base_regime

        except Exception as e:
            self.logger.error(f"Regime adjustment error: {e}")
            return base_regime

    def get_onchain_macro_report(self):
        """
        ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ë°ì´í„° ìƒì„¸ ë¦¬í¬íŠ¸
        """
        try:
            signals = self._get_onchain_macro_signals()

            report = {
                'timestamp': datetime.now().isoformat(),
                'onchain_signal': {
                    'overall': signals['onchain']['signal'],
                    'score': signals['onchain']['score'],
                    'details': signals['onchain']['details'],
                    'component_scores': signals['onchain']['component_scores']
                },
                'macro_signal': {
                    'overall': signals['macro']['signal'],
                    'score': signals['macro']['score'],
                    'details': signals['macro']['details'],
                    'component_scores': signals['macro']['component_scores']
                },
                'merged_signal': signals['merged'],
                'historical_correlation': self._calculate_signal_correlation(),
                'signal_quality_metrics': self._calculate_signal_quality_metrics()
            }

            return report

        except Exception as e:
            self.logger.error(f"Onchain/Macro report generation error: {e}")
            return {}

    def _calculate_signal_correlation(self):
        """ì˜¨ì²´ì¸ê³¼ ë§¤í¬ë¡œ ì‹ í˜¸ ê°„ ìƒê´€ê´€ê³„ ê³„ì‚°"""
        try:
            if len(self.onchain_signal_history) < 10 or len(self.macro_signal_history) < 10:
                return 0.0

            onchain_scores = [s['score'] for s in list(self.onchain_signal_history)[-20:]]
            macro_scores = [s['score'] for s in list(self.macro_signal_history)[-20:]]

            correlation = np.corrcoef(onchain_scores, macro_scores)[0, 1]

            # ìƒê´€ê´€ê³„ íˆìŠ¤í† ë¦¬ ì €ì¥
            self.signal_correlation_history.append({
                'timestamp': datetime.now(),
                'correlation': correlation
            })

            return correlation

        except Exception as e:
            self.logger.debug(f"Signal correlation calculation error: {e}")
            return 0.0

    def _calculate_signal_quality_metrics(self):
        """ì‹ í˜¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {
                'onchain_consistency': 0.0,
                'macro_consistency': 0.0,
                'overall_reliability': 0.0,
                'prediction_accuracy': 0.0
            }

            # ì˜¨ì²´ì¸ ì¼ê´€ì„±
            if len(self.onchain_signal_history) >= 10:
                recent_signals = [s['signal'] for s in list(self.onchain_signal_history)[-10:]]
                unique_signals = len(set(recent_signals))
                metrics['onchain_consistency'] = 1.0 - (unique_signals / 10)

            # ë§¤í¬ë¡œ ì¼ê´€ì„±
            if len(self.macro_signal_history) >= 10:
                recent_signals = [s['signal'] for s in list(self.macro_signal_history)[-10:]]
                unique_signals = len(set(recent_signals))
                metrics['macro_consistency'] = 1.0 - (unique_signals / 10)

            # ì „ì²´ ì‹ ë¢°ë„
            metrics['overall_reliability'] = (
                    metrics['onchain_consistency'] * 0.5 +
                    metrics['macro_consistency'] * 0.5
            )

            return metrics

        except Exception as e:
            self.logger.debug(f"Signal quality metrics calculation error: {e}")
            return {
                'onchain_consistency': 0.0,
                'macro_consistency': 0.0,
                'overall_reliability': 0.0,
                'prediction_accuracy': 0.0
            }

    # (ì´í•˜ ê¸°ì¡´ ë©”ì„œë“œë“¤ ìƒëµ - ë„ˆë¬´ ê¸¸ì–´ì„œ ì£¼ìš” ë¶€ë¶„ë§Œ í¬í•¨)
    # ... (ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì€ market_regime_analyzer5.pyì™€ ë™ì¼)

    def get_comprehensive_analysis_report(self):
        """
        ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ (ëª¨ë“  ì‹œìŠ¤í…œì˜ ì •ë³´ í†µí•© + ğŸ”¥ğŸ”¥ ìœ ë™ì„± í¬í•¨ ğŸ”¥ğŸ”¥)
        """
        try:
            # ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸
            mtf_report = self.mtf_consensus.get_consensus_report()
            
            # ğŸ”¥ğŸ”¥ ìœ ë™ì„± ì¢…í•© ë¦¬í¬íŠ¸
            liquidity_report = self.liquidity_detector.get_comprehensive_liquidity_report()
            
            return {
                'timestamp': datetime.now().isoformat(),
                'regime_analysis': {
                    'current_regime': self.current_regime,
                    'regime_strength': self.current_regime_strength,
                    'regime_duration': self.current_regime_duration.total_seconds() if self.current_regime_duration else 0,
                    'stability_score': self.stability_score,
                    'transition_probability': self.transition_probability
                },
                'adaptive_weights': self.get_current_weights(),
                'regime_stability': self.get_regime_stability_report(),
                'onchain_macro': self.get_onchain_macro_report(),
                'multidimensional_confidence': self.confidence_scorer.get_confidence_report(),
                'multi_timeframe_consensus': mtf_report,
                'liquidity_analysis': liquidity_report  # ğŸ”¥ğŸ”¥ğŸ”¥ ìœ ë™ì„± ë¶„ì„ ì¶”ê°€ ğŸ”¥ğŸ”¥ğŸ”¥
            }
        except Exception as e:
            self.logger.error(f"Comprehensive analysis report error: {e}")
            return {}

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (market_regime_analyzer5.pyì™€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ)
    # _get_cached_data, _set_cached_data, _calculate_rsi, _calculate_macd, 
    # _analyze_volume_profile, _get_market_breadth, _get_macro_trend,
    # _get_market_volatility, _get_altcoin_fund_flow, _calculate_market_momentum,
    # _estimate_market_sentiment, _detect_market_state, _calculate_indicator_confidence,
    # _update_adaptive_weights, _calculate_weighted_regime_score, 
    # _track_indicator_performance, _evaluate_prediction_performance,
    # get_current_weights, get_weight_history,
    # _calculate_regime_strength, _calculate_stability_score, _update_dynamic_threshold,
    # _should_transition_to_new_regime, _is_frequent_flip, _predict_transition_probability,
    # _record_regime_transition, _update_regime_statistics, get_regime_stability_report,
    # analyze (ë©”ì¸ ë¶„ì„ í•¨ìˆ˜), _calculate_confidence_score
    # ... ë“±ë“± (ë„ˆë¬´ ê¸¸ì–´ì ¸ì„œ ìƒëµ)

# ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ:
# analyzer = MarketRegimeAnalyzer(market_data_manager)
#
# # ê¸°ë³¸ ë¶„ì„ (ê¸°ì¡´ê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
# regime, fund_flow = analyzer.analyze()
#
# # ğŸ”¥ğŸ”¥ğŸ”¥ ì¶”ê°€ 1: ìœ ë™ì„± ì¢…í•© ë¦¬í¬íŠ¸ ğŸ”¥ğŸ”¥ğŸ”¥
# liquidity_report = analyzer.liquidity_detector.get_comprehensive_liquidity_report()
# print(f"ìœ ë™ì„± ë¦¬í¬íŠ¸: {liquidity_report}")
#
# # ğŸ”¥ğŸ”¥ğŸ”¥ ì¶”ê°€ 2: ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ (ëª¨ë“  ì‹œìŠ¤í…œ í†µí•© + ìœ ë™ì„±) ğŸ”¥ğŸ”¥ğŸ”¥
# comprehensive_report = analyzer.get_comprehensive_analysis_report()
# print(f"ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸: {comprehensive_report}")
