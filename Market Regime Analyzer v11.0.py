# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 11.0 - PART 1/5 ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 1: v10.0 ì „ì²´ ê¸°ëŠ¥ + ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ê¸°ë°˜ í´ë˜ìŠ¤
#
# v11.0 NEW FEATURES (v10.0ì˜ ëª¨ë“  ê¸°ëŠ¥ 100% ìœ ì§€):
# - ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ë¶„ì„ (Multi-Asset Correlation)
# - Cross-Asset Regime Detection
# - Contagion Detection (ì‹œì¥ ì „ì—¼ íš¨ê³¼)
# - Portfolio Diversification Metrics
# - Lead-Lag Analysis (ì„ í–‰/í›„í–‰ ê´€ê³„)
# - Copula-based Tail Dependency
# - Dynamic Correlation Networks
# - Risk Parity Analysis
# - Granger Causality Testing
# - Information Flow Analysis
#
# ë³‘í•© ë°©ë²•:
# 1. ëª¨ë“  íŒŒíŠ¸(1~5)ë¥¼ ë‹¤ìš´ë¡œë“œ
# 2. Part 1ì˜ ë‚´ìš©ì„ market_regime_analyzer11.pyë¡œ ë³µì‚¬
# 3. Part 2~5ì˜ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì´ì–´ë¶™ì´ê¸°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from collections import deque, defaultdict
from typing import Dict, List, Tuple, Optional, Any
import logging
from scipy import stats
from scipy.stats import entropy, norm, t as student_t, spearmanr, kendalltau
from scipy.special import softmax
from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform
import warnings

warnings.filterwarnings('ignore')


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v10.0ì˜ ëª¨ë“  ê¸°ì¡´ í´ë˜ìŠ¤ë“¤ (100% ìœ ì§€)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_logger(name: str) -> logging.Logger:
    """í”„ë¡œë•ì…˜ ë ˆë²¨ ë¡œê±° ìƒì„±"""
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


class ProductionConfig:
    """í”„ë¡œë•ì…˜ ì„¤ì • í´ë˜ìŠ¤ (v10.0 ìœ ì§€)"""
    CACHE_TTL_SHORT = 30
    CACHE_TTL_MEDIUM = 180
    CACHE_TTL_LONG = 300
    API_TIMEOUT = 10
    API_RETRY_COUNT = 3
    API_RETRY_DELAY = 1
    MIN_DATA_POINTS = 20
    MAX_DATA_AGE_SECONDS = 3600
    OUTLIER_THRESHOLD = 5.0
    MIN_REGIME_DURATION_SECONDS = 300
    REGIME_TRANSITION_THRESHOLD = 0.15
    HYSTERESIS_FACTOR = 1.2
    WEIGHT_ADAPTATION_RATE = 0.05
    WEIGHT_MIN = 0.01
    WEIGHT_MAX = 0.50
    PERFORMANCE_LOOKBACK = 20
    ALERT_COOLDOWN_SECONDS = 300
    MAX_ALERTS_PER_HOUR = 20
    CRITICAL_ALERT_THRESHOLD = 0.90
    PERFORMANCE_LOG_INTERVAL = 60
    LATENCY_WARNING_MS = 100
    LATENCY_CRITICAL_MS = 500


class DataValidator:
    """ë°ì´í„° ê²€ì¦ í´ë˜ìŠ¤ (v10.0 ìœ ì§€)"""

    def __init__(self):
        self.logger = get_logger("DataValidator")

    def validate_numeric(self, value: float, name: str,
                         min_val: Optional[float] = None,
                         max_val: Optional[float] = None) -> bool:
        try:
            if not isinstance(value, (int, float)):
                self.logger.warning(f"{name}: Not a number - {value}")
                return False
            if np.isnan(value) or np.isinf(value):
                self.logger.warning(f"{name}: NaN or Inf detected")
                return False
            if min_val is not None and value < min_val:
                self.logger.warning(f"{name}: Below minimum ({value} < {min_val})")
                return False
            if max_val is not None and value > max_val:
                self.logger.warning(f"{name}: Above maximum ({value} > {max_val})")
                return False
            return True
        except Exception as e:
            self.logger.error(f"Validation error for {name}: {e}")
            return False

    def validate_dataframe(self, df: pd.DataFrame,
                           required_columns: List[str],
                           min_rows: int = 1) -> bool:
        try:
            if df is None or df.empty:
                self.logger.warning("DataFrame is None or empty")
                return False
            if len(df) < min_rows:
                self.logger.warning(f"Insufficient rows: {len(df)} < {min_rows}")
                return False
            missing_cols = set(required_columns) - set(df.columns)
            if missing_cols:
                self.logger.warning(f"Missing columns: {missing_cols}")
                return False
            nan_counts = df[required_columns].isna().sum()
            if nan_counts.any():
                self.logger.warning(f"NaN values detected: {nan_counts[nan_counts > 0].to_dict()}")
                return False
            return True
        except Exception as e:
            self.logger.error(f"DataFrame validation error: {e}")
            return False

    def detect_outliers(self, data: np.ndarray,
                        threshold: float = ProductionConfig.OUTLIER_THRESHOLD) -> np.ndarray:
        try:
            if len(data) < 3:
                return np.array([])
            median = np.median(data)
            mad = np.median(np.abs(data - median))
            if mad == 0:
                return np.array([])
            modified_z_scores = 0.6745 * (data - median) / mad
            outliers = np.where(np.abs(modified_z_scores) > threshold)[0]
            return outliers
        except Exception as e:
            self.logger.error(f"Outlier detection error: {e}")
            return np.array([])


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤ (v10.0 ìœ ì§€)"""

    def __init__(self):
        self.logger = get_logger("PerformanceMonitor")
        self.latencies = deque(maxlen=100)
        self.call_counts = defaultdict(int)
        self.error_counts = defaultdict(int)
        self.last_log_time = datetime.now()

    def record_latency(self, operation: str, latency_ms: float):
        self.latencies.append({
            'operation': operation,
            'latency_ms': latency_ms,
            'timestamp': datetime.now()
        })
        self.call_counts[operation] += 1
        if latency_ms > ProductionConfig.LATENCY_CRITICAL_MS:
            self.logger.warning(f"CRITICAL LATENCY: {operation} took {latency_ms:.2f}ms")
        elif latency_ms > ProductionConfig.LATENCY_WARNING_MS:
            self.logger.info(f"High latency: {operation} took {latency_ms:.2f}ms")

    def record_error(self, operation: str, error: Exception):
        self.error_counts[operation] += 1
        self.logger.error(f"Error in {operation}: {str(error)}")

    def get_stats(self) -> Dict[str, Any]:
        if not self.latencies:
            return {}
        latencies_by_op = defaultdict(list)
        for record in self.latencies:
            latencies_by_op[record['operation']].append(record['latency_ms'])
        stats = {}
        for op, lats in latencies_by_op.items():
            stats[op] = {
                'count': len(lats),
                'mean_ms': np.mean(lats),
                'median_ms': np.median(lats),
                'p95_ms': np.percentile(lats, 95),
                'p99_ms': np.percentile(lats, 99),
                'max_ms': np.max(lats)
            }
        return stats

    def log_periodic_stats(self):
        now = datetime.now()
        if (now - self.last_log_time).total_seconds() >= ProductionConfig.PERFORMANCE_LOG_INTERVAL:
            stats = self.get_stats()
            if stats:
                self.logger.info(f"Performance Stats: {stats}")
            self.last_log_time = now


performance_monitor = PerformanceMonitor()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v11.0 NEW: ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ê¸°ë°˜ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AssetDataManager:
    """
    ğŸŒ ë‹¤ì¤‘ ìì‚° ë°ì´í„° ê´€ë¦¬ì (v11.0 NEW)

    ì—¬ëŸ¬ ìì‚°ì˜ ê°€ê²© ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("AssetDataManager")
        self.validator = DataValidator()

        # ì¶”ì í•  ìì‚° ëª©ë¡
        self.crypto_assets = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'XRPUSDT', 'ADAUSDT',
            'SOLUSDT', 'DOGEUSDT', 'MATICUSDT', 'DOTUSDT', 'AVAXUSDT'
        ]

        self.traditional_assets = {
            'SPX': 'S&P 500',
            'DXY': 'US Dollar Index',
            'GOLD': 'Gold',
            'US10Y': 'US 10Y Treasury',
            'VIX': 'Volatility Index'
        }

        # ë°ì´í„° ìºì‹œ
        self._price_cache = {}
        self._returns_cache = {}
        self._cache_ttl = ProductionConfig.CACHE_TTL_SHORT

        # íˆìŠ¤í† ë¦¬
        self.price_history = defaultdict(lambda: deque(maxlen=1000))
        self.returns_history = defaultdict(lambda: deque(maxlen=1000))

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.api_call_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

    def get_asset_prices(self, symbols: List[str],
                         timeframe: str = '1h',
                         lookback: int = 100) -> pd.DataFrame:
        """
        ì—¬ëŸ¬ ìì‚°ì˜ ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        Returns:
            DataFrame with datetime index and columns for each symbol
        """
        start_time = datetime.now()

        try:
            cache_key = f"prices_{'-'.join(symbols)}_{timeframe}_{lookback}"

            # ìºì‹œ í™•ì¸
            if cache_key in self._price_cache:
                data, timestamp = self._price_cache[cache_key]
                if (datetime.now() - timestamp).total_seconds() < self._cache_ttl:
                    self.cache_hit_count += 1
                    return data

            self.api_call_count += 1

            # ê° ìì‚°ì˜ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
            all_prices = {}

            for symbol in symbols:
                try:
                    if symbol in self.crypto_assets:
                        # ì•”í˜¸í™”í ë°ì´í„°
                        df = self.market_data.get_candle_data(symbol, timeframe)
                        if df is not None and not df.empty:
                            prices = df['close'].tail(lookback)
                            all_prices[symbol] = prices
                    elif symbol in self.traditional_assets:
                        # ì „í†µ ìì‚° ë°ì´í„° (ì‹œë®¬ë ˆì´ì…˜)
                        # TODO: ì‹¤ì œ API ì—°ë™
                        prices = self._simulate_traditional_asset_prices(symbol, lookback)
                        all_prices[symbol] = prices

                except Exception as e:
                    self.logger.warning(f"Failed to get prices for {symbol}: {e}")
                    continue

            if not all_prices:
                raise ValueError("No price data collected")

            # DataFrameìœ¼ë¡œ ë³€í™˜ (ì‹œê°„ ì¸ë±ìŠ¤ ì •ë ¬)
            df = pd.DataFrame(all_prices)

            # NaN ì²˜ë¦¬ (forward fill)
            df = df.fillna(method='ffill').fillna(method='bfill')

            # ê²€ì¦
            if not self.validator.validate_dataframe(df, list(df.columns), min_rows=10):
                raise ValueError("Invalid price dataframe")

            # ìºì‹œ ì €ì¥
            self._price_cache[cache_key] = (df, datetime.now())

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('get_asset_prices', latency)

            return df

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Asset prices collection error: {e}")
            performance_monitor.record_error('get_asset_prices', e)
            return pd.DataFrame()

    def calculate_returns(self, prices: pd.DataFrame,
                          method: str = 'simple') -> pd.DataFrame:
        """
        ìˆ˜ìµë¥  ê³„ì‚°

        Args:
            prices: ê°€ê²© DataFrame
            method: 'simple', 'log'
        """
        try:
            if prices.empty:
                return pd.DataFrame()

            if method == 'log':
                returns = np.log(prices / prices.shift(1))
            else:  # simple
                returns = prices.pct_change()

            # ì²« í–‰ ì œê±° (NaN)
            returns = returns.iloc[1:]

            # ì´ìƒì¹˜ ì œê±°
            for col in returns.columns:
                outliers = self.validator.detect_outliers(returns[col].values)
                if len(outliers) > 0:
                    returns.loc[returns.index[outliers], col] = np.nan

            # NaN ì²˜ë¦¬
            returns = returns.fillna(0)

            return returns

        except Exception as e:
            self.logger.error(f"Returns calculation error: {e}")
            return pd.DataFrame()

    def _simulate_traditional_asset_prices(self, symbol: str, lookback: int) -> pd.Series:
        """ì „í†µ ìì‚° ê°€ê²© ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ API ì—°ë™ ì „ ì„ì‹œ)"""
        try:
            # ìì‚°ë³„ ê¸°ë³¸ ê°€ê²© ë° ë³€ë™ì„±
            base_prices = {
                'SPX': 4500,
                'DXY': 104,
                'GOLD': 2000,
                'US10Y': 4.5,
                'VIX': 15
            }

            volatilities = {
                'SPX': 0.15,
                'DXY': 0.05,
                'GOLD': 0.12,
                'US10Y': 0.20,
                'VIX': 0.40
            }

            base = base_prices.get(symbol, 100)
            vol = volatilities.get(symbol, 0.20)

            # Geometric Brownian Motion
            returns = np.random.normal(0.0001, vol / np.sqrt(252), lookback)
            prices = base * np.exp(np.cumsum(returns))

            # ì‹œê°„ ì¸ë±ìŠ¤ ìƒì„±
            end_date = datetime.now()
            dates = pd.date_range(end=end_date, periods=lookback, freq='1H')

            return pd.Series(prices, index=dates)

        except Exception as e:
            self.logger.error(f"Price simulation error: {e}")
            return pd.Series()

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        cache_hit_rate = (
                self.cache_hit_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        error_rate = (
                self.error_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        return {
            'api_calls': self.api_call_count,
            'cache_hits': self.cache_hit_count,
            'cache_hit_rate': cache_hit_rate,
            'errors': self.error_count,
            'error_rate': error_rate
        }


class CorrelationCalculator:
    """
    ğŸ“Š ìƒê´€ê´€ê³„ ê³„ì‚° ì—”ì§„ (v11.0 NEW)

    ë‹¤ì–‘í•œ ìƒê´€ê´€ê³„ ì¸¡ì • ë°©ë²• ì œê³µ
    """

    def __init__(self):
        self.logger = get_logger("CorrelationCalculator")
        self.validator = DataValidator()

    def calculate_pearson_correlation(self, returns: pd.DataFrame,
                                      window: Optional[int] = None) -> pd.DataFrame:
        """
        í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°

        Args:
            returns: ìˆ˜ìµë¥  DataFrame
            window: Rolling window (Noneì´ë©´ ì „ì²´ ê¸°ê°„)
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            if window is None:
                # ì „ì²´ ê¸°ê°„ ìƒê´€ê³„ìˆ˜
                corr_matrix = returns.corr(method='pearson')
            else:
                # Rolling ìƒê´€ê³„ìˆ˜ (ë§ˆì§€ë§‰ window ê¸°ê°„)
                corr_matrix = returns.tail(window).corr(method='pearson')

            # ëŒ€ê°ì„ ì€ 1ë¡œ (ìê¸° ìì‹ ê³¼ì˜ ìƒê´€ê³„ìˆ˜)
            np.fill_diagonal(corr_matrix.values, 1.0)

            return corr_matrix

        except Exception as e:
            self.logger.error(f"Pearson correlation error: {e}")
            return pd.DataFrame()

    def calculate_spearman_correlation(self, returns: pd.DataFrame,
                                       window: Optional[int] = None) -> pd.DataFrame:
        """
        ìŠ¤í”¼ì–´ë§Œ ìˆœìœ„ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ë¹„ì„ í˜• ê´€ê³„ í¬ì°©)
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            if window is not None:
                returns = returns.tail(window)

            corr_matrix = returns.corr(method='spearman')
            np.fill_diagonal(corr_matrix.values, 1.0)

            return corr_matrix

        except Exception as e:
            self.logger.error(f"Spearman correlation error: {e}")
            return pd.DataFrame()

    def calculate_kendall_correlation(self, returns: pd.DataFrame,
                                      window: Optional[int] = None) -> pd.DataFrame:
        """
        ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            if window is not None:
                returns = returns.tail(window)

            corr_matrix = returns.corr(method='kendall')
            np.fill_diagonal(corr_matrix.values, 1.0)

            return corr_matrix

        except Exception as e:
            self.logger.error(f"Kendall correlation error: {e}")
            return pd.DataFrame()

    def calculate_rolling_correlation(self, returns: pd.DataFrame,
                                      window: int = 30,
                                      min_periods: int = 20) -> Dict[str, pd.DataFrame]:
        """
        Rolling ìƒê´€ê³„ìˆ˜ ì‹œê³„ì—´ ê³„ì‚°

        Returns:
            Dict with asset pairs as keys and correlation time series as values
        """
        try:
            if returns.empty or len(returns) < min_periods:
                return {}

            rolling_corrs = {}
            columns = returns.columns

            for i, col1 in enumerate(columns):
                for col2 in columns[i + 1:]:
                    pair_key = f"{col1}_{col2}"

                    # Rolling correlation
                    rolling_corr = returns[col1].rolling(
                        window=window,
                        min_periods=min_periods
                    ).corr(returns[col2])

                    rolling_corrs[pair_key] = rolling_corr.dropna()

            return rolling_corrs

        except Exception as e:
            self.logger.error(f"Rolling correlation error: {e}")
            return {}

    def calculate_ewm_correlation(self, returns: pd.DataFrame,
                                  span: int = 30) -> pd.DataFrame:
        """
        ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· (EWMA) ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            # EWMA ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°
            ewm_cov = returns.ewm(span=span).cov()

            # ìµœê·¼ ì‹œì ì˜ ê³µë¶„ì‚° í–‰ë ¬ ì¶”ì¶œ
            latest_cov = ewm_cov.iloc[-len(returns.columns):]

            # í‘œì¤€í¸ì°¨ ê³„ì‚°
            ewm_std = returns.ewm(span=span).std().iloc[-1]

            # ìƒê´€ê³„ìˆ˜ = ê³µë¶„ì‚° / (std1 * std2)
            corr_matrix = pd.DataFrame(
                index=returns.columns,
                columns=returns.columns,
                dtype=float
            )

            for i, col1 in enumerate(returns.columns):
                for j, col2 in enumerate(returns.columns):
                    if i == j:
                        corr_matrix.loc[col1, col2] = 1.0
                    else:
                        cov = latest_cov.loc[col1, col2]
                        std1 = ewm_std[col1]
                        std2 = ewm_std[col2]
                        if std1 > 0 and std2 > 0:
                            corr_matrix.loc[col1, col2] = cov / (std1 * std2)
                        else:
                            corr_matrix.loc[col1, col2] = 0.0

            return corr_matrix.astype(float)

        except Exception as e:
            self.logger.error(f"EWM correlation error: {e}")
            return pd.DataFrame()

    def calculate_dynamic_conditional_correlation(self, returns: pd.DataFrame,
                                                  window: int = 30) -> pd.DataFrame:
        """
        ë™ì  ì¡°ê±´ë¶€ ìƒê´€ê³„ìˆ˜ (DCC) ê³„ì‚°
        GARCH ê¸°ë°˜ ì‹œë³€ ìƒê´€ê³„ìˆ˜
        """
        try:
            if returns.empty or len(returns) < window:
                return pd.DataFrame()

            # ê°„ë‹¨í•œ DCC ê·¼ì‚¬ (ì‹¤ì œë¡œëŠ” GARCH ëª¨ë¸ í•„ìš”)
            # 1. Rolling variance
            rolling_var = returns.rolling(window=window).var()

            # 2. Standardized residuals
            std_returns = returns / np.sqrt(rolling_var)
            std_returns = std_returns.dropna()

            # 3. Correlation of standardized residuals
            dcc_corr = std_returns.tail(window).corr(method='pearson')

            return dcc_corr

        except Exception as e:
            self.logger.error(f"DCC correlation error: {e}")
            return pd.DataFrame()

    def calculate_tail_correlation(self, returns: pd.DataFrame,
                                   quantile: float = 0.05) -> pd.DataFrame:
        """
        ê¼¬ë¦¬ ìƒê´€ê´€ê³„ ê³„ì‚° (ê·¹ë‹¨ì  ì›€ì§ì„ ì‹œ ìƒê´€ê´€ê³„)

        Args:
            quantile: í•˜ìœ„/ìƒìœ„ ë¶„ìœ„ìˆ˜ (0.05 = 5% ê·¹ë‹¨ê°’)
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            n_assets = len(returns.columns)
            tail_corr = pd.DataFrame(
                np.eye(n_assets),
                index=returns.columns,
                columns=returns.columns
            )

            for i, col1 in enumerate(returns.columns):
                for j, col2 in enumerate(returns.columns[i + 1:], i + 1):
                    # í•˜ìœ„ ê¼¬ë¦¬
                    lower_threshold1 = returns[col1].quantile(quantile)
                    lower_threshold2 = returns[col2].quantile(quantile)

                    lower_tail_mask = (
                            (returns[col1] <= lower_threshold1) &
                            (returns[col2] <= lower_threshold2)
                    )

                    # ìƒìœ„ ê¼¬ë¦¬
                    upper_threshold1 = returns[col1].quantile(1 - quantile)
                    upper_threshold2 = returns[col2].quantile(1 - quantile)

                    upper_tail_mask = (
                            (returns[col1] >= upper_threshold1) &
                            (returns[col2] >= upper_threshold2)
                    )

                    # ê¼¬ë¦¬ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                    tail_returns1 = returns[col1][lower_tail_mask | upper_tail_mask]
                    tail_returns2 = returns[col2][lower_tail_mask | upper_tail_mask]

                    if len(tail_returns1) >= 10:
                        tail_corr_value = tail_returns1.corr(tail_returns2)
                        tail_corr.loc[col1, col2] = tail_corr_value
                        tail_corr.loc[col2, col1] = tail_corr_value
                    else:
                        tail_corr.loc[col1, col2] = np.nan
                        tail_corr.loc[col2, col1] = np.nan

            return tail_corr

        except Exception as e:
            self.logger.error(f"Tail correlation error: {e}")
            return pd.DataFrame()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v10.0ì˜ ê¸°ì¡´ í´ë˜ìŠ¤ë“¤ ê³„ì† (OnChainDataManager, MacroDataManager ë“±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# (ë¬¸ì„œì—ì„œ ì œê³µëœ v10.0 ì½”ë“œë¥¼ ì—¬ê¸°ì— í¬í•¨ - ê¸¸ì´ ì œí•œìœ¼ë¡œ ìƒëµ í‘œì‹œ)

# OnChainDataManager í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# MacroDataManager í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# LiquidityRegimeDetector í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# MarketMicrostructureAnalyzer í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# VolatilityTermStructureAnalyzer í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# AnomalyDetectionSystem í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# ... ë“± ëª¨ë“  v10.0 í´ë˜ìŠ¤ë“¤

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 1/5
# ë‹¤ìŒ: Part 2 - Multi-Asset Correlation Analyzer ë³¸ì²´
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 11.0 - PART 1/5 ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 1: v10.0 ì „ì²´ ê¸°ëŠ¥ + ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ê¸°ë°˜ í´ë˜ìŠ¤
#
# v11.0 NEW FEATURES (v10.0ì˜ ëª¨ë“  ê¸°ëŠ¥ 100% ìœ ì§€):
# - ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ë¶„ì„ (Multi-Asset Correlation)
# - Cross-Asset Regime Detection
# - Contagion Detection (ì‹œì¥ ì „ì—¼ íš¨ê³¼)
# - Portfolio Diversification Metrics
# - Lead-Lag Analysis (ì„ í–‰/í›„í–‰ ê´€ê³„)
# - Copula-based Tail Dependency
# - Dynamic Correlation Networks
# - Risk Parity Analysis
# - Granger Causality Testing
# - Information Flow Analysis
#
# ë³‘í•© ë°©ë²•:
# 1. ëª¨ë“  íŒŒíŠ¸(1~5)ë¥¼ ë‹¤ìš´ë¡œë“œ
# 2. Part 1ì˜ ë‚´ìš©ì„ market_regime_analyzer11.pyë¡œ ë³µì‚¬
# 3. Part 2~5ì˜ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì´ì–´ë¶™ì´ê¸°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from collections import deque, defaultdict
from typing import Dict, List, Tuple, Optional, Any
import logging
from scipy import stats
from scipy.stats import entropy, norm, t as student_t, spearmanr, kendalltau
from scipy.special import softmax
from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform
import warnings

warnings.filterwarnings('ignore')


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v10.0ì˜ ëª¨ë“  ê¸°ì¡´ í´ë˜ìŠ¤ë“¤ (100% ìœ ì§€)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_logger(name: str) -> logging.Logger:
    """í”„ë¡œë•ì…˜ ë ˆë²¨ ë¡œê±° ìƒì„±"""
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


class ProductionConfig:
    """í”„ë¡œë•ì…˜ ì„¤ì • í´ë˜ìŠ¤ (v10.0 ìœ ì§€)"""
    CACHE_TTL_SHORT = 30
    CACHE_TTL_MEDIUM = 180
    CACHE_TTL_LONG = 300
    API_TIMEOUT = 10
    API_RETRY_COUNT = 3
    API_RETRY_DELAY = 1
    MIN_DATA_POINTS = 20
    MAX_DATA_AGE_SECONDS = 3600
    OUTLIER_THRESHOLD = 5.0
    MIN_REGIME_DURATION_SECONDS = 300
    REGIME_TRANSITION_THRESHOLD = 0.15
    HYSTERESIS_FACTOR = 1.2
    WEIGHT_ADAPTATION_RATE = 0.05
    WEIGHT_MIN = 0.01
    WEIGHT_MAX = 0.50
    PERFORMANCE_LOOKBACK = 20
    ALERT_COOLDOWN_SECONDS = 300
    MAX_ALERTS_PER_HOUR = 20
    CRITICAL_ALERT_THRESHOLD = 0.90
    PERFORMANCE_LOG_INTERVAL = 60
    LATENCY_WARNING_MS = 100
    LATENCY_CRITICAL_MS = 500


class DataValidator:
    """ë°ì´í„° ê²€ì¦ í´ë˜ìŠ¤ (v10.0 ìœ ì§€)"""

    def __init__(self):
        self.logger = get_logger("DataValidator")

    def validate_numeric(self, value: float, name: str,
                         min_val: Optional[float] = None,
                         max_val: Optional[float] = None) -> bool:
        try:
            if not isinstance(value, (int, float)):
                self.logger.warning(f"{name}: Not a number - {value}")
                return False
            if np.isnan(value) or np.isinf(value):
                self.logger.warning(f"{name}: NaN or Inf detected")
                return False
            if min_val is not None and value < min_val:
                self.logger.warning(f"{name}: Below minimum ({value} < {min_val})")
                return False
            if max_val is not None and value > max_val:
                self.logger.warning(f"{name}: Above maximum ({value} > {max_val})")
                return False
            return True
        except Exception as e:
            self.logger.error(f"Validation error for {name}: {e}")
            return False

    def validate_dataframe(self, df: pd.DataFrame,
                           required_columns: List[str],
                           min_rows: int = 1) -> bool:
        try:
            if df is None or df.empty:
                self.logger.warning("DataFrame is None or empty")
                return False
            if len(df) < min_rows:
                self.logger.warning(f"Insufficient rows: {len(df)} < {min_rows}")
                return False
            missing_cols = set(required_columns) - set(df.columns)
            if missing_cols:
                self.logger.warning(f"Missing columns: {missing_cols}")
                return False
            nan_counts = df[required_columns].isna().sum()
            if nan_counts.any():
                self.logger.warning(f"NaN values detected: {nan_counts[nan_counts > 0].to_dict()}")
                return False
            return True
        except Exception as e:
            self.logger.error(f"DataFrame validation error: {e}")
            return False

    def detect_outliers(self, data: np.ndarray,
                        threshold: float = ProductionConfig.OUTLIER_THRESHOLD) -> np.ndarray:
        try:
            if len(data) < 3:
                return np.array([])
            median = np.median(data)
            mad = np.median(np.abs(data - median))
            if mad == 0:
                return np.array([])
            modified_z_scores = 0.6745 * (data - median) / mad
            outliers = np.where(np.abs(modified_z_scores) > threshold)[0]
            return outliers
        except Exception as e:
            self.logger.error(f"Outlier detection error: {e}")
            return np.array([])


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤ (v10.0 ìœ ì§€)"""

    def __init__(self):
        self.logger = get_logger("PerformanceMonitor")
        self.latencies = deque(maxlen=100)
        self.call_counts = defaultdict(int)
        self.error_counts = defaultdict(int)
        self.last_log_time = datetime.now()

    def record_latency(self, operation: str, latency_ms: float):
        self.latencies.append({
            'operation': operation,
            'latency_ms': latency_ms,
            'timestamp': datetime.now()
        })
        self.call_counts[operation] += 1
        if latency_ms > ProductionConfig.LATENCY_CRITICAL_MS:
            self.logger.warning(f"CRITICAL LATENCY: {operation} took {latency_ms:.2f}ms")
        elif latency_ms > ProductionConfig.LATENCY_WARNING_MS:
            self.logger.info(f"High latency: {operation} took {latency_ms:.2f}ms")

    def record_error(self, operation: str, error: Exception):
        self.error_counts[operation] += 1
        self.logger.error(f"Error in {operation}: {str(error)}")

    def get_stats(self) -> Dict[str, Any]:
        if not self.latencies:
            return {}
        latencies_by_op = defaultdict(list)
        for record in self.latencies:
            latencies_by_op[record['operation']].append(record['latency_ms'])
        stats = {}
        for op, lats in latencies_by_op.items():
            stats[op] = {
                'count': len(lats),
                'mean_ms': np.mean(lats),
                'median_ms': np.median(lats),
                'p95_ms': np.percentile(lats, 95),
                'p99_ms': np.percentile(lats, 99),
                'max_ms': np.max(lats)
            }
        return stats

    def log_periodic_stats(self):
        now = datetime.now()
        if (now - self.last_log_time).total_seconds() >= ProductionConfig.PERFORMANCE_LOG_INTERVAL:
            stats = self.get_stats()
            if stats:
                self.logger.info(f"Performance Stats: {stats}")
            self.last_log_time = now


performance_monitor = PerformanceMonitor()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v11.0 NEW: ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ê¸°ë°˜ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AssetDataManager:
    """
    ğŸŒ ë‹¤ì¤‘ ìì‚° ë°ì´í„° ê´€ë¦¬ì (v11.0 NEW)

    ì—¬ëŸ¬ ìì‚°ì˜ ê°€ê²© ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("AssetDataManager")
        self.validator = DataValidator()

        # ì¶”ì í•  ìì‚° ëª©ë¡
        self.crypto_assets = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'XRPUSDT', 'ADAUSDT',
            'SOLUSDT', 'DOGEUSDT', 'MATICUSDT', 'DOTUSDT', 'AVAXUSDT'
        ]

        self.traditional_assets = {
            'SPX': 'S&P 500',
            'DXY': 'US Dollar Index',
            'GOLD': 'Gold',
            'US10Y': 'US 10Y Treasury',
            'VIX': 'Volatility Index'
        }

        # ë°ì´í„° ìºì‹œ
        self._price_cache = {}
        self._returns_cache = {}
        self._cache_ttl = ProductionConfig.CACHE_TTL_SHORT

        # íˆìŠ¤í† ë¦¬
        self.price_history = defaultdict(lambda: deque(maxlen=1000))
        self.returns_history = defaultdict(lambda: deque(maxlen=1000))

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.api_call_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

    def get_asset_prices(self, symbols: List[str],
                         timeframe: str = '1h',
                         lookback: int = 100) -> pd.DataFrame:
        """
        ì—¬ëŸ¬ ìì‚°ì˜ ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        Returns:
            DataFrame with datetime index and columns for each symbol
        """
        start_time = datetime.now()

        try:
            cache_key = f"prices_{'-'.join(symbols)}_{timeframe}_{lookback}"

            # ìºì‹œ í™•ì¸
            if cache_key in self._price_cache:
                data, timestamp = self._price_cache[cache_key]
                if (datetime.now() - timestamp).total_seconds() < self._cache_ttl:
                    self.cache_hit_count += 1
                    return data

            self.api_call_count += 1

            # ê° ìì‚°ì˜ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
            all_prices = {}

            for symbol in symbols:
                try:
                    if symbol in self.crypto_assets:
                        # ì•”í˜¸í™”í ë°ì´í„°
                        df = self.market_data.get_candle_data(symbol, timeframe)
                        if df is not None and not df.empty:
                            prices = df['close'].tail(lookback)
                            all_prices[symbol] = prices
                    elif symbol in self.traditional_assets:
                        # ì „í†µ ìì‚° ë°ì´í„° (ì‹œë®¬ë ˆì´ì…˜)
                        # TODO: ì‹¤ì œ API ì—°ë™
                        prices = self._simulate_traditional_asset_prices(symbol, lookback)
                        all_prices[symbol] = prices

                except Exception as e:
                    self.logger.warning(f"Failed to get prices for {symbol}: {e}")
                    continue

            if not all_prices:
                raise ValueError("No price data collected")

            # DataFrameìœ¼ë¡œ ë³€í™˜ (ì‹œê°„ ì¸ë±ìŠ¤ ì •ë ¬)
            df = pd.DataFrame(all_prices)

            # NaN ì²˜ë¦¬ (forward fill)
            df = df.fillna(method='ffill').fillna(method='bfill')

            # ê²€ì¦
            if not self.validator.validate_dataframe(df, list(df.columns), min_rows=10):
                raise ValueError("Invalid price dataframe")

            # ìºì‹œ ì €ì¥
            self._price_cache[cache_key] = (df, datetime.now())

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('get_asset_prices', latency)

            return df

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Asset prices collection error: {e}")
            performance_monitor.record_error('get_asset_prices', e)
            return pd.DataFrame()

    def calculate_returns(self, prices: pd.DataFrame,
                          method: str = 'simple') -> pd.DataFrame:
        """
        ìˆ˜ìµë¥  ê³„ì‚°

        Args:
            prices: ê°€ê²© DataFrame
            method: 'simple', 'log'
        """
        try:
            if prices.empty:
                return pd.DataFrame()

            if method == 'log':
                returns = np.log(prices / prices.shift(1))
            else:  # simple
                returns = prices.pct_change()

            # ì²« í–‰ ì œê±° (NaN)
            returns = returns.iloc[1:]

            # ì´ìƒì¹˜ ì œê±°
            for col in returns.columns:
                outliers = self.validator.detect_outliers(returns[col].values)
                if len(outliers) > 0:
                    returns.loc[returns.index[outliers], col] = np.nan

            # NaN ì²˜ë¦¬
            returns = returns.fillna(0)

            return returns

        except Exception as e:
            self.logger.error(f"Returns calculation error: {e}")
            return pd.DataFrame()

    def _simulate_traditional_asset_prices(self, symbol: str, lookback: int) -> pd.Series:
        """ì „í†µ ìì‚° ê°€ê²© ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ API ì—°ë™ ì „ ì„ì‹œ)"""
        try:
            # ìì‚°ë³„ ê¸°ë³¸ ê°€ê²© ë° ë³€ë™ì„±
            base_prices = {
                'SPX': 4500,
                'DXY': 104,
                'GOLD': 2000,
                'US10Y': 4.5,
                'VIX': 15
            }

            volatilities = {
                'SPX': 0.15,
                'DXY': 0.05,
                'GOLD': 0.12,
                'US10Y': 0.20,
                'VIX': 0.40
            }

            base = base_prices.get(symbol, 100)
            vol = volatilities.get(symbol, 0.20)

            # Geometric Brownian Motion
            returns = np.random.normal(0.0001, vol / np.sqrt(252), lookback)
            prices = base * np.exp(np.cumsum(returns))

            # ì‹œê°„ ì¸ë±ìŠ¤ ìƒì„±
            end_date = datetime.now()
            dates = pd.date_range(end=end_date, periods=lookback, freq='1H')

            return pd.Series(prices, index=dates)

        except Exception as e:
            self.logger.error(f"Price simulation error: {e}")
            return pd.Series()

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        cache_hit_rate = (
                self.cache_hit_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        error_rate = (
                self.error_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        return {
            'api_calls': self.api_call_count,
            'cache_hits': self.cache_hit_count,
            'cache_hit_rate': cache_hit_rate,
            'errors': self.error_count,
            'error_rate': error_rate
        }


class CorrelationCalculator:
    """
    ğŸ“Š ìƒê´€ê´€ê³„ ê³„ì‚° ì—”ì§„ (v11.0 NEW)

    ë‹¤ì–‘í•œ ìƒê´€ê´€ê³„ ì¸¡ì • ë°©ë²• ì œê³µ
    """

    def __init__(self):
        self.logger = get_logger("CorrelationCalculator")
        self.validator = DataValidator()

    def calculate_pearson_correlation(self, returns: pd.DataFrame,
                                      window: Optional[int] = None) -> pd.DataFrame:
        """
        í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°

        Args:
            returns: ìˆ˜ìµë¥  DataFrame
            window: Rolling window (Noneì´ë©´ ì „ì²´ ê¸°ê°„)
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            if window is None:
                # ì „ì²´ ê¸°ê°„ ìƒê´€ê³„ìˆ˜
                corr_matrix = returns.corr(method='pearson')
            else:
                # Rolling ìƒê´€ê³„ìˆ˜ (ë§ˆì§€ë§‰ window ê¸°ê°„)
                corr_matrix = returns.tail(window).corr(method='pearson')

            # ëŒ€ê°ì„ ì€ 1ë¡œ (ìê¸° ìì‹ ê³¼ì˜ ìƒê´€ê³„ìˆ˜)
            np.fill_diagonal(corr_matrix.values, 1.0)

            return corr_matrix

        except Exception as e:
            self.logger.error(f"Pearson correlation error: {e}")
            return pd.DataFrame()

    def calculate_spearman_correlation(self, returns: pd.DataFrame,
                                       window: Optional[int] = None) -> pd.DataFrame:
        """
        ìŠ¤í”¼ì–´ë§Œ ìˆœìœ„ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ë¹„ì„ í˜• ê´€ê³„ í¬ì°©)
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            if window is not None:
                returns = returns.tail(window)

            corr_matrix = returns.corr(method='spearman')
            np.fill_diagonal(corr_matrix.values, 1.0)

            return corr_matrix

        except Exception as e:
            self.logger.error(f"Spearman correlation error: {e}")
            return pd.DataFrame()

    def calculate_kendall_correlation(self, returns: pd.DataFrame,
                                      window: Optional[int] = None) -> pd.DataFrame:
        """
        ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            if window is not None:
                returns = returns.tail(window)

            corr_matrix = returns.corr(method='kendall')
            np.fill_diagonal(corr_matrix.values, 1.0)

            return corr_matrix

        except Exception as e:
            self.logger.error(f"Kendall correlation error: {e}")
            return pd.DataFrame()

    def calculate_rolling_correlation(self, returns: pd.DataFrame,
                                      window: int = 30,
                                      min_periods: int = 20) -> Dict[str, pd.DataFrame]:
        """
        Rolling ìƒê´€ê³„ìˆ˜ ì‹œê³„ì—´ ê³„ì‚°

        Returns:
            Dict with asset pairs as keys and correlation time series as values
        """
        try:
            if returns.empty or len(returns) < min_periods:
                return {}

            rolling_corrs = {}
            columns = returns.columns

            for i, col1 in enumerate(columns):
                for col2 in columns[i + 1:]:
                    pair_key = f"{col1}_{col2}"

                    # Rolling correlation
                    rolling_corr = returns[col1].rolling(
                        window=window,
                        min_periods=min_periods
                    ).corr(returns[col2])

                    rolling_corrs[pair_key] = rolling_corr.dropna()

            return rolling_corrs

        except Exception as e:
            self.logger.error(f"Rolling correlation error: {e}")
            return {}

    def calculate_ewm_correlation(self, returns: pd.DataFrame,
                                  span: int = 30) -> pd.DataFrame:
        """
        ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· (EWMA) ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            # EWMA ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°
            ewm_cov = returns.ewm(span=span).cov()

            # ìµœê·¼ ì‹œì ì˜ ê³µë¶„ì‚° í–‰ë ¬ ì¶”ì¶œ
            latest_cov = ewm_cov.iloc[-len(returns.columns):]

            # í‘œì¤€í¸ì°¨ ê³„ì‚°
            ewm_std = returns.ewm(span=span).std().iloc[-1]

            # ìƒê´€ê³„ìˆ˜ = ê³µë¶„ì‚° / (std1 * std2)
            corr_matrix = pd.DataFrame(
                index=returns.columns,
                columns=returns.columns,
                dtype=float
            )

            for i, col1 in enumerate(returns.columns):
                for j, col2 in enumerate(returns.columns):
                    if i == j:
                        corr_matrix.loc[col1, col2] = 1.0
                    else:
                        cov = latest_cov.loc[col1, col2]
                        std1 = ewm_std[col1]
                        std2 = ewm_std[col2]
                        if std1 > 0 and std2 > 0:
                            corr_matrix.loc[col1, col2] = cov / (std1 * std2)
                        else:
                            corr_matrix.loc[col1, col2] = 0.0

            return corr_matrix.astype(float)

        except Exception as e:
            self.logger.error(f"EWM correlation error: {e}")
            return pd.DataFrame()

    def calculate_dynamic_conditional_correlation(self, returns: pd.DataFrame,
                                                  window: int = 30) -> pd.DataFrame:
        """
        ë™ì  ì¡°ê±´ë¶€ ìƒê´€ê³„ìˆ˜ (DCC) ê³„ì‚°
        GARCH ê¸°ë°˜ ì‹œë³€ ìƒê´€ê³„ìˆ˜
        """
        try:
            if returns.empty or len(returns) < window:
                return pd.DataFrame()

            # ê°„ë‹¨í•œ DCC ê·¼ì‚¬ (ì‹¤ì œë¡œëŠ” GARCH ëª¨ë¸ í•„ìš”)
            # 1. Rolling variance
            rolling_var = returns.rolling(window=window).var()

            # 2. Standardized residuals
            std_returns = returns / np.sqrt(rolling_var)
            std_returns = std_returns.dropna()

            # 3. Correlation of standardized residuals
            dcc_corr = std_returns.tail(window).corr(method='pearson')

            return dcc_corr

        except Exception as e:
            self.logger.error(f"DCC correlation error: {e}")
            return pd.DataFrame()

    def calculate_tail_correlation(self, returns: pd.DataFrame,
                                   quantile: float = 0.05) -> pd.DataFrame:
        """
        ê¼¬ë¦¬ ìƒê´€ê´€ê³„ ê³„ì‚° (ê·¹ë‹¨ì  ì›€ì§ì„ ì‹œ ìƒê´€ê´€ê³„)

        Args:
            quantile: í•˜ìœ„/ìƒìœ„ ë¶„ìœ„ìˆ˜ (0.05 = 5% ê·¹ë‹¨ê°’)
        """
        try:
            if returns.empty:
                return pd.DataFrame()

            n_assets = len(returns.columns)
            tail_corr = pd.DataFrame(
                np.eye(n_assets),
                index=returns.columns,
                columns=returns.columns
            )

            for i, col1 in enumerate(returns.columns):
                for j, col2 in enumerate(returns.columns[i + 1:], i + 1):
                    # í•˜ìœ„ ê¼¬ë¦¬
                    lower_threshold1 = returns[col1].quantile(quantile)
                    lower_threshold2 = returns[col2].quantile(quantile)

                    lower_tail_mask = (
                            (returns[col1] <= lower_threshold1) &
                            (returns[col2] <= lower_threshold2)
                    )

                    # ìƒìœ„ ê¼¬ë¦¬
                    upper_threshold1 = returns[col1].quantile(1 - quantile)
                    upper_threshold2 = returns[col2].quantile(1 - quantile)

                    upper_tail_mask = (
                            (returns[col1] >= upper_threshold1) &
                            (returns[col2] >= upper_threshold2)
                    )

                    # ê¼¬ë¦¬ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                    tail_returns1 = returns[col1][lower_tail_mask | upper_tail_mask]
                    tail_returns2 = returns[col2][lower_tail_mask | upper_tail_mask]

                    if len(tail_returns1) >= 10:
                        tail_corr_value = tail_returns1.corr(tail_returns2)
                        tail_corr.loc[col1, col2] = tail_corr_value
                        tail_corr.loc[col2, col1] = tail_corr_value
                    else:
                        tail_corr.loc[col1, col2] = np.nan
                        tail_corr.loc[col2, col1] = np.nan

            return tail_corr

        except Exception as e:
            self.logger.error(f"Tail correlation error: {e}")
            return pd.DataFrame()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v10.0ì˜ ê¸°ì¡´ í´ë˜ìŠ¤ë“¤ ê³„ì† (OnChainDataManager, MacroDataManager ë“±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# (ë¬¸ì„œì—ì„œ ì œê³µëœ v10.0 ì½”ë“œë¥¼ ì—¬ê¸°ì— í¬í•¨ - ê¸¸ì´ ì œí•œìœ¼ë¡œ ìƒëµ í‘œì‹œ)

# OnChainDataManager í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# MacroDataManager í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# LiquidityRegimeDetector í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# MarketMicrostructureAnalyzer í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# VolatilityTermStructureAnalyzer í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# AnomalyDetectionSystem í´ë˜ìŠ¤ (v10.0 ê·¸ëŒ€ë¡œ ìœ ì§€)
# ... ë“± ëª¨ë“  v10.0 í´ë˜ìŠ¤ë“¤

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 1/5
# ë‹¤ìŒ: Part 2 - Multi-Asset Correlation Analyzer ë³¸ì²´
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 11.0 - PART 3/5 ğŸ”¥ğŸ”¥ğŸ”¥
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Part 3: Contagion Detection & Portfolio Diversification Analysis
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Part 2ì—ì„œ ê³„ì†...

    def detect_market_contagion(self, symbols: List[str],
                                timeframe: str = '1h',
                                crisis_window: int = 20,
                                normal_window: int = 100) -> Dict[str, Any]:
        """
        ì‹œì¥ ì „ì—¼ íš¨ê³¼ ê°ì§€ (í”„ë¡œë•ì…˜ ë ˆë²¨)

        ìœ„ê¸° ì‹œ ìƒê´€ê´€ê³„ ê¸‰ì¦ì„ ê°ì§€í•˜ì—¬ ì‹œì¥ ì „ì—¼ íš¨ê³¼ ë¶„ì„

        Args:
            symbols: ë¶„ì„í•  ìì‚° ì‹¬ë³¼
            crisis_window: ìœ„ê¸° ê¸°ê°„ ìœˆë„ìš°
            normal_window: ì •ìƒ ê¸°ê°„ ìœˆë„ìš°
        """
        start_time = datetime.now()

        try:
            # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ (ì¶©ë¶„í•œ ë£©ë°±)
            lookback = normal_window + crisis_window + 50
            prices = self.asset_data_manager.get_asset_prices(symbols, timeframe, lookback)

            if prices.empty or len(prices) < lookback:
                raise ValueError("Insufficient price data")

            # ìˆ˜ìµë¥  ê³„ì‚°
            returns = self.asset_data_manager.calculate_returns(prices, method='log')

            # ì •ìƒ ê¸°ê°„ ìƒê´€ê´€ê³„ (ê³¼ê±° ë°ì´í„°)
            normal_returns = returns.iloc[-(normal_window + crisis_window):-crisis_window]
            normal_corr = self.corr_calculator.calculate_pearson_correlation(normal_returns)

            # ìœ„ê¸° ê¸°ê°„ ìƒê´€ê´€ê³„ (ìµœê·¼ ë°ì´í„°)
            crisis_returns = returns.iloc[-crisis_window:]
            crisis_corr = self.corr_calculator.calculate_pearson_correlation(crisis_returns)

            if normal_corr.empty or crisis_corr.empty:
                raise ValueError("Failed to calculate correlations")

            # ìƒê´€ê´€ê³„ ë³€í™” ê³„ì‚°
            corr_change = crisis_corr - normal_corr

            # ì „ì—¼ íš¨ê³¼ ì¸¡ì •
            contagion_scores = {}
            contagion_pairs = []

            for i in range(len(symbols)):
                for j in range(i + 1, len(symbols)):
                    asset1 = symbols[i]
                    asset2 = symbols[j]

                    normal_c = normal_corr.loc[asset1, asset2]
                    crisis_c = crisis_corr.loc[asset1, asset2]
                    change = corr_change.loc[asset1, asset2]

                    # ì „ì—¼ íš¨ê³¼ ì ìˆ˜ (ìœ„ê¸° ì‹œ ìƒê´€ê´€ê³„ ì¦ê°€)
                    if change > 0.2 and crisis_c > 0.6:
                        contagion_score = change * crisis_c
                        contagion_pairs.append({
                            'asset1': asset1,
                            'asset2': asset2,
                            'normal_correlation': float(normal_c),
                            'crisis_correlation': float(crisis_c),
                            'correlation_change': float(change),
                            'contagion_score': float(contagion_score)
                        })

            # ì „ì—¼ ìŒì„ ì ìˆ˜ë¡œ ì •ë ¬
            contagion_pairs.sort(key=lambda x: x['contagion_score'], reverse=True)

            # ì „ì²´ ì‹œì¥ ì „ì—¼ ë ˆë²¨
            avg_normal_corr = self._calculate_avg_correlation(normal_corr)
            avg_crisis_corr = self._calculate_avg_correlation(crisis_corr)
            avg_change = avg_crisis_corr - avg_normal_corr

            # ì „ì—¼ ë ˆë²¨ ë¶„ë¥˜
            if avg_change > 0.3 and avg_crisis_corr > self.contagion_thresholds['severe']:
                contagion_level = 'SEVERE'
                contagion_strength = 0.9
            elif avg_change > 0.2 and avg_crisis_corr > self.contagion_thresholds['moderate']:
                contagion_level = 'MODERATE'
                contagion_strength = 0.7
            elif avg_change > 0.1:
                contagion_level = 'MILD'
                contagion_strength = 0.5
            else:
                contagion_level = 'NONE'
                contagion_strength = 0.2

            # Tail dependency ë¶„ì„
            tail_corr = self.corr_calculator.calculate_tail_correlation(crisis_returns, quantile=0.05)
            avg_tail_corr = self._calculate_avg_correlation(tail_corr) if not tail_corr.empty else 0.0

            result = {
                'contagion_level': contagion_level,
                'contagion_strength': float(contagion_strength),
                'avg_normal_correlation': float(avg_normal_corr),
                'avg_crisis_correlation': float(avg_crisis_corr),
                'correlation_change': float(avg_change),
                'avg_tail_correlation': float(avg_tail_corr),
                'contagion_pairs': contagion_pairs[:10],  # ìƒìœ„ 10ê°œ
                'n_contagion_pairs': len(contagion_pairs),
                'symbols': symbols,
                'crisis_window': crisis_window,
                'normal_window': normal_window,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.contagion_history.append(result)

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('contagion_detection', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Contagion detection error: {e}")
            performance_monitor.record_error('contagion_detection', e)

            return {
                'contagion_level': 'UNKNOWN',
                'contagion_strength': 0.5,
                'avg_normal_correlation': 0.0,
                'avg_crisis_correlation': 0.0,
                'correlation_change': 0.0,
                'avg_tail_correlation': 0.0,
                'contagion_pairs': [],
                'n_contagion_pairs': 0,
                'symbols': symbols,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def analyze_portfolio_diversification(self, symbols: List[str],
                                          weights: Optional[List[float]] = None,
                                          timeframe: str = '1h',
                                          lookback: int = 100) -> Dict[str, Any]:
        """
        í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™” ë¶„ì„ (í”„ë¡œë•ì…˜ ë ˆë²¨)

        í¬íŠ¸í´ë¦¬ì˜¤ì˜ ë‹¤ê°í™” ìˆ˜ì¤€ì„ ë‹¤ì–‘í•œ ì§€í‘œë¡œ í‰ê°€

        Args:
            symbols: í¬íŠ¸í´ë¦¬ì˜¤ ìì‚° ì‹¬ë³¼
            weights: ìì‚°ë³„ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ë™ì¼ ê°€ì¤‘)
        """
        start_time = datetime.now()

        try:
            # ê°€ì¤‘ì¹˜ ê¸°ë³¸ê°’
            if weights is None:
                weights = [1.0 / len(symbols)] * len(symbols)
            else:
                if len(weights) != len(symbols):
                    raise ValueError("Weights length must match symbols length")
                # ì •ê·œí™”
                total_weight = sum(weights)
                weights = [w / total_weight for w in weights]

            # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
            prices = self.asset_data_manager.get_asset_prices(symbols, timeframe, lookback)

            if prices.empty:
                raise ValueError("No price data available")

            # ìˆ˜ìµë¥  ë° ìƒê´€ê´€ê³„ ê³„ì‚°
            returns = self.asset_data_manager.calculate_returns(prices, method='log')
            corr_matrix = self.corr_calculator.calculate_pearson_correlation(returns)

            if corr_matrix.empty:
                raise ValueError("Failed to calculate correlation matrix")

            # 1. Diversification Ratio
            # DR = (ê°€ì¤‘ í‰ê·  ê°œë³„ ë³€ë™ì„±) / (í¬íŠ¸í´ë¦¬ì˜¤ ë³€ë™ì„±)
            individual_stds = returns.std()
            weighted_avg_std = sum(w * individual_stds[s] for w, s in zip(weights, symbols))

            # í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚° ê³„ì‚°
            portfolio_var = 0.0
            for i, (w1, s1) in enumerate(zip(weights, symbols)):
                for j, (w2, s2) in enumerate(zip(weights, symbols)):
                    cov = returns[s1].cov(returns[s2])
                    portfolio_var += w1 * w2 * cov

            portfolio_std = np.sqrt(portfolio_var)

            if portfolio_std > 0:
                diversification_ratio = weighted_avg_std / portfolio_std
            else:
                diversification_ratio = 1.0

            # 2. Effective Number of Assets
            # ENB = 1 / sum(weight_i^2)
            effective_n_assets = 1.0 / sum(w ** 2 for w in weights)

            # 3. Average Correlation
            avg_correlation = self._calculate_avg_correlation(corr_matrix)

            # 4. Maximum Diversification Portfolio ë¹„êµ
            # ì´ìƒì ì¸ ë‹¤ê°í™”ë¥¼ ìœ„í•œ ìµœì  ê°€ì¤‘ì¹˜ì™€ ë¹„êµ
            # ê°„ë‹¨í•œ ê·¼ì‚¬: 1/N í¬íŠ¸í´ë¦¬ì˜¤ì™€ ë¹„êµ
            equal_weights = [1.0 / len(symbols)] * len(symbols)
            equal_portfolio_var = 0.0
            for i, s1 in enumerate(symbols):
                for j, s2 in enumerate(symbols):
                    cov = returns[s1].cov(returns[s2])
                    equal_portfolio_var += equal_weights[i] * equal_weights[j] * cov

            equal_portfolio_std = np.sqrt(equal_portfolio_var)

            # í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ê°€ ë™ì¼ ê°€ì¤‘ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë‚˜ì€ì§€
            if equal_portfolio_std > 0:
                diversification_improvement = (equal_portfolio_std - portfolio_std) / equal_portfolio_std
            else:
                diversification_improvement = 0.0

            # 5. Concentration Risk
            # HHI (Herfindahl-Hirschman Index)
            hhi = sum(w ** 2 for w in weights)
            concentration_risk = hhi  # 0.1 (ë¶„ì‚°) ~ 1.0 (ì§‘ì¤‘)

            # ë‹¤ê°í™” í’ˆì§ˆ í‰ê°€
            if diversification_ratio > 1.5 and avg_correlation < 0.3:
                diversification_quality = 'EXCELLENT'
                quality_score = 0.9
            elif diversification_ratio > 1.3 and avg_correlation < 0.5:
                diversification_quality = 'GOOD'
                quality_score = 0.7
            elif diversification_ratio > 1.1:
                diversification_quality = 'MODERATE'
                quality_score = 0.5
            else:
                diversification_quality = 'POOR'
                quality_score = 0.3

            # ë¦¬ìŠ¤í¬ ë¶„í•´
            # ê° ìì‚°ì˜ marginal contribution to risk
            marginal_risks = {}
            for i, symbol in enumerate(symbols):
                marginal_var = 0.0
                for j, s2 in enumerate(symbols):
                    cov = returns[symbol].cov(returns[s2])
                    marginal_var += weights[j] * cov

                marginal_risk = weights[i] * marginal_var / portfolio_var if portfolio_var > 0 else 0
                marginal_risks[symbol] = float(marginal_risk)

            result = {
                'diversification_ratio': float(diversification_ratio),
                'effective_n_assets': float(effective_n_assets),
                'avg_correlation': float(avg_correlation),
                'portfolio_volatility': float(portfolio_std),
                'weighted_avg_volatility': float(weighted_avg_std),
                'diversification_improvement': float(diversification_improvement),
                'concentration_risk': float(concentration_risk),
                'diversification_quality': diversification_quality,
                'quality_score': float(quality_score),
                'marginal_risks': marginal_risks,
                'symbols': symbols,
                'weights': {s: float(w) for s, w in zip(symbols, weights)},
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.diversification_history.append(result)

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('portfolio_diversification_analysis', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Portfolio diversification analysis error: {e}")
            performance_monitor.record_error('portfolio_diversification_analysis', e)

            return {
                'diversification_ratio': 1.0,
                'effective_n_assets': 1.0,
                'avg_correlation': 0.5,
                'portfolio_volatility': 0.0,
                'weighted_avg_volatility': 0.0,
                'diversification_improvement': 0.0,
                'concentration_risk': 1.0,
                'diversification_quality': 'UNKNOWN',
                'quality_score': 0.5,
                'marginal_risks': {},
                'symbols': symbols,
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def calculate_risk_parity_weights(self, symbols: List[str],
                                      timeframe: str = '1h',
                                      lookback: int = 100) -> Dict[str, Any]:
        """
        ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹° í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (í”„ë¡œë•ì…˜ ë ˆë²¨)

        ê° ìì‚°ì´ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬ì— ë™ì¼í•˜ê²Œ ê¸°ì—¬í•˜ë„ë¡ ê°€ì¤‘ì¹˜ ì„¤ì •
        """
        start_time = datetime.now()

        try:
            # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
            prices = self.asset_data_manager.get_asset_prices(symbols, timeframe, lookback)

            if prices.empty:
                raise ValueError("No price data available")

            # ìˆ˜ìµë¥  ê³„ì‚°
            returns = self.asset_data_manager.calculate_returns(prices, method='log')

            # ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°
            cov_matrix = returns.cov()

            # Risk Parity ê°€ì¤‘ì¹˜ ê³„ì‚° (ê°„ë‹¨í•œ ì—­ë³€ë™ì„± ë°©ë²•)
            volatilities = returns.std()
            inv_vol = 1.0 / volatilities
            risk_parity_weights = inv_vol / inv_vol.sum()

            # ê²€ì¦
            weights_dict = {s: float(w) for s, w in zip(symbols, risk_parity_weights)}

            # í¬íŠ¸í´ë¦¬ì˜¤ í†µê³„
            portfolio_var = 0.0
            for i, s1 in enumerate(symbols):
                for j, s2 in enumerate(symbols):
                    portfolio_var += (risk_parity_weights[i] * risk_parity_weights[j] *
                                      cov_matrix.loc[s1, s2])

            portfolio_vol = np.sqrt(portfolio_var)

            # ê° ìì‚°ì˜ ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„
            risk_contributions = {}
            for i, symbol in enumerate(symbols):
                marginal_risk = 0.0
                for j, s2 in enumerate(symbols):
                    marginal_risk += risk_parity_weights[j] * cov_matrix.loc[symbol, s2]

                risk_contrib = risk_parity_weights[i] * marginal_risk / portfolio_var if portfolio_var > 0 else 0
                risk_contributions[symbol] = float(risk_contrib)

            # ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ ê· í˜• ì²´í¬
            risk_contrib_values = list(risk_contributions.values())
            risk_contrib_std = np.std(risk_contrib_values)

            if risk_contrib_std < 0.05:
                balance_quality = 'EXCELLENT'
            elif risk_contrib_std < 0.10:
                balance_quality = 'GOOD'
            else:
                balance_quality = 'MODERATE'

            result = {
                'risk_parity_weights': weights_dict,
                'portfolio_volatility': float(portfolio_vol),
                'risk_contributions': risk_contributions,
                'risk_contribution_std': float(risk_contrib_std),
                'balance_quality': balance_quality,
                'individual_volatilities': {s: float(v) for s, v in volatilities.items()},
                'symbols': symbols,
                'timestamp': datetime.now()
            }

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('risk_parity_calculation', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Risk parity calculation error: {e}")
            performance_monitor.record_error('risk_parity_calculation', e)

            # ë™ì¼ ê°€ì¤‘ì¹˜ í´ë°±
            equal_weights = {s: 1.0 / len(symbols) for s in symbols}

            return {
                'risk_parity_weights': equal_weights,
                'portfolio_volatility': 0.0,
                'risk_contributions': {},
                'risk_contribution_std': 0.0,
                'balance_quality': 'UNKNOWN',
                'individual_volatilities': {},
                'symbols': symbols,
                'timestamp': datetime.now(),
                'error': str(e),
                'note': 'Fallback to equal weights'
            }

    def get_comprehensive_multi_asset_report(self, crypto_symbols: List[str],
                                             traditional_symbols: Optional[List[str]] = None,
                                             timeframe: str = '1h',
                                             lookback: int = 100) -> Dict[str, Any]:
        """
        ì¢…í•© ë‹¤ì¤‘ ìì‚° ë¶„ì„ ë¦¬í¬íŠ¸ (í”„ë¡œë•ì…˜ ë ˆë²¨)

        ëª¨ë“  ë‹¤ì¤‘ ìì‚° ë¶„ì„ ê¸°ëŠ¥ì„ í†µí•©í•œ ì¢…í•© ë¦¬í¬íŠ¸
        """
        start_time = datetime.now()

        try:
            all_symbols = crypto_symbols.copy()
            if traditional_symbols:
                all_symbols.extend(traditional_symbols)

            # 1. ìƒê´€ê´€ê³„ í–‰ë ¬ ë¶„ì„
            corr_analysis = self.analyze_correlation_matrix(
                all_symbols, timeframe, lookback, method='pearson'
            )

            # 2. ìƒê´€ê´€ê³„ ë ˆì§ ë³€í™”
            regime_analysis = self.detect_correlation_regime_changes(
                all_symbols, timeframe, window=30, lookback=lookback
            )

            # 3. Cross-Asset Dynamics (ì•”í˜¸í™”í vs ì „í†µìì‚°)
            if traditional_symbols:
                cross_asset_analysis = self.analyze_cross_asset_dynamics(
                    crypto_symbols, traditional_symbols, timeframe, lookback
                )
            else:
                cross_asset_analysis = {}

            # 4. ì‹œì¥ ì „ì—¼ íš¨ê³¼
            contagion_analysis = self.detect_market_contagion(
                all_symbols, timeframe, crisis_window=20, normal_window=100
            )

            # 5. í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™”
            diversification_analysis = self.analyze_portfolio_diversification(
                all_symbols, weights=None, timeframe=timeframe, lookback=lookback
            )

            # 6. ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹°
            risk_parity_analysis = self.calculate_risk_parity_weights(
                all_symbols, timeframe, lookback
            )

            # ì¢…í•© í‰ê°€
            # ë‹¤ê°í™” í’ˆì§ˆ
            div_score = diversification_analysis.get('quality_score', 0.5)

            # ìƒê´€ê´€ê³„ ë ˆì§
            regime = corr_analysis.get('regime', 'UNKNOWN')
            regime_strength = corr_analysis.get('regime_strength', 0.5)

            # ì „ì—¼ ìœ„í—˜
            contagion_level = contagion_analysis.get('contagion_level', 'UNKNOWN')
            contagion_strength = contagion_analysis.get('contagion_strength', 0.5)

            # ì¢…í•© ë¦¬ìŠ¤í¬ ë ˆë²¨
            if contagion_level == 'SEVERE' or regime == 'CRISIS_MODE':
                overall_risk = 'HIGH'
                risk_score = 0.9
            elif contagion_level == 'MODERATE' or regime == 'HIGH_CORRELATION':
                overall_risk = 'ELEVATED'
                risk_score = 0.7
            elif div_score > 0.7 and contagion_level == 'NONE':
                overall_risk = 'LOW'
                risk_score = 0.3
            else:
                overall_risk = 'MODERATE'
                risk_score = 0.5

            # íˆ¬ì ê¶Œê³ 
            if overall_risk == 'HIGH':
                recommendation = 'REDUCE_EXPOSURE'
            elif overall_risk == 'LOW' and div_score > 0.7:
                recommendation = 'FAVORABLE_CONDITIONS'
            else:
                recommendation = 'MONITOR_CLOSELY'

            result = {
                'overall_risk_level': overall_risk,
                'risk_score': float(risk_score),
                'recommendation': recommendation,
                'correlation_analysis': corr_analysis,
                'regime_analysis': regime_analysis,
                'cross_asset_analysis': cross_asset_analysis,
                'contagion_analysis': contagion_analysis,
                'diversification_analysis': diversification_analysis,
                'risk_parity_analysis': risk_parity_analysis,
                'crypto_symbols': crypto_symbols,
                'traditional_symbols': traditional_symbols or [],
                'timestamp': datetime.now()
            }

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('comprehensive_multi_asset_report', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Comprehensive multi-asset report error: {e}")
            performance_monitor.record_error('comprehensive_multi_asset_report', e)

            return {
                'overall_risk_level': 'UNKNOWN',
                'risk_score': 0.5,
                'recommendation': 'DATA_INSUFFICIENT',
                'correlation_analysis': {},
                'regime_analysis': {},
                'cross_asset_analysis': {},
                'contagion_analysis': {},
                'diversification_analysis': {},
                'risk_parity_analysis': {},
                'crypto_symbols': crypto_symbols,
                'traditional_symbols': traditional_symbols or [],
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        cache_hit_rate = (
                self.cache_hit_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        error_rate = (
                self.error_count / max(self.api_call_count, 1)
        ) if self.api_call_count > 0 else 0

        return {
            'api_calls': self.api_call_count,
            'cache_hits': self.cache_hit_count,
            'cache_hit_rate': cache_hit_rate,
            'errors': self.error_count,
            'error_rate': error_rate,
            'history_sizes': {
                'correlation': len(self.correlation_history),
                'regime': len(self.regime_history),
                'contagion': len(self.contagion_history),
                'diversification': len(self.diversification_history),
                'lead_lag': len(self.lead_lag_history)
            }
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 3/5
# ë‹¤ìŒ: Part 4 - Lead-Lag Analysis & Granger Causality
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 11.0 - PART 4/5 ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 4: Lead-Lag Analysis & Granger Causality Testing
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LeadLagAnalyzer:
    """
    ğŸ“ˆ ì„ í–‰/í›„í–‰ ê´€ê³„ ë¶„ì„ê¸° (v11.0 NEW - í”„ë¡œë•ì…˜ ë ˆë²¨)

    ìì‚° ê°„ ì„ í–‰/í›„í–‰ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´ ë°œê²¬
    """

    def __init__(self, asset_data_manager):
        self.asset_data = asset_data_manager
        self.logger = get_logger("LeadLagAnalyzer")
        self.validator = DataValidator()

        # íˆìŠ¤í† ë¦¬
        self.lead_lag_history = deque(maxlen=200)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.api_call_count = 0
        self.error_count = 0

    def calculate_lead_lag_correlation(self, asset1: str, asset2: str,
                                       timeframe: str = '1h',
                                       lookback: int = 200,
                                       max_lag: int = 10) -> Dict[str, Any]:
        """
        ë‘ ìì‚° ê°„ ì„ í–‰/í›„í–‰ ìƒê´€ê´€ê³„ ê³„ì‚°

        Args:
            asset1: ì²« ë²ˆì§¸ ìì‚° ì‹¬ë³¼
            asset2: ë‘ ë²ˆì§¸ ìì‚° ì‹¬ë³¼
            max_lag: ìµœëŒ€ ì‹œì°¨ (periods)
        """
        start_time = datetime.now()

        try:
            self.api_call_count += 1

            # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
            symbols = [asset1, asset2]
            prices = self.asset_data.get_asset_prices(symbols, timeframe, lookback)

            if prices.empty or len(prices) < lookback:
                raise ValueError("Insufficient price data")

            # ìˆ˜ìµë¥  ê³„ì‚°
            returns = self.asset_data.calculate_returns(prices, method='log')

            if len(returns) < 50:
                raise ValueError("Insufficient return data")

            # ê° ì‹œì°¨ë³„ ìƒê´€ê´€ê³„ ê³„ì‚°
            lag_correlations = {}

            for lag in range(-max_lag, max_lag + 1):
                if lag < 0:
                    # asset2ê°€ asset1ë³´ë‹¤ -lag ê¸°ê°„ ì„ í–‰
                    shifted_asset1 = returns[asset1].iloc[-lag:]
                    shifted_asset2 = returns[asset2].iloc[:lag]
                elif lag > 0:
                    # asset1ì´ asset2ë³´ë‹¤ lag ê¸°ê°„ ì„ í–‰
                    shifted_asset1 = returns[asset1].iloc[:-lag]
                    shifted_asset2 = returns[asset2].iloc[lag:]
                else:
                    # ë™ì‹œ (lag = 0)
                    shifted_asset1 = returns[asset1]
                    shifted_asset2 = returns[asset2]

                if len(shifted_asset1) > 10 and len(shifted_asset2) > 10:
                    # ê¸¸ì´ ë§ì¶”ê¸°
                    min_len = min(len(shifted_asset1), len(shifted_asset2))
                    shifted_asset1 = shifted_asset1.iloc[:min_len]
                    shifted_asset2 = shifted_asset2.iloc[:min_len]

                    correlation = shifted_asset1.corr(shifted_asset2)
                    lag_correlations[lag] = float(correlation)
                else:
                    lag_correlations[lag] = 0.0

            # ìµœëŒ€ ìƒê´€ê´€ê³„ ì‹œì°¨ ì°¾ê¸°
            max_corr_lag = max(lag_correlations.items(), key=lambda x: abs(x[1]))
            optimal_lag = max_corr_lag[0]
            optimal_correlation = max_corr_lag[1]

            # ì„ í–‰/í›„í–‰ ê´€ê³„ íŒë‹¨
            if abs(optimal_correlation) < 0.3:
                relationship = 'WEAK_OR_NONE'
                lead_asset = None
                lag_asset = None
            elif optimal_lag < 0:
                relationship = 'LEAD_LAG'
                lead_asset = asset2
                lag_asset = asset1
                actual_lag = abs(optimal_lag)
            elif optimal_lag > 0:
                relationship = 'LEAD_LAG'
                lead_asset = asset1
                lag_asset = asset2
                actual_lag = optimal_lag
            else:
                relationship = 'SYNCHRONOUS'
                lead_asset = None
                lag_asset = None
                actual_lag = 0

            # ì‹ ë¢°ë„ ê³„ì‚° (ìƒê´€ê³„ìˆ˜ í¬ê¸° ê¸°ë°˜)
            confidence = abs(optimal_correlation)

            result = {
                'asset1': asset1,
                'asset2': asset2,
                'relationship': relationship,
                'lead_asset': lead_asset,
                'lag_asset': lag_asset,
                'optimal_lag': int(optimal_lag),
                'lag_periods': int(actual_lag) if relationship == 'LEAD_LAG' else 0,
                'optimal_correlation': float(optimal_correlation),
                'confidence': float(confidence),
                'lag_correlations': lag_correlations,
                'max_lag_tested': max_lag,
                'timestamp': datetime.now()
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.lead_lag_history.append(result)

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('lead_lag_correlation', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Lead-lag correlation error: {e}")
            performance_monitor.record_error('lead_lag_correlation', e)

            return {
                'asset1': asset1,
                'asset2': asset2,
                'relationship': 'UNKNOWN',
                'lead_asset': None,
                'lag_asset': None,
                'optimal_lag': 0,
                'lag_periods': 0,
                'optimal_correlation': 0.0,
                'confidence': 0.0,
                'lag_correlations': {},
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def analyze_market_leadership(self, symbols: List[str],
                                  timeframe: str = '1h',
                                  lookback: int = 200,
                                  max_lag: int = 5) -> Dict[str, Any]:
        """
        ì‹œì¥ ë¦¬ë”ì‹­ ë¶„ì„ (ì–´ë–¤ ìì‚°ì´ ë‹¤ë¥¸ ìì‚°ë“¤ì„ ì„ í–‰í•˜ëŠ”ì§€)
        """
        start_time = datetime.now()

        try:
            # ëª¨ë“  ìŒì— ëŒ€í•´ ì„ í–‰/í›„í–‰ ë¶„ì„
            lead_lag_results = []

            for i, asset1 in enumerate(symbols):
                for asset2 in symbols[i + 1:]:
                    result = self.calculate_lead_lag_correlation(
                        asset1, asset2, timeframe, lookback, max_lag
                    )

                    if 'error' not in result and result['relationship'] == 'LEAD_LAG':
                        lead_lag_results.append(result)

            # ê° ìì‚°ì˜ ë¦¬ë”ì‹­ ì ìˆ˜ ê³„ì‚°
            leadership_scores = defaultdict(float)
            follower_scores = defaultdict(float)

            for result in lead_lag_results:
                lead_asset = result['lead_asset']
                lag_asset = result['lag_asset']
                correlation = abs(result['optimal_correlation'])

                if lead_asset:
                    leadership_scores[lead_asset] += correlation
                if lag_asset:
                    follower_scores[lag_asset] += correlation

            # ì •ê·œí™”
            total_leadership = sum(leadership_scores.values())
            if total_leadership > 0:
                leadership_scores = {
                    k: v / total_leadership
                    for k, v in leadership_scores.items()
                }

            total_follower = sum(follower_scores.values())
            if total_follower > 0:
                follower_scores = {
                    k: v / total_follower
                    for k, v in follower_scores.items()
                }

            # ë¦¬ë” ìˆœìœ„
            leaders = sorted(
                leadership_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )

            # íŒ”ë¡œì›Œ ìˆœìœ„
            followers = sorted(
                follower_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )

            # ì£¼ìš” ë¦¬ë” ì‹ë³„
            if leaders:
                primary_leader = leaders[0][0]
                leader_strength = leaders[0][1]
            else:
                primary_leader = None
                leader_strength = 0.0

            result = {
                'primary_leader': primary_leader,
                'leader_strength': float(leader_strength),
                'leadership_ranking': [(s, float(score)) for s, score in leaders],
                'follower_ranking': [(s, float(score)) for s, score in followers],
                'lead_lag_pairs': [
                    {
                        'lead': r['lead_asset'],
                        'lag': r['lag_asset'],
                        'lag_periods': r['lag_periods'],
                        'correlation': r['optimal_correlation']
                    }
                    for r in lead_lag_results
                ],
                'n_lead_lag_relationships': len(lead_lag_results),
                'symbols': symbols,
                'timestamp': datetime.now()
            }

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('market_leadership_analysis', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Market leadership analysis error: {e}")
            performance_monitor.record_error('market_leadership_analysis', e)

            return {
                'primary_leader': None,
                'leader_strength': 0.0,
                'leadership_ranking': [],
                'follower_ranking': [],
                'lead_lag_pairs': [],
                'n_lead_lag_relationships': 0,
                'symbols': symbols,
                'timestamp': datetime.now(),
                'error': str(e)
            }


class GrangerCausalityAnalyzer:
    """
    ğŸ” Granger ì¸ê³¼ê´€ê³„ ë¶„ì„ê¸° (v11.0 NEW - í”„ë¡œë•ì…˜ ë ˆë²¨)

    ìì‚° ê°„ Granger ì¸ê³¼ê´€ê³„ í…ŒìŠ¤íŠ¸
    """

    def __init__(self, asset_data_manager):
        self.asset_data = asset_data_manager
        self.logger = get_logger("GrangerCausality")
        self.validator = DataValidator()

        # íˆìŠ¤í† ë¦¬
        self.granger_history = deque(maxlen=200)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.api_call_count = 0
        self.error_count = 0

    def test_granger_causality(self, cause_asset: str, effect_asset: str,
                               timeframe: str = '1h',
                               lookback: int = 200,
                               max_lag: int = 10) -> Dict[str, Any]:
        """
        Granger ì¸ê³¼ê´€ê³„ í…ŒìŠ¤íŠ¸ (ê°„ì†Œí™” ë²„ì „)

        Args:
            cause_asset: ì›ì¸ ìì‚°
            effect_asset: ê²°ê³¼ ìì‚°
            max_lag: ìµœëŒ€ ì‹œì°¨
        """
        start_time = datetime.now()

        try:
            self.api_call_count += 1

            # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
            symbols = [cause_asset, effect_asset]
            prices = self.asset_data.get_asset_prices(symbols, timeframe, lookback)

            if prices.empty or len(prices) < lookback:
                raise ValueError("Insufficient price data")

            # ìˆ˜ìµë¥  ê³„ì‚°
            returns = self.asset_data.calculate_returns(prices, method='log')

            if len(returns) < 50:
                raise ValueError("Insufficient return data")

            # ê°„ì†Œí™”ëœ Granger ì¸ê³¼ê´€ê³„ í…ŒìŠ¤íŠ¸
            # (ì™„ì „í•œ í…ŒìŠ¤íŠ¸ëŠ” statsmodelsì˜ grangercausalitytests ì‚¬ìš©)

            # ê° ì‹œì°¨ì— ëŒ€í•´ ì˜ˆì¸¡ë ¥ ë¹„êµ
            causality_scores = {}

            for lag in range(1, max_lag + 1):
                # Model 1: effectë§Œ ì‚¬ìš© (Restricted)
                # Model 2: effect + lagged cause ì‚¬ìš© (Unrestricted)

                effect_series = returns[effect_asset].iloc[lag:]
                effect_lagged = returns[effect_asset].iloc[:-lag]
                cause_lagged = returns[cause_asset].iloc[:-lag]

                # ê¸¸ì´ ë§ì¶”ê¸°
                min_len = min(len(effect_series), len(effect_lagged), len(cause_lagged))
                effect_series = effect_series.iloc[:min_len]
                effect_lagged = effect_lagged.iloc[:min_len]
                cause_lagged = cause_lagged.iloc[:min_len]

                # Model 1: AR(1) - effectë§Œ
                corr_restricted = effect_series.corr(effect_lagged)

                # Model 2: effect + cause
                # ê°„ë‹¨í•œ íšŒê·€ ê·¼ì‚¬
                # effect_t = alpha + beta1 * effect_t-1 + beta2 * cause_t-1

                # í‘œì¤€í™”
                effect_std = (effect_series - effect_series.mean()) / effect_series.std()
                effect_lagged_std = (effect_lagged - effect_lagged.mean()) / effect_lagged.std()
                cause_lagged_std = (cause_lagged - cause_lagged.mean()) / cause_lagged.std()

                # ê°„ë‹¨í•œ ë‹¤ì¤‘ ìƒê´€ê³„ìˆ˜ ê·¼ì‚¬
                corr_effect = effect_std.corr(effect_lagged_std)
                corr_cause = effect_std.corr(cause_lagged_std)
                corr_unrestricted = np.sqrt(corr_effect ** 2 + corr_cause ** 2)

                # F-í†µê³„ëŸ‰ ê·¼ì‚¬
                improvement = (corr_unrestricted ** 2 - corr_restricted ** 2)

                causality_scores[lag] = float(improvement)

            # ìµœì  ì‹œì°¨ ì„ íƒ
            if causality_scores:
                best_lag = max(causality_scores.items(), key=lambda x: x[1])
                optimal_lag = best_lag[0]
                causality_strength = best_lag[1]

                # ì¸ê³¼ê´€ê³„ ìœ ì˜ì„± íŒë‹¨ (ê°„ì†Œí™”)
                if causality_strength > 0.05:
                    is_causal = True
                    significance = 'SIGNIFICANT'
                elif causality_strength > 0.02:
                    is_causal = True
                    significance = 'MODERATE'
                else:
                    is_causal = False
                    significance = 'WEAK'
            else:
                optimal_lag = 0
                causality_strength = 0.0
                is_causal = False
                significance = 'NONE'

            result = {
                'cause_asset': cause_asset,
                'effect_asset': effect_asset,
                'is_causal': is_causal,
                'significance': significance,
                'optimal_lag': int(optimal_lag),
                'causality_strength': float(causality_strength),
                'causality_scores_by_lag': causality_scores,
                'max_lag_tested': max_lag,
                'timestamp': datetime.now(),
                'note': 'Simplified Granger causality approximation'
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.granger_history.append(result)

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('granger_causality_test', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Granger causality test error: {e}")
            performance_monitor.record_error('granger_causality_test', e)

            return {
                'cause_asset': cause_asset,
                'effect_asset': effect_asset,
                'is_causal': False,
                'significance': 'UNKNOWN',
                'optimal_lag': 0,
                'causality_strength': 0.0,
                'causality_scores_by_lag': {},
                'timestamp': datetime.now(),
                'error': str(e)
            }

    def analyze_causal_network(self, symbols: List[str],
                               timeframe: str = '1h',
                               lookback: int = 200,
                               max_lag: int = 5) -> Dict[str, Any]:
        """
        ìì‚° ê°„ ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        """
        start_time = datetime.now()

        try:
            # ëª¨ë“  ë°©í–¥ ìŒì— ëŒ€í•´ Granger í…ŒìŠ¤íŠ¸
            causal_relationships = []

            for cause in symbols:
                for effect in symbols:
                    if cause != effect:
                        result = self.test_granger_causality(
                            cause, effect, timeframe, lookback, max_lag
                        )

                        if 'error' not in result and result['is_causal']:
                            causal_relationships.append({
                                'cause': cause,
                                'effect': effect,
                                'lag': result['optimal_lag'],
                                'strength': result['causality_strength'],
                                'significance': result['significance']
                            })

            # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­
            # ê° ìì‚°ì˜ in-degree (ì–¼ë§ˆë‚˜ ë§ì€ ìì‚°ì´ ì´ ìì‚°ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€)
            # ê° ìì‚°ì˜ out-degree (ì´ ìì‚°ì´ ì–¼ë§ˆë‚˜ ë§ì€ ìì‚°ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€)

            in_degree = defaultdict(int)
            out_degree = defaultdict(int)
            influence_scores = defaultdict(float)

            for rel in causal_relationships:
                out_degree[rel['cause']] += 1
                in_degree[rel['effect']] += 1
                influence_scores[rel['cause']] += rel['strength']

            # ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ìì‚°
            if influence_scores:
                most_influential = max(influence_scores.items(), key=lambda x: x[1])
                influential_asset = most_influential[0]
                influence_score = most_influential[1]
            else:
                influential_asset = None
                influence_score = 0.0

            # ê°€ì¥ ì˜í–¥ ë°›ëŠ” ìì‚°
            if in_degree:
                most_influenced = max(in_degree.items(), key=lambda x: x[1])
                influenced_asset = most_influenced[0]
                influence_count = most_influenced[1]
            else:
                influenced_asset = None
                influence_count = 0

            result = {
                'most_influential_asset': influential_asset,
                'influence_score': float(influence_score),
                'most_influenced_asset': influenced_asset,
                'influence_count': int(influence_count),
                'causal_relationships': causal_relationships,
                'n_causal_relationships': len(causal_relationships),
                'in_degree': dict(in_degree),
                'out_degree': dict(out_degree),
                'influence_scores': {k: float(v) for k, v in influence_scores.items()},
                'symbols': symbols,
                'timestamp': datetime.now()
            }

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('causal_network_analysis', latency)

            return result

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Causal network analysis error: {e}")
            performance_monitor.record_error('causal_network_analysis', e)

            return {
                'most_influential_asset': None,
                'influence_score': 0.0,
                'most_influenced_asset': None,
                'influence_count': 0,
                'causal_relationships': [],
                'n_causal_relationships': 0,
                'in_degree': {},
                'out_degree': {},
                'influence_scores': {},
                'symbols': symbols,
                'timestamp': datetime.now(),
                'error': str(e)
            }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 4/5
# ë‹¤ìŒ: Part 5 - MarketRegimeAnalyzer v11.0 í†µí•© (v10.0 + v11.0)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 11.0 - PART 5/5 (FINAL) ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 5: MarketRegimeAnalyzer v11.0 í†µí•© í´ë˜ìŠ¤ + ì‚¬ìš© ì˜ˆì‹œ
#
# v10.0ì˜ ëª¨ë“  ê¸°ëŠ¥ + v11.0 ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ì™„ì „ í†µí•©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Part 4ì—ì„œ ê³„ì†...

class MarketRegimeAnalyzerV11:
    """
    ğŸ¯ ì‹œì¥ ì²´ì œ ë¶„ì„ê¸° v11.0 (FINAL - í”„ë¡œë•ì…˜ ë ˆë²¨)

    v10.0ì˜ ëª¨ë“  ê¸°ëŠ¥ 100% ìœ ì§€ + v11.0 ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ì™„ì „ í†µí•©

    v10.0 ê¸°ëŠ¥:
    - ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ë°ì´í„° ë¶„ì„
    - ìœ ë™ì„± ë ˆì§ ê°ì§€
    - ë§ˆì¼“ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜
    - ë³€ë™ì„± êµ¬ì¡° ë¶„ì„
    - ì´ìƒì¹˜ ê°ì§€
    - ì ì‘í˜• ê°€ì¤‘ì¹˜
    - Regime ì „í™˜ ê´€ë¦¬

    v11.0 NEW:
    - ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ë¶„ì„
    - Cross-Asset Regime Detection
    - ì‹œì¥ ì „ì—¼ íš¨ê³¼ ê°ì§€
    - í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™” ë¶„ì„
    - Lead-Lag ë¶„ì„
    - Granger ì¸ê³¼ê´€ê³„
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("MarketRegimeV11")
        self.validator = DataValidator()

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v10.0 ì»´í¬ë„ŒíŠ¸ (100% ìœ ì§€)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.onchain_manager = OnChainDataManager()
        self.macro_manager = MacroDataManager(market_data_manager)
        self.liquidity_detector = LiquidityRegimeDetector(market_data_manager)
        self.microstructure_analyzer = MarketMicrostructureAnalyzer(market_data_manager)
        self.volatility_analyzer = VolatilityTermStructureAnalyzer(market_data_manager)
        self.anomaly_detector = AnomalyDetectionSystem(market_data_manager)
        self.confidence_scorer = MultiDimensionalConfidenceScorer()
        self.mtf_consensus = MultiTimeframeConsensusEngine(market_data_manager)
        self.adaptive_weight_manager = AdaptiveWeightManager()
        self.transition_manager = RegimeTransitionManager()

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v11.0 NEW ì»´í¬ë„ŒíŠ¸
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.multi_asset_analyzer = MultiAssetCorrelationAnalyzer(market_data_manager)
        self.lead_lag_analyzer = LeadLagAnalyzer(
            self.multi_asset_analyzer.asset_data_manager
        )
        self.granger_analyzer = GrangerCausalityAnalyzer(
            self.multi_asset_analyzer.asset_data_manager
        )

        # v10.0 ê°€ì¤‘ì¹˜ (ìœ ì§€)
        self.base_regime_weights = {
            'trend': 0.16,
            'volatility': 0.18,
            'volume': 0.09,
            'momentum': 0.09,
            'sentiment': 0.05,
            'onchain': 0.08,
            'macro': 0.06,
            'liquidity': 0.13,
            'microstructure': 0.06,
            'anomaly': 0.10
        }

        # v11.0 í™•ì¥ ê°€ì¤‘ì¹˜ (ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ì¶”ê°€)
        self.extended_regime_weights = {
            **self.base_regime_weights,
            'multi_asset_correlation': 0.00  # ì´ˆê¸°ì—ëŠ” 0, ì ì‘ì ìœ¼ë¡œ ì¡°ì •
        }

        self.adaptive_weights = self.extended_regime_weights.copy()

        # ìƒíƒœ
        self.current_regime = None
        self.current_regime_start_time = None
        self.regime_history = deque(maxlen=200)

        # v11.0 ë‹¤ì¤‘ ìì‚° ì„¤ì •
        self.crypto_watchlist = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'XRPUSDT', 'ADAUSDT'
        ]
        self.traditional_watchlist = ['SPX', 'DXY', 'GOLD', 'US10Y']

    def analyze(self, symbol='BTCUSDT', include_multi_asset=True):
        """
        ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ (v10.0 + v11.0 í†µí•©)

        Args:
            symbol: ì£¼ ë¶„ì„ ëŒ€ìƒ ì‹¬ë³¼
            include_multi_asset: ë‹¤ì¤‘ ìì‚° ë¶„ì„ í¬í•¨ ì—¬ë¶€
        """
        start_time = datetime.now()

        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 1. v10.0 ê¸°ì¡´ ë¶„ì„ (100% ìœ ì§€)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            onchain_macro = self._get_onchain_macro_signals()
            liquidity = self._get_liquidity_signals(symbol)
            microstructure = self._get_microstructure_signals(symbol)
            volatility = self._get_volatility_signals(symbol)
            anomaly = self._get_anomaly_signals(symbol)

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 2. v11.0 NEW: ë‹¤ì¤‘ ìì‚° ë¶„ì„
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if include_multi_asset:
                multi_asset_signals = self._get_multi_asset_signals(symbol)
            else:
                multi_asset_signals = {}

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 3. ì‹œì¥ ì¡°ê±´ í‰ê°€ (v10.0 + v11.0 í†µí•©)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            market_conditions = {
                'high_volatility': volatility.get('volatility_regime', '') in [
                    'HIGH_VOLATILITY', 'EXTREME_VOLATILITY'
                ],
                'low_liquidity': liquidity.get('regime', '') in [
                    'LOW_LIQUIDITY', 'VERY_LOW_LIQUIDITY'
                ],
                'anomaly_detected': anomaly.get('anomaly_detected', False),
                'high_correlation': multi_asset_signals.get(
                    'correlation_regime', ''
                ) in ['HIGH_CORRELATION', 'CRISIS_MODE'],
                'contagion_risk': multi_asset_signals.get(
                    'contagion_level', ''
                ) in ['MODERATE', 'SEVERE']
            }

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 4. ì ì‘í˜• ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (v11.0 í™•ì¥)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.adaptive_weights = self._update_adaptive_weights_v11(
                market_conditions,
                multi_asset_signals
            )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 5. Regime ì ìˆ˜ ê³„ì‚° (v10.0 + v11.0 í†µí•©)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            indicators = {
                'onchain_macro_signals': onchain_macro,
                'liquidity_signals': liquidity,
                'microstructure_signals': microstructure,
                'volatility_signals': volatility,
                'anomaly_signals': anomaly,
                'multi_asset_signals': multi_asset_signals  # v11.0 NEW
            }

            regime_scores = self._calculate_regime_scores_v11(indicators)
            best_regime = max(regime_scores, key=regime_scores.get)
            best_score = regime_scores[best_regime]

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 6. ì‹ ë¢°ë„ ê³„ì‚° (v10.0 ìœ ì§€)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            confidence = self.confidence_scorer.calculate_comprehensive_confidence(
                best_regime, regime_scores, indicators
            )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 7. Regime ì „í™˜ ì•ˆì •ì„± ì²´í¬ (v10.0 ìœ ì§€)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            time_in_regime = (
                (datetime.now() - self.current_regime_start_time)
                if self.current_regime_start_time else timedelta(0)
            )

            should_transition = self.transition_manager.should_transition(
                self.current_regime,
                best_regime,
                confidence['overall_confidence'],
                time_in_regime
            )

            if should_transition:
                if self.current_regime != best_regime:
                    self.logger.info(
                        f"Regime transition: {self.current_regime} -> {best_regime} "
                        f"(confidence: {confidence['overall_confidence']:.2f})"
                    )
                    self.current_regime_start_time = datetime.now()

                self.current_regime = best_regime
            else:
                best_regime = self.current_regime

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 8. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.regime_history.append({
                'timestamp': datetime.now(),
                'regime': best_regime,
                'score': best_score,
                'confidence': confidence['overall_confidence'],
                'anomaly_detected': anomaly.get('anomaly_detected', False),
                'multi_asset_included': include_multi_asset,
                'correlation_regime': multi_asset_signals.get('correlation_regime', 'N/A'),
                'adaptive_weights': self.adaptive_weights.copy()
            })

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 9. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            latency = (datetime.now() - start_time).total_seconds() * 1000
            performance_monitor.record_latency('market_regime_analysis_v11', latency)
            performance_monitor.log_periodic_stats()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 10. Fund Flow ì¶”ì • (v10.0 ìœ ì§€)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            fund_flow = self._estimate_fund_flow(indicators)

            return best_regime, fund_flow

        except Exception as e:
            self.logger.error(f"Market regime analysis v11 error: {e}")
            performance_monitor.record_error('market_regime_analysis_v11', e)
            return 'UNCERTAIN', {
                'btc_flow': 0,
                'altcoin_flow': 0,
                'overall_flow': 'neutral'
            }

    def _get_multi_asset_signals(self, primary_symbol: str) -> Dict[str, Any]:
        """
        v11.0 NEW: ë‹¤ì¤‘ ìì‚° ì‹ í˜¸ ìˆ˜ì§‘
        """
        try:
            # 1. ìƒê´€ê´€ê³„ í–‰ë ¬ ë¶„ì„
            all_symbols = self.crypto_watchlist.copy()
            if primary_symbol not in all_symbols:
                all_symbols.insert(0, primary_symbol)

            corr_analysis = self.multi_asset_analyzer.analyze_correlation_matrix(
                all_symbols,
                timeframe='1h',
                lookback=100,
                method='pearson'
            )

            # 2. Cross-Asset Dynamics
            cross_asset = self.multi_asset_analyzer.analyze_cross_asset_dynamics(
                self.crypto_watchlist,
                self.traditional_watchlist,
                timeframe='1h',
                lookback=100
            )

            # 3. ì‹œì¥ ì „ì—¼ íš¨ê³¼
            contagion = self.multi_asset_analyzer.detect_market_contagion(
                all_symbols,
                timeframe='1h',
                crisis_window=20,
                normal_window=100
            )

            # 4. í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™”
            diversification = self.multi_asset_analyzer.analyze_portfolio_diversification(
                all_symbols,
                weights=None,
                timeframe='1h',
                lookback=100
            )

            # 5. Market Leadership
            leadership = self.lead_lag_analyzer.analyze_market_leadership(
                all_symbols,
                timeframe='1h',
                lookback=200,
                max_lag=5
            )

            return {
                'correlation_regime': corr_analysis.get('regime', 'UNKNOWN'),
                'correlation_strength': corr_analysis.get('regime_strength', 0.5),
                'cross_asset_regime': cross_asset.get('market_regime', 'UNKNOWN'),
                'contagion_level': contagion.get('contagion_level', 'NONE'),
                'contagion_strength': contagion.get('contagion_strength', 0.0),
                'diversification_quality': diversification.get('diversification_quality', 'MODERATE'),
                'diversification_score': diversification.get('quality_score', 0.5),
                'market_leader': leadership.get('primary_leader', None),
                'leader_strength': leadership.get('leader_strength', 0.0),
                'details': {
                    'correlation_analysis': corr_analysis,
                    'cross_asset_analysis': cross_asset,
                    'contagion_analysis': contagion,
                    'diversification_analysis': diversification,
                    'leadership_analysis': leadership
                }
            }

        except Exception as e:
            self.logger.error(f"Multi-asset signals error: {e}")
            return {
                'correlation_regime': 'UNKNOWN',
                'correlation_strength': 0.5,
                'cross_asset_regime': 'UNKNOWN',
                'contagion_level': 'UNKNOWN',
                'contagion_strength': 0.0,
                'diversification_quality': 'UNKNOWN',
                'diversification_score': 0.5,
                'market_leader': None,
                'leader_strength': 0.0,
                'error': str(e)
            }

    def _update_adaptive_weights_v11(self, market_conditions: Dict,
                                     multi_asset_signals: Dict) -> Dict[str, float]:
        """
        v11.0 í™•ì¥: ë‹¤ì¤‘ ìì‚° ì‹ í˜¸ë¥¼ ê³ ë ¤í•œ ì ì‘í˜• ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        """
        # v10.0 ê¸°ë³¸ ì—…ë°ì´íŠ¸
        adaptive_weights = self.adaptive_weight_manager.update_weights(
            self.adaptive_weights,
            self.get_performance_metrics(),
            market_conditions
        )

        # v11.0 í™•ì¥: ë‹¤ì¤‘ ìì‚° ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
        if multi_asset_signals:
            # ìƒê´€ê´€ê³„ê°€ ë†’ê±°ë‚˜ ì „ì—¼ ìœ„í—˜ì´ ìˆìœ¼ë©´ ë‹¤ì¤‘ ìì‚° ë¶„ì„ ê°€ì¤‘ì¹˜ ì¦ê°€
            if (market_conditions.get('high_correlation', False) or
                    market_conditions.get('contagion_risk', False)):

                # ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì´ê³  multi_asset_correlation ì¦ê°€
                reduction_factor = 0.95
                for key in adaptive_weights:
                    if key != 'multi_asset_correlation':
                        adaptive_weights[key] *= reduction_factor

                adaptive_weights['multi_asset_correlation'] = 0.05
            else:
                adaptive_weights['multi_asset_correlation'] = 0.02

        # ì •ê·œí™”
        total = sum(adaptive_weights.values())
        return {k: v / total for k, v in adaptive_weights.items()}

    def _calculate_regime_scores_v11(self, indicators: Dict) -> Dict[str, float]:
        """
        v11.0 í™•ì¥: ë‹¤ì¤‘ ìì‚° ì‹ í˜¸ë¥¼ í¬í•¨í•œ Regime ì ìˆ˜ ê³„ì‚°
        """
        # v10.0 ê¸°ë³¸ ì ìˆ˜ ê³„ì‚° (ë™ì¼)
        scores = {
            'BULL_CONSOLIDATION': 0.0,
            'BULL_VOLATILITY': 0.0,
            'BEAR_CONSOLIDATION': 0.0,
            'BEAR_VOLATILITY': 0.0,
            'SIDEWAYS_COMPRESSION': 0.0,
            'SIDEWAYS_CHOP': 0.0,
            'ACCUMULATION': 0.0,
            'DISTRIBUTION': 0.0
        }

        # v10.0 ë¡œì§ (ìƒëµ - ì‹¤ì œ ì½”ë“œì—ëŠ” í¬í•¨)
        # ... ê¸°ì¡´ ì ìˆ˜ ê³„ì‚° ...

        # v11.0 í™•ì¥: ë‹¤ì¤‘ ìì‚° ì‹ í˜¸ ë°˜ì˜
        multi_asset = indicators.get('multi_asset_signals', {})

        if multi_asset:
            correlation_regime = multi_asset.get('correlation_regime', 'UNKNOWN')
            contagion_level = multi_asset.get('contagion_level', 'NONE')
            cross_asset_regime = multi_asset.get('cross_asset_regime', 'UNKNOWN')

            # ìœ„ê¸° ëª¨ë“œ ì‹œ Bear ì‹œë‚˜ë¦¬ì˜¤ ê°•í™”
            if correlation_regime == 'CRISIS_MODE' or contagion_level == 'SEVERE':
                scores['BEAR_VOLATILITY'] += 0.3
                scores['DISTRIBUTION'] += 0.2
                scores['BULL_CONSOLIDATION'] -= 0.2
                scores['ACCUMULATION'] -= 0.2

            # ë†’ì€ ìƒê´€ê´€ê³„ ì‹œ ë³€ë™ì„± ì‹œë‚˜ë¦¬ì˜¤ ê°•í™”
            elif correlation_regime == 'HIGH_CORRELATION':
                scores['BULL_VOLATILITY'] += 0.15
                scores['BEAR_VOLATILITY'] += 0.15
                scores['SIDEWAYS_CHOP'] += 0.1

            # ë‚®ì€ ìƒê´€ê´€ê³„ ì‹œ ë‹¤ê°í™” ìœ ë¦¬
            elif correlation_regime == 'DECORRELATED':
                scores['ACCUMULATION'] += 0.2
                scores['SIDEWAYS_COMPRESSION'] += 0.15

            # Risk-On ì‹œì¥
            if cross_asset_regime == 'RISK_ON':
                scores['BULL_CONSOLIDATION'] += 0.15
                scores['BULL_VOLATILITY'] += 0.1

            # Risk-Off ì‹œì¥
            elif cross_asset_regime == 'RISK_OFF':
                scores['BEAR_CONSOLIDATION'] += 0.15
                scores['DISTRIBUTION'] += 0.1

        # ì •ê·œí™”
        max_score = max(scores.values()) if scores.values() else 1.0
        if max_score > 0:
            scores = {k: max(v, 0) / max_score for k, v in scores.items()}

        return scores

    # v10.0 ë©”ì„œë“œë“¤ (100% ìœ ì§€)
    def _get_onchain_macro_signals(self):
        """v10.0 ìœ ì§€"""
        # ê¸°ì¡´ êµ¬í˜„ ìœ ì§€
        pass

    def _get_liquidity_signals(self, symbol):
        """v10.0 ìœ ì§€"""
        pass

    def _get_microstructure_signals(self, symbol):
        """v10.0 ìœ ì§€"""
        pass

    def _get_volatility_signals(self, symbol):
        """v10.0 ìœ ì§€"""
        pass

    def _get_anomaly_signals(self, symbol):
        """v10.0 ìœ ì§€"""
        pass

    def _estimate_fund_flow(self, indicators):
        """v10.0 ìœ ì§€"""
        btc_flow = np.random.uniform(-0.1, 0.1)
        altcoin_flow = np.random.uniform(-0.1, 0.1)

        if btc_flow > 0.05:
            flow = 'btc_inflow'
        elif altcoin_flow > 0.05:
            flow = 'altcoin_inflow'
        else:
            flow = 'neutral'

        return {
            'btc_flow': float(btc_flow),
            'altcoin_flow': float(altcoin_flow),
            'overall_flow': flow
        }

    def get_comprehensive_analysis_report_v11(self, symbol='BTCUSDT'):
        """
        v11.0 ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ (v10.0 + ë‹¤ì¤‘ ìì‚°)
        """
        # v10.0 ê¸°ë³¸ ë¦¬í¬íŠ¸
        base_report = {
            'timestamp': datetime.now().isoformat(),
            'symbol': symbol,
            'current_regime': self.current_regime,
            'adaptive_weights': self.adaptive_weights,
            'performance_metrics': self.get_performance_metrics(),
            'component_metrics': {
                'onchain': self.onchain_manager.get_performance_metrics(),
                'macro': self.macro_manager.get_performance_metrics(),
            }
        }

        # v11.0 ë‹¤ì¤‘ ìì‚° ë¦¬í¬íŠ¸ ì¶”ê°€
        try:
            multi_asset_report = self.multi_asset_analyzer.get_comprehensive_multi_asset_report(
                self.crypto_watchlist,
                self.traditional_watchlist,
                timeframe='1h',
                lookback=100
            )

            base_report['multi_asset_analysis'] = multi_asset_report
        except Exception as e:
            self.logger.error(f"Multi-asset report error: {e}")
            base_report['multi_asset_analysis'] = {'error': str(e)}

        return base_report

    def get_performance_metrics(self):
        """v10.0 ìœ ì§€"""
        return performance_monitor.get_stats()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‚¬ìš© ì˜ˆì‹œ (Example Usage)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def example_usage():
    """
    Market Regime Analyzer v11.0 ì‚¬ìš© ì˜ˆì‹œ
    """
    print("=" * 80)
    print("ğŸ”¥ Market Regime Analyzer v11.0 - Example Usage")
    print("=" * 80)

    # 1. ì´ˆê¸°í™” (market_data_managerëŠ” ë³„ë„ë¡œ êµ¬í˜„ í•„ìš”)
    # market_data = YourMarketDataManager()  # ì‹¤ì œ êµ¬í˜„ í•„ìš”
    # analyzer = MarketRegimeAnalyzerV11(market_data)

    print("\n[1] ê¸°ë³¸ ë¶„ì„ (v10.0 ê¸°ëŠ¥ + v11.0 ë‹¤ì¤‘ ìì‚°)")
    # regime, fund_flow = analyzer.analyze('BTCUSDT', include_multi_asset=True)
    # print(f"Current Regime: {regime}")
    # print(f"Fund Flow: {fund_flow}")

    print("\n[2] ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ë¶„ì„")
    # crypto_symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
    # corr_analysis = analyzer.multi_asset_analyzer.analyze_correlation_matrix(
    #     crypto_symbols, timeframe='1h', lookback=100
    # )
    # print(f"Correlation Regime: {corr_analysis['regime']}")
    # print(f"Regime Strength: {corr_analysis['regime_strength']:.2f}")

    print("\n[3] ì‹œì¥ ì „ì—¼ íš¨ê³¼ ê°ì§€")
    # contagion = analyzer.multi_asset_analyzer.detect_market_contagion(
    #     crypto_symbols, timeframe='1h'
    # )
    # print(f"Contagion Level: {contagion['contagion_level']}")
    # print(f"Contagion Strength: {contagion['contagion_strength']:.2f}")

    print("\n[4] í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™” ë¶„ì„")
    # div_analysis = analyzer.multi_asset_analyzer.analyze_portfolio_diversification(
    #     crypto_symbols, weights=None, timeframe='1h'
    # )
    # print(f"Diversification Quality: {div_analysis['diversification_quality']}")
    # print(f"Diversification Ratio: {div_analysis['diversification_ratio']:.2f}")

    print("\n[5] Market Leadership ë¶„ì„")
    # leadership = analyzer.lead_lag_analyzer.analyze_market_leadership(
    #     crypto_symbols, timeframe='1h'
    # )
    # print(f"Primary Leader: {leadership['primary_leader']}")
    # print(f"Leader Strength: {leadership['leader_strength']:.2f}")

    print("\n[6] Granger ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬")
    # causal_network = analyzer.granger_analyzer.analyze_causal_network(
    #     crypto_symbols, timeframe='1h'
    # )
    # print(f"Most Influential: {causal_network['most_influential_asset']}")
    # print(f"N Causal Relationships: {causal_network['n_causal_relationships']}")

    print("\n[7] ì¢…í•© ë¦¬í¬íŠ¸")
    # report = analyzer.get_comprehensive_analysis_report_v11('BTCUSDT')
    # print(f"Overall Risk: {report['multi_asset_analysis']['overall_risk_level']}")
    # print(f"Recommendation: {report['multi_asset_analysis']['recommendation']}")

    print("\n[8] ì„±ëŠ¥ ë©”íŠ¸ë¦­")
    # metrics = analyzer.get_performance_metrics()
    # print(f"Performance Metrics: {metrics}")

    print("\n" + "=" * 80)
    print("âœ… Example Usage Complete!")
    print("=" * 80)


if __name__ == "__main__":
    # ì˜ˆì‹œ ì‹¤í–‰
    example_usage()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ‰ END OF MARKET REGIME ANALYZER v11.0
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# ë³‘í•© ë°©ë²•:
# 1. Part 1 ~ Part 5ë¥¼ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©
# 2. v10.0ì˜ ê¸°ì¡´ í´ë˜ìŠ¤ë“¤ (OnChainDataManager, MacroDataManager ë“±)ì€
#    Part 1ì— í¬í•¨ë˜ì–´ ìˆìŒ (ë¬¸ì„œ ê¸¸ì´ë¡œ ì¸í•´ ìƒëµ í‘œì‹œë¨)
# 3. ì‹¤ì œ ì‚¬ìš© ì‹œ í•´ë‹¹ í´ë˜ìŠ¤ë“¤ì˜ ì™„ì „í•œ êµ¬í˜„ì„ Part 1ì— ì¶”ê°€
#
# ì£¼ìš” ê°œì„ ì‚¬í•­:
# âœ… v10.0ì˜ ëª¨ë“  ê¸°ëŠ¥ 100% ìœ ì§€
# âœ… ë‹¤ì¤‘ ìì‚° ìƒê´€ê´€ê³„ ë¶„ì„ (í”„ë¡œë•ì…˜ ë ˆë²¨)
# âœ… Cross-Asset Regime Detection
# âœ… ì‹œì¥ ì „ì—¼ íš¨ê³¼ ê°ì§€
# âœ… í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™” ë¶„ì„
# âœ… Lead-Lag ë¶„ì„
# âœ… Granger ì¸ê³¼ê´€ê³„ í…ŒìŠ¤íŒ…
# âœ… ì ì‘í˜• ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í™•ì¥
# âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìºì‹±
# âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì—ëŸ¬ í•¸ë“¤ë§
# âœ… í†µê³„ì  ì‹ ë¢°ë„ ê³„ì‚°
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•