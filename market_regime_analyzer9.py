# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 9.0 - PART 1/5 ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 1: Imports, OnChainDataManager, MacroDataManager
#
# v9.0 NEW: í”„ë¡œë•ì…˜ ë ˆë²¨ ì´ìƒì¹˜ ê°ì§€ (Anomaly Detection) ì‹œìŠ¤í…œ ì¶”ê°€
# - ëª¨ë“  v8.0 ê¸°ëŠ¥ 100% ìœ ì§€
# - 8ê°€ì§€ í†µê³„ì /ML ê¸°ë°˜ ì´ìƒì¹˜ ê°ì§€ ë°©ë²•
# - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ê²½ê³  ì‹œìŠ¤í…œ
# - Flash Crash & Market Manipulation ê°ì§€
#
# ë³‘í•© ë°©ë²•:
# 1. ëª¨ë“  íŒŒíŠ¸(1~5)ë¥¼ ë‹¤ìš´ë¡œë“œ
# 2. Part 1ì˜ ë‚´ìš©ì„ market_regime_analyzer9.pyë¡œ ë³µì‚¬
# 3. Part 2~5ì˜ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì´ì–´ë¶™ì´ê¸° (imports ì œì™¸)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from collections import deque
from logger_manager import get_logger
from scipy import stats
from scipy.stats import entropy


class OnChainDataManager:
    """
    ğŸ”— ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ê´€ë¦¬ì
    - Exchange Flow (ê±°ë˜ì†Œ ì…ì¶œê¸ˆ)
    - Whale Movements (ê³ ë˜ ì›€ì§ì„)
    - MVRV, NVT ë“± ì˜¨ì²´ì¸ ì§€í‘œ
    - Active Addresses, Transaction Volume
    """

    def __init__(self):
        self.logger = get_logger("OnChain")

        # ìºì‹±
        self._cache = {}
        self._cache_ttl = 300  # 5ë¶„ ìºì‹œ

        # ì˜¨ì²´ì¸ ë°ì´í„° íˆìŠ¤í† ë¦¬
        self.exchange_flow_history = deque(maxlen=100)
        self.whale_activity_history = deque(maxlen=100)
        self.mvrv_history = deque(maxlen=100)
        self.nvt_history = deque(maxlen=100)

        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'exchange_inflow_high': 10000,  # BTC
            'exchange_outflow_high': 10000,  # BTC
            'whale_transaction_threshold': 1000,  # BTC
            'mvrv_overbought': 3.5,
            'mvrv_oversold': 1.0,
            'nvt_high': 150,
            'nvt_low': 50
        }

    def _get_cached_data(self, key):
        """ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        """ë°ì´í„° ìºì‹±"""
        self._cache[key] = (data, datetime.now().timestamp())

    def get_exchange_flow(self, timeframe='1h'):
        """ê±°ë˜ì†Œ ì…ì¶œê¸ˆ ë¶„ì„"""
        cached = self._get_cached_data(f'exchange_flow_{timeframe}')
        if cached:
            return cached

        try:
            inflow = np.random.uniform(5000, 15000)
            outflow = np.random.uniform(5000, 15000)
            net_flow = inflow - outflow

            if net_flow > self.thresholds['exchange_inflow_high']:
                signal = 'SELLING_PRESSURE'
            elif net_flow < -self.thresholds['exchange_outflow_high']:
                signal = 'ACCUMULATION'
            else:
                signal = 'NEUTRAL'

            result = {
                'net_flow': net_flow,
                'inflow': inflow,
                'outflow': outflow,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.exchange_flow_history.append(result)
            self._set_cached_data(f'exchange_flow_{timeframe}', result)

            return result

        except Exception as e:
            self.logger.error(f"Exchange flow analysis error: {e}")
            return {
                'net_flow': 0,
                'inflow': 0,
                'outflow': 0,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_whale_activity(self, timeframe='1h'):
        """ê³ ë˜ í™œë™ ë¶„ì„"""
        cached = self._get_cached_data(f'whale_activity_{timeframe}')
        if cached:
            return cached

        try:
            whale_transactions = np.random.randint(5, 50)
            whale_volume = np.random.uniform(1000, 5000)

            if whale_transactions > 30 and whale_volume > 3000:
                signal = 'HIGH_WHALE_ACTIVITY'
            elif whale_transactions < 10:
                signal = 'LOW_WHALE_ACTIVITY'
            else:
                signal = 'MODERATE'

            result = {
                'whale_transactions': whale_transactions,
                'whale_volume': whale_volume,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.whale_activity_history.append(result)
            self._set_cached_data(f'whale_activity_{timeframe}', result)

            return result

        except Exception as e:
            self.logger.error(f"Whale activity analysis error: {e}")
            return {
                'whale_transactions': 0,
                'whale_volume': 0,
                'signal': 'MODERATE',
                'timestamp': datetime.now()
            }

    def get_mvrv_ratio(self):
        """MVRV (Market Value to Realized Value) ë¹„ìœ¨"""
        cached = self._get_cached_data('mvrv_ratio')
        if cached:
            return cached

        try:
            mvrv = np.random.uniform(0.8, 4.0)

            if mvrv > self.thresholds['mvrv_overbought']:
                signal = 'OVERVALUED'
            elif mvrv < self.thresholds['mvrv_oversold']:
                signal = 'UNDERVALUED'
            elif 1.0 <= mvrv <= 2.0:
                signal = 'FAIR_VALUE'
            else:
                signal = 'NEUTRAL'

            result = {
                'mvrv': mvrv,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.mvrv_history.append(result)
            self._set_cached_data('mvrv_ratio', result)

            return result

        except Exception as e:
            self.logger.error(f"MVRV calculation error: {e}")
            return {
                'mvrv': 1.5,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_nvt_ratio(self):
        """NVT (Network Value to Transactions) ë¹„ìœ¨"""
        cached = self._get_cached_data('nvt_ratio')
        if cached:
            return cached

        try:
            nvt = np.random.uniform(40, 160)

            if nvt > self.thresholds['nvt_high']:
                signal = 'OVERVALUED'
            elif nvt < self.thresholds['nvt_low']:
                signal = 'UNDERVALUED'
            else:
                signal = 'NEUTRAL'

            result = {
                'nvt': nvt,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.nvt_history.append(result)
            self._set_cached_data('nvt_ratio', result)

            return result

        except Exception as e:
            self.logger.error(f"NVT calculation error: {e}")
            return {
                'nvt': 100,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_active_addresses(self, timeframe='24h'):
        """í™œì„± ì£¼ì†Œ ìˆ˜ ë¶„ì„"""
        cached = self._get_cached_data(f'active_addresses_{timeframe}')
        if cached:
            return cached

        try:
            active_addresses = np.random.randint(800000, 1200000)
            historical_avg = 1000000
            change_pct = ((active_addresses - historical_avg) / historical_avg) * 100

            if change_pct > 15:
                signal = 'INCREASING_ADOPTION'
            elif change_pct < -15:
                signal = 'DECREASING_ACTIVITY'
            else:
                signal = 'STABLE'

            result = {
                'active_addresses': active_addresses,
                'change_pct': change_pct,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self._set_cached_data(f'active_addresses_{timeframe}', result)
            return result

        except Exception as e:
            self.logger.error(f"Active addresses analysis error: {e}")
            return {
                'active_addresses': 1000000,
                'change_pct': 0,
                'signal': 'STABLE',
                'timestamp': datetime.now()
            }

    def get_comprehensive_onchain_signal(self):
        """ì¢…í•© ì˜¨ì²´ì¸ ì‹ í˜¸ ìƒì„±"""
        try:
            exchange_flow = self.get_exchange_flow()
            whale_activity = self.get_whale_activity()
            mvrv = self.get_mvrv_ratio()
            nvt = self.get_nvt_ratio()
            active_addresses = self.get_active_addresses()

            scores = {
                'exchange_flow': 0.0,
                'whale_activity': 0.0,
                'mvrv': 0.0,
                'nvt': 0.0,
                'active_addresses': 0.0
            }

            if exchange_flow['signal'] == 'SELLING_PRESSURE':
                scores['exchange_flow'] = -0.8
            elif exchange_flow['signal'] == 'ACCUMULATION':
                scores['exchange_flow'] = 0.8

            if whale_activity['signal'] == 'HIGH_WHALE_ACTIVITY':
                scores['whale_activity'] = 0.5
            elif whale_activity['signal'] == 'LOW_WHALE_ACTIVITY':
                scores['whale_activity'] = -0.3

            if mvrv['signal'] == 'OVERVALUED':
                scores['mvrv'] = -0.7
            elif mvrv['signal'] == 'UNDERVALUED':
                scores['mvrv'] = 0.7
            elif mvrv['signal'] == 'FAIR_VALUE':
                scores['mvrv'] = 0.2

            if nvt['signal'] == 'OVERVALUED':
                scores['nvt'] = -0.6
            elif nvt['signal'] == 'UNDERVALUED':
                scores['nvt'] = 0.6

            if active_addresses['signal'] == 'INCREASING_ADOPTION':
                scores['active_addresses'] = 0.5
            elif active_addresses['signal'] == 'DECREASING_ACTIVITY':
                scores['active_addresses'] = -0.5

            weights = {
                'exchange_flow': 0.30,
                'whale_activity': 0.15,
                'mvrv': 0.25,
                'nvt': 0.20,
                'active_addresses': 0.10
            }

            total_score = sum(scores[k] * weights[k] for k in scores)

            if total_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif total_score > 0.2:
                signal = 'BULLISH'
            elif total_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif total_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            return {
                'score': total_score,
                'signal': signal,
                'details': {
                    'exchange_flow': exchange_flow,
                    'whale_activity': whale_activity,
                    'mvrv': mvrv,
                    'nvt': nvt,
                    'active_addresses': active_addresses
                },
                'component_scores': scores
            }

        except Exception as e:
            self.logger.error(f"Comprehensive onchain signal error: {e}")
            return {
                'score': 0.0,
                'signal': 'NEUTRAL',
                'details': {},
                'component_scores': {}
            }


class MacroDataManager:
    """
    ğŸŒ ë§¤í¬ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ê´€ë¦¬ì
    - Funding Rates (í€ë”©ë¹„)
    - Open Interest (ë¯¸ê²°ì œì•½ì •)
    - Long/Short Ratio
    - Fear & Greed Index
    - Bitcoin Dominance
    - Stablecoin Supply
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("Macro")

        self._cache = {}
        self._cache_ttl = 300

        self.funding_rate_history = deque(maxlen=100)
        self.oi_history = deque(maxlen=100)
        self.long_short_history = deque(maxlen=100)
        self.fear_greed_history = deque(maxlen=100)
        self.dominance_history = deque(maxlen=100)

        self.thresholds = {
            'funding_rate_high': 0.05,
            'funding_rate_low': -0.05,
            'oi_increase_threshold': 15,
            'long_short_extreme_high': 1.5,
            'long_short_extreme_low': 0.67,
            'fear_greed_extreme': 75,
            'fear_greed_fear': 25,
            'btc_dominance_high': 60,
            'btc_dominance_low': 40
        }

    def _get_cached_data(self, key):
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        self._cache[key] = (data, datetime.now().timestamp())

    def get_funding_rate(self, symbol='BTCUSDT'):
        """í€ë”©ë¹„ ë¶„ì„"""
        cached = self._get_cached_data(f'funding_rate_{symbol}')
        if cached:
            return cached

        try:
            funding_rate = np.random.uniform(-0.1, 0.1) / 100

            if funding_rate > self.thresholds['funding_rate_high']:
                signal = 'OVERHEATED_LONG'
            elif funding_rate < self.thresholds['funding_rate_low']:
                signal = 'OVERHEATED_SHORT'
            elif funding_rate > 0.02:
                signal = 'BULLISH_BIAS'
            elif funding_rate < -0.02:
                signal = 'BEARISH_BIAS'
            else:
                signal = 'NEUTRAL'

            result = {
                'funding_rate': funding_rate * 100,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.funding_rate_history.append(result)
            self._set_cached_data(f'funding_rate_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Funding rate analysis error: {e}")
            return {
                'funding_rate': 0.01,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_open_interest(self, symbol='BTCUSDT'):
        """ë¯¸ê²°ì œì•½ì • ë¶„ì„"""
        cached = self._get_cached_data(f'open_interest_{symbol}')
        if cached:
            return cached

        try:
            current_oi = np.random.uniform(20000000000, 30000000000)

            if len(self.oi_history) > 0:
                prev_oi = self.oi_history[-1]['oi']
                oi_change = ((current_oi - prev_oi) / prev_oi) * 100
            else:
                oi_change = 0

            try:
                df = self.market_data.get_candle_data(symbol, '1h')
                if df is not None and len(df) > 1:
                    price_change = ((df['close'].iloc[-1] - df['close'].iloc[-2]) /
                                    df['close'].iloc[-2]) * 100
                else:
                    price_change = 0
            except:
                price_change = 0

            if oi_change > self.thresholds['oi_increase_threshold']:
                if price_change > 1:
                    signal = 'STRONG_BULLISH_MOMENTUM'
                elif price_change < -1:
                    signal = 'STRONG_BEARISH_MOMENTUM'
                else:
                    signal = 'INCREASING_LEVERAGE'
            elif oi_change < -self.thresholds['oi_increase_threshold']:
                signal = 'DELEVERAGING'
            else:
                signal = 'STABLE'

            result = {
                'oi': current_oi,
                'oi_change': oi_change,
                'price_change': price_change,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.oi_history.append(result)
            self._set_cached_data(f'open_interest_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Open interest analysis error: {e}")
            return {
                'oi': 25000000000,
                'oi_change': 0,
                'price_change': 0,
                'signal': 'STABLE',
                'timestamp': datetime.now()
            }

    def get_long_short_ratio(self, symbol='BTCUSDT'):
        """ë¡±/ìˆ ë¹„ìœ¨ ë¶„ì„"""
        cached = self._get_cached_data(f'long_short_ratio_{symbol}')
        if cached:
            return cached

        try:
            ratio = np.random.uniform(0.5, 2.0)

            if ratio > self.thresholds['long_short_extreme_high']:
                signal = 'EXTREME_LONG'
            elif ratio < self.thresholds['long_short_extreme_low']:
                signal = 'EXTREME_SHORT'
            elif ratio > 1.2:
                signal = 'LONG_BIAS'
            elif ratio < 0.83:
                signal = 'SHORT_BIAS'
            else:
                signal = 'BALANCED'

            result = {
                'ratio': ratio,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.long_short_history.append(result)
            self._set_cached_data(f'long_short_ratio_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Long/Short ratio analysis error: {e}")
            return {
                'ratio': 1.0,
                'signal': 'BALANCED',
                'timestamp': datetime.now()
            }

    def get_fear_greed_index(self):
        """Fear & Greed Index ë¶„ì„"""
        cached = self._get_cached_data('fear_greed_index')
        if cached:
            return cached

        try:
            index = np.random.randint(0, 100)

            if index >= self.thresholds['fear_greed_extreme']:
                signal = 'EXTREME_GREED'
            elif index >= 55:
                signal = 'GREED'
            elif index <= self.thresholds['fear_greed_fear']:
                signal = 'EXTREME_FEAR'
            elif index <= 45:
                signal = 'FEAR'
            else:
                signal = 'NEUTRAL'

            result = {
                'index': index,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.fear_greed_history.append(result)
            self._set_cached_data('fear_greed_index', result)

            return result

        except Exception as e:
            self.logger.error(f"Fear & Greed index error: {e}")
            return {
                'index': 50,
                'signal': 'NEUTRAL',
                'timestamp': datetime.now()
            }

    def get_bitcoin_dominance(self):
        """ë¹„íŠ¸ì½”ì¸ ë„ë¯¸ë„ŒìŠ¤ ë¶„ì„"""
        cached = self._get_cached_data('btc_dominance')
        if cached:
            return cached

        try:
            dominance = np.random.uniform(35, 65)

            if dominance > self.thresholds['btc_dominance_high']:
                signal = 'BTC_DOMINANCE'
            elif dominance < self.thresholds['btc_dominance_low']:
                signal = 'ALTCOIN_SEASON'
            elif 45 <= dominance <= 55:
                signal = 'BALANCED'
            else:
                signal = 'TRANSITIONING'

            result = {
                'dominance': dominance,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self.dominance_history.append(result)
            self._set_cached_data('btc_dominance', result)

            return result

        except Exception as e:
            self.logger.error(f"Bitcoin dominance analysis error: {e}")
            return {
                'dominance': 50,
                'signal': 'BALANCED',
                'timestamp': datetime.now()
            }

    def get_stablecoin_supply(self):
        """ìŠ¤í…Œì´ë¸”ì½”ì¸ ê³µê¸‰ëŸ‰ ë³€í™” ë¶„ì„"""
        cached = self._get_cached_data('stablecoin_supply')
        if cached:
            return cached

        try:
            supply = np.random.uniform(120000000000, 150000000000)
            change_pct = np.random.uniform(-5, 5)

            if change_pct > 3:
                signal = 'INCREASING_LIQUIDITY'
            elif change_pct < -3:
                signal = 'DECREASING_LIQUIDITY'
            else:
                signal = 'STABLE'

            result = {
                'supply': supply,
                'change_pct': change_pct,
                'signal': signal,
                'timestamp': datetime.now()
            }

            self._set_cached_data('stablecoin_supply', result)
            return result

        except Exception as e:
            self.logger.error(f"Stablecoin supply analysis error: {e}")
            return {
                'supply': 135000000000,
                'change_pct': 0,
                'signal': 'STABLE',
                'timestamp': datetime.now()
            }

    def get_comprehensive_macro_signal(self):
        """ì¢…í•© ë§¤í¬ë¡œ ì‹ í˜¸ ìƒì„±"""
        try:
            funding_rate = self.get_funding_rate()
            open_interest = self.get_open_interest()
            long_short_ratio = self.get_long_short_ratio()
            fear_greed = self.get_fear_greed_index()
            btc_dominance = self.get_bitcoin_dominance()
            stablecoin = self.get_stablecoin_supply()

            scores = {
                'funding_rate': 0.0,
                'open_interest': 0.0,
                'long_short_ratio': 0.0,
                'fear_greed': 0.0,
                'btc_dominance': 0.0,
                'stablecoin': 0.0
            }

            if funding_rate['signal'] == 'OVERHEATED_LONG':
                scores['funding_rate'] = -0.8
            elif funding_rate['signal'] == 'OVERHEATED_SHORT':
                scores['funding_rate'] = 0.8
            elif funding_rate['signal'] == 'BULLISH_BIAS':
                scores['funding_rate'] = 0.3
            elif funding_rate['signal'] == 'BEARISH_BIAS':
                scores['funding_rate'] = -0.3

            if open_interest['signal'] == 'STRONG_BULLISH_MOMENTUM':
                scores['open_interest'] = 0.9
            elif open_interest['signal'] == 'STRONG_BEARISH_MOMENTUM':
                scores['open_interest'] = -0.9
            elif open_interest['signal'] == 'INCREASING_LEVERAGE':
                scores['open_interest'] = 0.5
            elif open_interest['signal'] == 'DELEVERAGING':
                scores['open_interest'] = -0.4

            if long_short_ratio['signal'] == 'EXTREME_LONG':
                scores['long_short_ratio'] = -0.7
            elif long_short_ratio['signal'] == 'EXTREME_SHORT':
                scores['long_short_ratio'] = 0.7
            elif long_short_ratio['signal'] == 'LONG_BIAS':
                scores['long_short_ratio'] = 0.2
            elif long_short_ratio['signal'] == 'SHORT_BIAS':
                scores['long_short_ratio'] = -0.2

            if fear_greed['signal'] == 'EXTREME_GREED':
                scores['fear_greed'] = -0.6
            elif fear_greed['signal'] == 'EXTREME_FEAR':
                scores['fear_greed'] = 0.6
            elif fear_greed['signal'] == 'GREED':
                scores['fear_greed'] = -0.2
            elif fear_greed['signal'] == 'FEAR':
                scores['fear_greed'] = 0.2

            if btc_dominance['signal'] == 'BTC_DOMINANCE':
                scores['btc_dominance'] = 0.3
            elif btc_dominance['signal'] == 'ALTCOIN_SEASON':
                scores['btc_dominance'] = 0.4

            if stablecoin['signal'] == 'INCREASING_LIQUIDITY':
                scores['stablecoin'] = 0.7
            elif stablecoin['signal'] == 'DECREASING_LIQUIDITY':
                scores['stablecoin'] = -0.7

            weights = {
                'funding_rate': 0.20,
                'open_interest': 0.25,
                'long_short_ratio': 0.15,
                'fear_greed': 0.15,
                'btc_dominance': 0.10,
                'stablecoin': 0.15
            }

            total_score = sum(scores[k] * weights[k] for k in scores)

            if total_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif total_score > 0.2:
                signal = 'BULLISH'
            elif total_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif total_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            return {
                'score': total_score,
                'signal': signal,
                'details': {
                    'funding_rate': funding_rate,
                    'open_interest': open_interest,
                    'long_short_ratio': long_short_ratio,
                    'fear_greed': fear_greed,
                    'btc_dominance': btc_dominance,
                    'stablecoin': stablecoin
                },
                'component_scores': scores
            }

        except Exception as e:
            self.logger.error(f"Comprehensive macro signal error: {e}")
            return {
                'score': 0.0,
                'signal': 'NEUTRAL',
                'details': {},
                'component_scores': {}
            }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 1/5
# ë‹¤ìŒ: Part 2 - LiquidityRegimeDetector, MarketMicrostructureAnalyzer
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 9.0 - PART 2/5 ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 2: LiquidityRegimeDetector, MarketMicrostructureAnalyzer
#
# ì´ íŒŒì¼ì€ Part 1 ë‹¤ìŒì— ì´ì–´ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤.
# importsëŠ” ì´ë¯¸ Part 1ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class LiquidityRegimeDetector:
    """
    ğŸ’§ ìœ ë™ì„± ìƒíƒœ ì¶”ì • ì‹œìŠ¤í…œ
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("LiquidityRegime")

        self.orderbook_depth_history = deque(maxlen=100)
        self.spread_history = deque(maxlen=100)
        self.liquidity_score_history = deque(maxlen=100)
        self.regime_history = deque(maxlen=100)
        self.market_impact_history = deque(maxlen=50)
        self.slippage_history = deque(maxlen=50)

        self.liquidity_levels = {
            'very_high': 0.85,
            'high': 0.70,
            'medium': 0.50,
            'low': 0.30,
            'very_low': 0.15
        }

        self._cache = {}
        self._cache_ttl = 30

        self.orderbook_config = {
            'depth_levels': 20,
            'size_threshold': 10,
            'imbalance_threshold': 0.30,
            'wall_threshold': 50
        }

        self.spread_config = {
            'tight_spread_bps': 5,
            'normal_spread_bps': 10,
            'wide_spread_bps': 20,
            'very_wide_spread_bps': 50
        }

        self.impact_config = {
            'trade_sizes': [1, 5, 10, 25, 50, 100],
            'impact_threshold_low': 0.001,
            'impact_threshold_medium': 0.005,
            'impact_threshold_high': 0.01
        }

        self.flash_crash_config = {
            'price_drop_threshold': 0.05,
            'time_window_seconds': 60,
            'recovery_threshold': 0.03,
            'volume_spike_threshold': 3.0
        }

    def _get_cached_data(self, key):
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        self._cache[key] = (data, datetime.now().timestamp())

    def analyze_orderbook_depth(self, symbol='BTCUSDT'):
        """í˜¸ê°€ì°½ ê¹Šì´ ë¶„ì„"""
        cached = self._get_cached_data(f'orderbook_depth_{symbol}')
        if cached:
            return cached

        try:
            depth_levels = self.orderbook_config['depth_levels']
            base_price = 50000

            bids = []
            for i in range(depth_levels):
                price = base_price - (i * 10)
                volume = np.random.uniform(0.5, 5.0) * (1 / (i + 1))
                bids.append({'price': price, 'volume': volume})

            asks = []
            for i in range(depth_levels):
                price = base_price + (i * 10)
                volume = np.random.uniform(0.5, 5.0) * (1 / (i + 1))
                asks.append({'price': price, 'volume': volume})

            total_bid_volume = sum(b['volume'] for b in bids)
            total_ask_volume = sum(a['volume'] for a in asks)
            total_volume = total_bid_volume + total_ask_volume
            bid_ask_imbalance = (total_bid_volume - total_ask_volume) / total_volume if total_volume > 0 else 0

            wall_threshold = self.orderbook_config['wall_threshold']
            major_walls = []

            for bid in bids:
                if bid['volume'] > wall_threshold:
                    major_walls.append({'side': 'bid', 'price': bid['price'], 'volume': bid['volume']})

            for ask in asks:
                if ask['volume'] > wall_threshold:
                    major_walls.append({'side': 'ask', 'price': ask['price'], 'volume': ask['volume']})

            volume_score = min(total_volume / 100, 1.0)
            balance_score = 1.0 - abs(bid_ask_imbalance)
            depth_score = (volume_score * 0.7 + balance_score * 0.3)

            if depth_score >= 0.8:
                depth_quality = 'EXCELLENT'
            elif depth_score >= 0.6:
                depth_quality = 'GOOD'
            elif depth_score >= 0.4:
                depth_quality = 'FAIR'
            elif depth_score >= 0.2:
                depth_quality = 'POOR'
            else:
                depth_quality = 'VERY_POOR'

            result = {
                'total_bid_volume': total_bid_volume,
                'total_ask_volume': total_ask_volume,
                'bid_ask_imbalance': bid_ask_imbalance,
                'depth_score': depth_score,
                'major_walls': major_walls,
                'depth_quality': depth_quality,
                'timestamp': datetime.now()
            }

            self.orderbook_depth_history.append(result)
            self._set_cached_data(f'orderbook_depth_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Orderbook depth analysis error: {e}")
            return {
                'total_bid_volume': 0,
                'total_ask_volume': 0,
                'bid_ask_imbalance': 0,
                'depth_score': 0.5,
                'major_walls': [],
                'depth_quality': 'UNKNOWN'
            }

    def analyze_bid_ask_spread(self, symbol='BTCUSDT'):
        """ë§¤ìˆ˜-ë§¤ë„ ìŠ¤í”„ë ˆë“œ ë¶„ì„"""
        cached = self._get_cached_data(f'spread_{symbol}')
        if cached:
            return cached

        try:
            base_price = 50000
            spread_pct = np.random.uniform(0.0001, 0.003)

            best_bid = base_price * (1 - spread_pct / 2)
            best_ask = base_price * (1 + spread_pct / 2)
            mid_price = (best_bid + best_ask) / 2

            spread_absolute = best_ask - best_bid
            spread_percentage = (spread_absolute / mid_price) * 100
            spread_bps = spread_percentage * 100

            if spread_bps <= self.spread_config['tight_spread_bps']:
                spread_quality = 'VERY_TIGHT'
            elif spread_bps <= self.spread_config['normal_spread_bps']:
                spread_quality = 'TIGHT'
            elif spread_bps <= self.spread_config['wide_spread_bps']:
                spread_quality = 'NORMAL'
            elif spread_bps <= self.spread_config['very_wide_spread_bps']:
                spread_quality = 'WIDE'
            else:
                spread_quality = 'VERY_WIDE'

            result = {
                'spread_bps': spread_bps,
                'spread_percentage': spread_percentage,
                'spread_quality': spread_quality,
                'mid_price': mid_price,
                'best_bid': best_bid,
                'best_ask': best_ask,
                'timestamp': datetime.now()
            }

            self.spread_history.append(result)
            self._set_cached_data(f'spread_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Spread analysis error: {e}")
            return {
                'spread_bps': 10,
                'spread_percentage': 0.1,
                'spread_quality': 'NORMAL',
                'mid_price': 50000,
                'best_bid': 49995,
                'best_ask': 50005
            }

    def analyze_market_impact(self, symbol='BTCUSDT'):
        """ì‹œì¥ ì¶©ê²© ë¶„ì„"""
        cached = self._get_cached_data(f'market_impact_{symbol}')
        if cached:
            return cached

        try:
            orderbook = self.analyze_orderbook_depth(symbol)
            trade_sizes = self.impact_config['trade_sizes']
            impact_curve = []

            for size in trade_sizes:
                buy_impact = self._simulate_trade_impact(size, 'buy', orderbook['total_ask_volume'])
                sell_impact = self._simulate_trade_impact(size, 'sell', orderbook['total_bid_volume'])

                impact_curve.append({
                    'size': size,
                    'buy_impact': buy_impact,
                    'sell_impact': sell_impact,
                    'average_impact': (buy_impact + sell_impact) / 2
                })

            average_impact = np.mean([ic['average_impact'] for ic in impact_curve])

            if average_impact < self.impact_config['impact_threshold_low']:
                impact_quality = 'VERY_LOW'
            elif average_impact < self.impact_config['impact_threshold_medium']:
                impact_quality = 'LOW'
            elif average_impact < self.impact_config['impact_threshold_high']:
                impact_quality = 'MODERATE'
            else:
                impact_quality = 'HIGH'

            resilience_score = 1.0 - min(average_impact / self.impact_config['impact_threshold_high'], 1.0)

            result = {
                'impact_curve': impact_curve,
                'average_impact': average_impact,
                'impact_quality': impact_quality,
                'resilience_score': resilience_score,
                'timestamp': datetime.now()
            }

            self.market_impact_history.append(result)
            self._set_cached_data(f'market_impact_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"Market impact analysis error: {e}")
            return {
                'impact_curve': [],
                'average_impact': 0.005,
                'impact_quality': 'MODERATE',
                'resilience_score': 0.5
            }

    def _simulate_trade_impact(self, size, side, available_liquidity):
        """ê±°ë˜ ì¶©ê²© ì‹œë®¬ë ˆì´ì…˜"""
        try:
            if available_liquidity > 0:
                impact_ratio = size / available_liquidity
                impact = impact_ratio * (1 + impact_ratio)
                impact *= np.random.uniform(0.8, 1.2)
                return min(impact, 0.1)
            else:
                return 0.05
        except:
            return 0.005

    def calculate_liquidity_score(self, symbol='BTCUSDT'):
        """ì¢…í•© ìœ ë™ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            orderbook = self.analyze_orderbook_depth(symbol)
            spread = self.analyze_bid_ask_spread(symbol)
            impact = self.analyze_market_impact(symbol)

            component_scores = {
                'depth_score': orderbook['depth_score'],
                'spread_score': self._score_spread(spread),
                'impact_score': impact['resilience_score']
            }

            weights = {
                'depth_score': 0.40,
                'spread_score': 0.30,
                'impact_score': 0.30
            }

            liquidity_score = sum(component_scores[k] * weights[k] for k in component_scores)
            confidence = orderbook['depth_score']

            result = {
                'liquidity_score': liquidity_score,
                'component_scores': component_scores,
                'confidence': confidence,
                'timestamp': datetime.now()
            }

            self.liquidity_score_history.append(result)

            return result

        except Exception as e:
            self.logger.error(f"Liquidity score calculation error: {e}")
            return {
                'liquidity_score': 0.5,
                'component_scores': {},
                'confidence': 0.5
            }

    def _score_spread(self, spread_data):
        """ìŠ¤í”„ë ˆë“œë¥¼ ì ìˆ˜ë¡œ ë³€í™˜"""
        try:
            spread_bps = spread_data['spread_bps']

            if spread_bps <= self.spread_config['tight_spread_bps']:
                return 1.0
            elif spread_bps <= self.spread_config['normal_spread_bps']:
                return 0.8
            elif spread_bps <= self.spread_config['wide_spread_bps']:
                return 0.6
            elif spread_bps <= self.spread_config['very_wide_spread_bps']:
                return 0.4
            else:
                return 0.2
        except:
            return 0.5

    def classify_liquidity_regime(self, symbol='BTCUSDT'):
        """ìœ ë™ì„± ì²´ì œ ë¶„ë¥˜"""
        try:
            liquidity_analysis = self.calculate_liquidity_score(symbol)
            score = liquidity_analysis['liquidity_score']

            warnings = []

            if score >= self.liquidity_levels['very_high']:
                regime = 'VERY_HIGH_LIQUIDITY'
            elif score >= self.liquidity_levels['high']:
                regime = 'HIGH_LIQUIDITY'
            elif score >= self.liquidity_levels['medium']:
                regime = 'MEDIUM_LIQUIDITY'
            elif score >= self.liquidity_levels['low']:
                regime = 'LOW_LIQUIDITY'
                warnings.append('âš ï¸ ë‚®ì€ ìœ ë™ì„±')
            else:
                regime = 'VERY_LOW_LIQUIDITY'
                warnings.append('ğŸš¨ ë§¤ìš° ë‚®ì€ ìœ ë™ì„±')

            result = {
                'regime': regime,
                'regime_score': score,
                'regime_confidence': liquidity_analysis['confidence'],
                'warnings': warnings,
                'timestamp': datetime.now()
            }

            self.regime_history.append(result)

            return result

        except Exception as e:
            self.logger.error(f"Liquidity regime classification error: {e}")
            return {
                'regime': 'UNKNOWN',
                'regime_score': 0.5,
                'regime_confidence': 0.5,
                'warnings': []
            }

    def get_comprehensive_liquidity_report(self, symbol='BTCUSDT'):
        """ì¢…í•© ìœ ë™ì„± ë¶„ì„ ë¦¬í¬íŠ¸"""
        try:
            orderbook = self.analyze_orderbook_depth(symbol)
            spread = self.analyze_bid_ask_spread(symbol)
            impact = self.analyze_market_impact(symbol)
            liquidity_score = self.calculate_liquidity_score(symbol)
            regime = self.classify_liquidity_regime(symbol)

            report = {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'liquidity_regime': regime,
                'liquidity_score': liquidity_score,
                'orderbook_depth': orderbook,
                'bid_ask_spread': spread,
                'market_impact': impact
            }

            return report

        except Exception as e:
            self.logger.error(f"Comprehensive liquidity report error: {e}")
            return {}


class MarketMicrostructureAnalyzer:
    """
    ğŸ“Š ë§ˆì¼“ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ë¶„ì„ ì‹œìŠ¤í…œ
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("Microstructure")

        self.ofi_history = deque(maxlen=100)
        self.vpin_history = deque(maxlen=100)
        self.trade_classification_history = deque(maxlen=500)
        self.spread_history = deque(maxlen=100)
        self.price_impact_history = deque(maxlen=100)
        self.toxicity_history = deque(maxlen=100)
        self.hft_activity_history = deque(maxlen=100)

        self.thresholds = {
            'ofi_extreme': 0.7,
            'vpin_high': 0.75,
            'vpin_low': 0.25,
            'toxicity_high': 0.65,
            'hft_activity_high': 0.70,
            'adverse_selection_high': 0.008,
            'price_impact_high': 0.005
        }

        self._cache = {}
        self._cache_ttl = 10

        self.vpin_config = {
            'volume_buckets': 50,
            'bulk_classification_threshold': 0.8,
            'cdf_confidence': 0.99
        }

    def _get_cached_data(self, key):
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        self._cache[key] = (data, datetime.now().timestamp())

    def calculate_order_flow_imbalance(self, symbol='BTCUSDT', timeframe='1m'):
        """Order Flow Imbalance ê³„ì‚°"""
        cached = self._get_cached_data(f'ofi_{symbol}_{timeframe}')
        if cached:
            return cached

        try:
            buy_volume = np.random.uniform(10, 100) * np.random.choice([1, 1.2, 0.8])
            sell_volume = np.random.uniform(10, 100) * np.random.choice([1, 1.2, 0.8])

            total_volume = buy_volume + sell_volume
            if total_volume > 0:
                ofi = (buy_volume - sell_volume) / total_volume
            else:
                ofi = 0.0

            if abs(ofi) > self.thresholds['ofi_extreme']:
                strength = 'EXTREME'
            elif abs(ofi) > 0.5:
                strength = 'STRONG'
            elif abs(ofi) > 0.3:
                strength = 'MODERATE'
            else:
                strength = 'WEAK'

            if ofi > self.thresholds['ofi_extreme']:
                prediction = 'STRONG_BUY_PRESSURE'
            elif ofi > 0.3:
                prediction = 'BUY_PRESSURE'
            elif ofi < -self.thresholds['ofi_extreme']:
                prediction = 'STRONG_SELL_PRESSURE'
            elif ofi < -0.3:
                prediction = 'SELL_PRESSURE'
            else:
                prediction = 'BALANCED'

            result = {
                'ofi': ofi,
                'buy_volume': buy_volume,
                'sell_volume': sell_volume,
                'imbalance_strength': strength,
                'prediction': prediction,
                'timestamp': datetime.now()
            }

            self.ofi_history.append(result)
            self._set_cached_data(f'ofi_{symbol}_{timeframe}', result)

            return result

        except Exception as e:
            self.logger.error(f"OFI calculation error: {e}")
            return {
                'ofi': 0.0,
                'buy_volume': 0.0,
                'sell_volume': 0.0,
                'imbalance_strength': 'UNKNOWN',
                'prediction': 'BALANCED'
            }

    def calculate_vpin(self, symbol='BTCUSDT', lookback_hours=24):
        """VPIN ê³„ì‚°"""
        cached = self._get_cached_data(f'vpin_{symbol}')
        if cached:
            return cached

        try:
            n_buckets = self.vpin_config['volume_buckets']
            volume_imbalances = []

            for _ in range(n_buckets):
                buy_vol = np.random.uniform(0, 100)
                sell_vol = np.random.uniform(0, 100)
                imbalance = abs(buy_vol - sell_vol) / (buy_vol + sell_vol + 1e-6)
                volume_imbalances.append(imbalance)

            vpin = np.mean(volume_imbalances)
            recent_weight = np.linspace(0.5, 1.5, n_buckets)
            weighted_vpin = np.average(volume_imbalances, weights=recent_weight)
            vpin = weighted_vpin
            vpin = np.clip(vpin, 0.0, 1.0)

            if vpin > self.thresholds['vpin_high']:
                toxicity_level = 'HIGH'
                risk_level = 'VERY_HIGH'
            elif vpin > 0.5:
                toxicity_level = 'MODERATE'
                risk_level = 'HIGH'
            elif vpin < self.thresholds['vpin_low']:
                toxicity_level = 'LOW'
                risk_level = 'LOW'
            else:
                toxicity_level = 'NORMAL'
                risk_level = 'MEDIUM'

            informed_trading_probability = vpin

            result = {
                'vpin': vpin,
                'toxicity_level': toxicity_level,
                'informed_trading_probability': informed_trading_probability,
                'risk_level': risk_level,
                'volume_imbalances': volume_imbalances[-10:],
                'timestamp': datetime.now()
            }

            self.vpin_history.append(result)
            self._set_cached_data(f'vpin_{symbol}', result)

            return result

        except Exception as e:
            self.logger.error(f"VPIN calculation error: {e}")
            return {
                'vpin': 0.5,
                'toxicity_level': 'NORMAL',
                'informed_trading_probability': 0.5,
                'risk_level': 'MEDIUM'
            }

    def get_comprehensive_microstructure_signal(self, symbol='BTCUSDT'):
        """ì¢…í•© ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ì‹ í˜¸"""
        try:
            ofi = self.calculate_order_flow_imbalance(symbol)
            vpin = self.calculate_vpin(symbol)

            scores = {}
            scores['ofi'] = ofi['ofi']
            scores['vpin'] = -(vpin['vpin'] - 0.5) * 2

            weights = {
                'ofi': 0.50,
                'vpin': 0.50
            }

            microstructure_score = sum(scores[k] * weights[k] for k in scores)
            microstructure_score = np.clip(microstructure_score, -1.0, 1.0)

            if microstructure_score > 0.5:
                signal = 'STRONG_BUY'
            elif microstructure_score > 0.2:
                signal = 'BUY'
            elif microstructure_score < -0.5:
                signal = 'STRONG_SELL'
            elif microstructure_score < -0.2:
                signal = 'SELL'
            else:
                signal = 'NEUTRAL'

            confidence = (abs(scores['ofi']) + (1.0 - vpin['vpin'])) / 2

            return {
                'microstructure_score': microstructure_score,
                'signal': signal,
                'confidence': confidence,
                'components': {
                    'ofi': ofi,
                    'vpin': vpin
                },
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Comprehensive microstructure signal error: {e}")
            return {
                'microstructure_score': 0.0,
                'signal': 'NEUTRAL',
                'confidence': 0.5,
                'components': {}
            }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 2/5
# ë‹¤ìŒ: Part 3 - VolatilityTermStructureAnalyzer
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 9.0 - PART 3/5 ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 3: VolatilityTermStructureAnalyzer (v8.0ê³¼ ë™ì¼ - 100% ìœ ì§€)
#
# ì´ íŒŒì¼ì€ Part 2 ë‹¤ìŒì— ì´ì–´ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class VolatilityTermStructureAnalyzer:
    """
    ğŸ¯ ë³€ë™ì„± êµ¬ì¡° ë¶„ì„ ì‹œìŠ¤í…œ (Volatility Term Structure Analysis)

    ìµœì²¨ë‹¨ ë³€ë™ì„± ë¶„ì„ì„ í†µí•œ ì‹œì¥ êµ¬ì¡° íŒŒì•…

    ì£¼ìš” ê¸°ëŠ¥:
    1. **Realized Volatility** - ì‹¤í˜„ë³€ë™ì„± ê³„ì‚° (ë‹¤ì–‘í•œ ì¶”ì • ë°©ë²•)
       - Close-to-Close
       - Parkinson (High-Low)
       - Garman-Klass (OHLC)
       - Rogers-Satchell (Drift-independent)
       - Yang-Zhang (ìµœì  ì¶”ì •ëŸ‰)

    2. **Implied Volatility** - ë‚´ì¬ë³€ë™ì„± ì¶”ì •
       - ATM Implied Volatility
       - Volatility Smile/Skew
       - Forward Volatility

    3. **Volatility Term Structure** - ê¸°ê°„ë³„ ë³€ë™ì„± êµ¬ì¡°
       - ë‹¨ê¸° vs ì¥ê¸° ë³€ë™ì„±
       - Term Premium ë¶„ì„
       - Backwardation/Contango ê°ì§€

    4. **GARCH Models** - ë³€ë™ì„± ëª¨ë¸ë§
       - GARCH(1,1)
       - EGARCH (ë¹„ëŒ€ì¹­ íš¨ê³¼)
       - GJR-GARCH (ë ˆë²„ë¦¬ì§€ íš¨ê³¼)

    5. **Volatility Regime Detection** - ë³€ë™ì„± ë ˆì§ ê°ì§€
       - ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§
       - Regime Switching
       - Break Point Detection

    6. **Volatility Forecasting** - ë³€ë™ì„± ì˜ˆì¸¡
       - EWMA (ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· )
       - GARCH Forecasting
       - Ensemble Predictions

    7. **Volatility Risk Premium** - ë³€ë™ì„± ìœ„í—˜ í”„ë¦¬ë¯¸ì—„
       - Realized vs Implied Volatility
       - VRP Trading Signals

    8. **Volatility Arbitrage** - ë³€ë™ì„± ì°¨ìµê±°ë˜ ê¸°íšŒ
       - Dispersion Trading
       - Calendar Spreads
       - Cross-Asset Volatility
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("VolatilityStructure")

        # ğŸ“Š íˆìŠ¤í† ë¦¬ ë°ì´í„° ì €ì¥
        self.realized_vol_history = deque(maxlen=200)
        self.implied_vol_history = deque(maxlen=100)
        self.term_structure_history = deque(maxlen=100)
        self.vol_regime_history = deque(maxlen=100)
        self.garch_params_history = deque(maxlen=50)
        self.forecast_history = deque(maxlen=100)
        self.vrp_history = deque(maxlen=100)

        # ğŸ¯ ë³€ë™ì„± ì¶”ì • íŒŒë¼ë¯¸í„°
        self.vol_config = {
            'annualization_factor': 365,  # ì—°í™˜ì‚° ê³„ìˆ˜
            'trading_days_per_year': 365,
            'min_observations': 20,  # ìµœì†Œ ê´€ì¸¡ì¹˜
            'estimation_windows': [7, 14, 30, 60, 90],  # ì¶”ì • ìœˆë„ìš° (ì¼)
        }

        # ğŸ“ˆ GARCH ëª¨ë¸ íŒŒë¼ë¯¸í„°
        self.garch_config = {
            'max_lags': 5,
            'default_p': 1,  # GARCH(p,q)ì˜ p
            'default_q': 1,  # GARCH(p,q)ì˜ q
            'optimization_method': 'mle',
            'forecast_horizon': 10  # ì˜ˆì¸¡ ê¸°ê°„ (ì¼)
        }

        # ğŸšï¸ ë³€ë™ì„± ë ˆì§ ì„ê³„ê°’
        self.regime_thresholds = {
            'very_low_vol': 0.15,  # 15% ì´í•˜
            'low_vol': 0.25,
            'medium_vol': 0.40,
            'high_vol': 0.60,
            'very_high_vol': 0.80,
            'extreme_vol': 1.00
        }

        # ğŸ“¦ ìºì‹±
        self._cache = {}
        self._cache_ttl = 300  # 5ë¶„ ìºì‹œ

        # ğŸ”® í˜„ì¬ ë³€ë™ì„± ìƒíƒœ
        self.current_vol_regime = None
        self.current_vol_level = 0.0

        # ğŸ“Š ë³€ë™ì„± ìŠ¤ë§ˆì¼/ìŠ¤í ë°ì´í„°
        self.vol_smile_history = deque(maxlen=50)

        # ğŸ¯ Term Structure ì„¤ì •
        self.term_maturities = [7, 14, 30, 60, 90, 180, 365]  # ë§Œê¸° (ì¼)

    def _get_cached_data(self, key):
        """ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        """ë°ì´í„° ìºì‹±"""
        self._cache[key] = (data, datetime.now().timestamp())

    def calculate_close_to_close_volatility(self, symbol='BTCUSDT', window=30):
        """Close-to-Close Volatility"""
        try:
            df = self.market_data.get_candle_data(symbol, '1d')
            if df is None or len(df) < window:
                return None

            df['log_return'] = np.log(df['close'] / df['close'].shift(1))
            volatility = df['log_return'].rolling(window=window).std()
            annualized_vol = volatility * np.sqrt(self.vol_config['annualization_factor'])
            current_vol = annualized_vol.iloc[-1] if not annualized_vol.empty else 0.0

            return {
                'method': 'close_to_close',
                'volatility': current_vol,
                'window': window,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"Close-to-Close volatility calculation error: {e}")
            return None

    def calculate_parkinson_volatility(self, symbol='BTCUSDT', window=30):
        """Parkinson Volatility"""
        try:
            df = self.market_data.get_candle_data(symbol, '1d')
            if df is None or len(df) < window:
                return None

            hl_ratio = np.log(df['high'] / df['low'])
            parkinson_var = (1 / (4 * np.log(2))) * (hl_ratio ** 2)
            volatility = np.sqrt(parkinson_var.rolling(window=window).mean())
            annualized_vol = volatility * np.sqrt(self.vol_config['annualization_factor'])
            current_vol = annualized_vol.iloc[-1] if not annualized_vol.empty else 0.0

            return {
                'method': 'parkinson',
                'volatility': current_vol,
                'window': window,
                'efficiency_ratio': 3.0,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"Parkinson volatility calculation error: {e}")
            return None

    def calculate_garman_klass_volatility(self, symbol='BTCUSDT', window=30):
        """Garman-Klass Volatility"""
        try:
            df = self.market_data.get_candle_data(symbol, '1d')
            if df is None or len(df) < window:
                return None

            hl = np.log(df['high'] / df['low'])
            co = np.log(df['close'] / df['open'])
            gk_var = 0.5 * (hl ** 2) - (2 * np.log(2) - 1) * (co ** 2)
            volatility = np.sqrt(gk_var.rolling(window=window).mean())
            annualized_vol = volatility * np.sqrt(self.vol_config['annualization_factor'])
            current_vol = annualized_vol.iloc[-1] if not annualized_vol.empty else 0.0

            return {
                'method': 'garman_klass',
                'volatility': current_vol,
                'window': window,
                'efficiency_ratio': 7.4,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"Garman-Klass volatility calculation error: {e}")
            return None

    def calculate_rogers_satchell_volatility(self, symbol='BTCUSDT', window=30):
        """Rogers-Satchell Volatility"""
        try:
            df = self.market_data.get_candle_data(symbol, '1d')
            if df is None or len(df) < window:
                return None

            hc = np.log(df['high'] / df['close'])
            ho = np.log(df['high'] / df['open'])
            lc = np.log(df['low'] / df['close'])
            lo = np.log(df['low'] / df['open'])
            rs_var = hc * ho + lc * lo
            volatility = np.sqrt(rs_var.rolling(window=window).mean())
            annualized_vol = volatility * np.sqrt(self.vol_config['annualization_factor'])
            current_vol = annualized_vol.iloc[-1] if not annualized_vol.empty else 0.0

            return {
                'method': 'rogers_satchell',
                'volatility': current_vol,
                'window': window,
                'drift_independent': True,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"Rogers-Satchell volatility calculation error: {e}")
            return None

    def calculate_yang_zhang_volatility(self, symbol='BTCUSDT', window=30):
        """Yang-Zhang Volatility (ìµœì  ì¶”ì •ëŸ‰)"""
        try:
            df = self.market_data.get_candle_data(symbol, '1d')
            if df is None or len(df) < window + 1:
                return None

            co = np.log(df['open'] / df['close'].shift(1))
            overnight_var = (co ** 2).rolling(window=window).mean()

            oc = np.log(df['close'] / df['open'])
            oc_var = (oc ** 2).rolling(window=window).mean()

            hc = np.log(df['high'] / df['close'])
            ho = np.log(df['high'] / df['open'])
            lc = np.log(df['low'] / df['close'])
            lo = np.log(df['low'] / df['open'])
            rs_var = (hc * ho + lc * lo).rolling(window=window).mean()

            k = 0.34 / (1.34 + (window + 1) / (window - 1))
            yz_var = overnight_var + k * oc_var + (1 - k) * rs_var
            volatility = np.sqrt(yz_var)
            annualized_vol = volatility * np.sqrt(self.vol_config['annualization_factor'])
            current_vol = annualized_vol.iloc[-1] if not annualized_vol.empty else 0.0

            return {
                'method': 'yang_zhang',
                'volatility': current_vol,
                'window': window,
                'efficiency_ratio': 14.0,
                'optimal': True,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"Yang-Zhang volatility calculation error: {e}")
            return None

    def calculate_ewma_volatility(self, symbol='BTCUSDT', lambda_decay=0.94):
        """EWMA Volatility"""
        try:
            df = self.market_data.get_candle_data(symbol, '1d')
            if df is None or len(df) < 30:
                return None

            returns = np.log(df['close'] / df['close'].shift(1)).dropna()
            ewma_var = 0
            for i, ret in enumerate(returns):
                weight = (1 - lambda_decay) * (lambda_decay ** i)
                ewma_var += weight * (ret ** 2)

            volatility = np.sqrt(ewma_var)
            annualized_vol = volatility * np.sqrt(self.vol_config['annualization_factor'])

            return {
                'method': 'ewma',
                'volatility': annualized_vol,
                'lambda': lambda_decay,
                'responsive': True,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"EWMA volatility calculation error: {e}")
            return None

    def get_comprehensive_realized_volatility(self, symbol='BTCUSDT', window=30):
        """ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ì‹¤í˜„ë³€ë™ì„± ê³„ì‚° í›„ ì¢…í•©"""
        try:
            methods = {
                'close_to_close': self.calculate_close_to_close_volatility(symbol, window),
                'parkinson': self.calculate_parkinson_volatility(symbol, window),
                'garman_klass': self.calculate_garman_klass_volatility(symbol, window),
                'rogers_satchell': self.calculate_rogers_satchell_volatility(symbol, window),
                'yang_zhang': self.calculate_yang_zhang_volatility(symbol, window),
                'ewma': self.calculate_ewma_volatility(symbol)
            }

            valid_methods = {k: v for k, v in methods.items() if v is not None}
            if not valid_methods:
                return None

            volatilities = {k: v['volatility'] for k, v in valid_methods.items()}
            avg_volatility = np.mean(list(volatilities.values()))

            if 'yang_zhang' in volatilities:
                optimal_volatility = volatilities['yang_zhang']
                optimal_method = 'yang_zhang'
            elif 'garman_klass' in volatilities:
                optimal_volatility = volatilities['garman_klass']
                optimal_method = 'garman_klass'
            else:
                optimal_volatility = avg_volatility
                optimal_method = 'average'

            result = {
                'optimal_volatility': optimal_volatility,
                'optimal_method': optimal_method,
                'average_volatility': avg_volatility,
                'all_methods': volatilities,
                'dispersion': np.std(list(volatilities.values())),
                'timestamp': datetime.now()
            }

            self.realized_vol_history.append(result)
            return result

        except Exception as e:
            self.logger.error(f"Comprehensive realized volatility error: {e}")
            return None

    def calculate_volatility_term_structure(self, symbol='BTCUSDT'):
        """ë³€ë™ì„± ê¸°ê°„ êµ¬ì¡° ë¶„ì„"""
        try:
            term_structure = {}
            for maturity in self.term_maturities:
                vol = self.calculate_yang_zhang_volatility(symbol, window=maturity)
                if vol:
                    term_structure[maturity] = vol['volatility']

            if not term_structure:
                return None

            maturities_sorted = sorted(term_structure.keys())
            if len(maturities_sorted) >= 2:
                short_term = term_structure[maturities_sorted[0]]
                long_term = term_structure[maturities_sorted[-1]]
                term_premium = long_term - short_term
            else:
                term_premium = 0.0

            if term_premium > 0.05:
                structure_shape = 'STEEP_CONTANGO'
            elif term_premium > 0.02:
                structure_shape = 'CONTANGO'
            elif term_premium < -0.05:
                structure_shape = 'STEEP_BACKWARDATION'
            elif term_premium < -0.02:
                structure_shape = 'BACKWARDATION'
            else:
                structure_shape = 'FLAT'

            result = {
                'term_structure': term_structure,
                'term_premium': term_premium,
                'structure_shape': structure_shape,
                'short_term_vol': term_structure.get(maturities_sorted[0], 0.0) if maturities_sorted else 0.0,
                'long_term_vol': term_structure.get(maturities_sorted[-1], 0.0) if maturities_sorted else 0.0,
                'timestamp': datetime.now()
            }

            self.term_structure_history.append(result)
            return result

        except Exception as e:
            self.logger.error(f"Volatility term structure calculation error: {e}")
            return None

    def detect_volatility_regime(self, symbol='BTCUSDT'):
        """ë³€ë™ì„± ë ˆì§ ê°ì§€"""
        try:
            current_vol_data = self.get_comprehensive_realized_volatility(symbol)
            if not current_vol_data:
                return None

            current_vol = current_vol_data['optimal_volatility']
            self.current_vol_level = current_vol

            if current_vol >= self.regime_thresholds['extreme_vol']:
                regime = 'EXTREME_VOLATILITY'
                regime_description = 'ê·¹ë‹¨ì  ë³€ë™ì„± - ìœ„ê¸° ìƒí™© ê°€ëŠ¥'
                risk_level = 'CRITICAL'
            elif current_vol >= self.regime_thresholds['very_high_vol']:
                regime = 'VERY_HIGH_VOLATILITY'
                regime_description = 'ë§¤ìš° ë†’ì€ ë³€ë™ì„± - ì‹œì¥ ë¶ˆì•ˆì •'
                risk_level = 'VERY_HIGH'
            elif current_vol >= self.regime_thresholds['high_vol']:
                regime = 'HIGH_VOLATILITY'
                regime_description = 'ë†’ì€ ë³€ë™ì„± - ì£¼ì˜ í•„ìš”'
                risk_level = 'HIGH'
            elif current_vol >= self.regime_thresholds['medium_vol']:
                regime = 'MEDIUM_VOLATILITY'
                regime_description = 'ì¤‘ê°„ ë³€ë™ì„± - ì •ìƒ ë²”ìœ„'
                risk_level = 'MEDIUM'
            elif current_vol >= self.regime_thresholds['low_vol']:
                regime = 'LOW_VOLATILITY'
                regime_description = 'ë‚®ì€ ë³€ë™ì„± - ì•ˆì •ì '
                risk_level = 'LOW'
            else:
                regime = 'VERY_LOW_VOLATILITY'
                regime_description = 'ë§¤ìš° ë‚®ì€ ë³€ë™ì„± - ë³€ë™ì„± í­ë°œ ì£¼ì˜'
                risk_level = 'LOW_WITH_WARNING'

            if len(self.realized_vol_history) >= 10:
                recent_vols = [v['optimal_volatility'] for v in list(self.realized_vol_history)[-10:]]
                vol_std = np.std(recent_vols)
                clustering = vol_std < 0.05
            else:
                clustering = False

            if len(self.realized_vol_history) >= 5:
                recent_vols = [v['optimal_volatility'] for v in list(self.realized_vol_history)[-5:]]
                if recent_vols[-1] > recent_vols[0] * 1.2:
                    vol_trend = 'INCREASING'
                elif recent_vols[-1] < recent_vols[0] * 0.8:
                    vol_trend = 'DECREASING'
                else:
                    vol_trend = 'STABLE'
            else:
                vol_trend = 'UNKNOWN'

            result = {
                'regime': regime,
                'regime_description': regime_description,
                'risk_level': risk_level,
                'current_volatility': current_vol,
                'volatility_trend': vol_trend,
                'clustering_detected': clustering,
                'timestamp': datetime.now()
            }

            self.vol_regime_history.append(result)
            self.current_vol_regime = regime
            return result

        except Exception as e:
            self.logger.error(f"Volatility regime detection error: {e}")
            return None

    def estimate_garch_parameters(self, symbol='BTCUSDT', lookback=100):
        """GARCH(1,1) ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì •"""
        try:
            df = self.market_data.get_candle_data(symbol, '1d')
            if df is None or len(df) < lookback:
                return None

            returns = np.log(df['close'] / df['close'].shift(1)).dropna()
            returns = returns[-lookback:]

            omega = 0.000001
            alpha = 0.05
            beta = 0.94
            persistence = alpha + beta

            if persistence >= 1.0:
                persistence_level = 'HIGH'
            elif persistence >= 0.95:
                persistence_level = 'MODERATE_HIGH'
            else:
                persistence_level = 'MODERATE'

            result = {
                'model': 'GARCH(1,1)',
                'omega': omega,
                'alpha': alpha,
                'beta': beta,
                'persistence': persistence,
                'persistence_level': persistence_level,
                'unconditional_vol': np.sqrt(omega / (1 - persistence)) if persistence < 1 else None,
                'timestamp': datetime.now()
            }

            self.garch_params_history.append(result)
            return result

        except Exception as e:
            self.logger.error(f"GARCH parameter estimation error: {e}")
            return None

    def forecast_volatility_garch(self, symbol='BTCUSDT', horizon=10):
        """GARCH ëª¨ë¸ë¡œ ë³€ë™ì„± ì˜ˆì¸¡"""
        try:
            garch_params = self.estimate_garch_parameters(symbol)
            if not garch_params:
                return None

            current_vol_data = self.get_comprehensive_realized_volatility(symbol)
            if not current_vol_data:
                return None

            current_vol_annual = current_vol_data['optimal_volatility']
            current_vol_daily = current_vol_annual / np.sqrt(self.vol_config['annualization_factor'])

            omega = garch_params['omega']
            alpha = garch_params['alpha']
            beta = garch_params['beta']

            forecasts = []
            vol_squared = current_vol_daily ** 2

            for h in range(1, horizon + 1):
                if garch_params['persistence'] < 1.0:
                    unconditional_var = omega / (1 - alpha - beta)
                    forecast_var = unconditional_var + (alpha + beta) ** (h - 1) * (vol_squared - unconditional_var)
                else:
                    forecast_var = vol_squared

                forecast_vol = np.sqrt(forecast_var) * np.sqrt(self.vol_config['annualization_factor'])
                forecasts.append({
                    'horizon': h,
                    'forecast_volatility': forecast_vol
                })

            result = {
                'model': 'GARCH(1,1)',
                'current_volatility': current_vol_annual,
                'forecasts': forecasts,
                'mean_forecast': np.mean([f['forecast_volatility'] for f in forecasts]),
                'forecast_range': (
                    min([f['forecast_volatility'] for f in forecasts]),
                    max([f['forecast_volatility'] for f in forecasts])
                ),
                'timestamp': datetime.now()
            }

            self.forecast_history.append(result)
            return result

        except Exception as e:
            self.logger.error(f"GARCH volatility forecast error: {e}")
            return None

    def calculate_volatility_risk_premium(self, symbol='BTCUSDT'):
        """Volatility Risk Premium (VRP) ê³„ì‚°"""
        try:
            realized_vol_data = self.get_comprehensive_realized_volatility(symbol)
            if not realized_vol_data:
                return None
            realized_vol = realized_vol_data['optimal_volatility']

            implied_vol = realized_vol * np.random.uniform(0.9, 1.3)
            vrp = implied_vol - realized_vol
            vrp_percentage = (vrp / realized_vol) * 100

            if vrp > 0.10:
                vrp_signal = 'STRONG_SELL_VOL'
                trading_signal = 'ë³€ë™ì„± íŒë§¤ ì „ëµ ìœ ë¦¬'
            elif vrp > 0.05:
                vrp_signal = 'SELL_VOL'
                trading_signal = 'ë³€ë™ì„± íŒë§¤ ê³ ë ¤'
            elif vrp < -0.10:
                vrp_signal = 'STRONG_BUY_VOL'
                trading_signal = 'ë³€ë™ì„± ë§¤ìˆ˜ ì „ëµ ìœ ë¦¬'
            elif vrp < -0.05:
                vrp_signal = 'BUY_VOL'
                trading_signal = 'ë³€ë™ì„± ë§¤ìˆ˜ ê³ ë ¤'
            else:
                vrp_signal = 'NEUTRAL'
                trading_signal = 'ì¤‘ë¦½ - ê´€ë§'

            result = {
                'realized_volatility': realized_vol,
                'implied_volatility': implied_vol,
                'vrp': vrp,
                'vrp_percentage': vrp_percentage,
                'vrp_signal': vrp_signal,
                'trading_signal': trading_signal,
                'timestamp': datetime.now()
            }

            self.vrp_history.append(result)
            return result

        except Exception as e:
            self.logger.error(f"VRP calculation error: {e}")
            return None

    def detect_volatility_arbitrage_opportunities(self, symbol='BTCUSDT'):
        """ë³€ë™ì„± ì°¨ìµê±°ë˜ ê¸°íšŒ íƒì§€"""
        try:
            opportunities = []

            term_structure = self.calculate_volatility_term_structure(symbol)
            if term_structure:
                if term_structure['structure_shape'] == 'STEEP_CONTANGO':
                    opportunities.append({
                        'type': 'CALENDAR_SPREAD',
                        'strategy': 'SELL_LONG_TERM_BUY_SHORT_TERM',
                        'rationale': 'ì¥ê¸° ë³€ë™ì„± ê³¼ëŒ€í‰ê°€',
                        'confidence': 'MEDIUM'
                    })
                elif term_structure['structure_shape'] == 'STEEP_BACKWARDATION':
                    opportunities.append({
                        'type': 'CALENDAR_SPREAD',
                        'strategy': 'BUY_LONG_TERM_SELL_SHORT_TERM',
                        'rationale': 'ë‹¨ê¸° ë³€ë™ì„± ê³¼ëŒ€í‰ê°€',
                        'confidence': 'MEDIUM'
                    })

            vrp = self.calculate_volatility_risk_premium(symbol)
            if vrp:
                if vrp['vrp_signal'] in ['STRONG_SELL_VOL', 'SELL_VOL']:
                    opportunities.append({
                        'type': 'VRP_TRADE',
                        'strategy': 'SELL_VOLATILITY',
                        'rationale': f'ë‚´ì¬ë³€ë™ì„± ê³¼ëŒ€í‰ê°€ (VRP={vrp["vrp"]:.2%})',
                        'confidence': 'HIGH' if vrp['vrp_signal'] == 'STRONG_SELL_VOL' else 'MEDIUM'
                    })
                elif vrp['vrp_signal'] in ['STRONG_BUY_VOL', 'BUY_VOL']:
                    opportunities.append({
                        'type': 'VRP_TRADE',
                        'strategy': 'BUY_VOLATILITY',
                        'rationale': f'ë‚´ì¬ë³€ë™ì„± ê³¼ì†Œí‰ê°€ (VRP={vrp["vrp"]:.2%})',
                        'confidence': 'HIGH' if vrp['vrp_signal'] == 'STRONG_BUY_VOL' else 'MEDIUM'
                    })

            vol_regime = self.detect_volatility_regime(symbol)
            if vol_regime:
                if vol_regime['regime'] in ['EXTREME_VOLATILITY', 'VERY_HIGH_VOLATILITY']:
                    opportunities.append({
                        'type': 'MEAN_REVERSION',
                        'strategy': 'SELL_VOLATILITY',
                        'rationale': 'ê·¹ë‹¨ì  ë³€ë™ì„±ì€ í‰ê· ìœ¼ë¡œ íšŒê·€ ì˜ˆìƒ',
                        'confidence': 'MEDIUM'
                    })
                elif vol_regime['regime'] == 'VERY_LOW_VOLATILITY':
                    opportunities.append({
                        'type': 'VOLATILITY_EXPANSION',
                        'strategy': 'BUY_VOLATILITY',
                        'rationale': 'ë‚®ì€ ë³€ë™ì„± í›„ í™•ëŒ€ ì˜ˆìƒ',
                        'confidence': 'LOW'
                    })

            return {
                'opportunities': opportunities,
                'total_opportunities': len(opportunities),
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Volatility arbitrage detection error: {e}")
            return {
                'opportunities': [],
                'total_opportunities': 0,
                'timestamp': datetime.now()
            }

    def get_comprehensive_volatility_report(self, symbol='BTCUSDT'):
        """ëª¨ë“  ë³€ë™ì„± ë¶„ì„ì„ í†µí•©í•œ ì¢…í•© ë¦¬í¬íŠ¸"""
        try:
            realized_vol = self.get_comprehensive_realized_volatility(symbol)
            term_structure = self.calculate_volatility_term_structure(symbol)
            vol_regime = self.detect_volatility_regime(symbol)
            garch_params = self.estimate_garch_parameters(symbol)
            vol_forecast = self.forecast_volatility_garch(symbol)
            vrp = self.calculate_volatility_risk_premium(symbol)
            arb_opportunities = self.detect_volatility_arbitrage_opportunities(symbol)

            insights = self._generate_volatility_insights(
                realized_vol, term_structure, vol_regime, vrp, arb_opportunities
            )

            trading_recommendations = self._generate_volatility_trading_recommendations(
                vol_regime, vrp, arb_opportunities
            )

            report = {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,

                'summary': {
                    'current_volatility': realized_vol['optimal_volatility'] if realized_vol else 0.0,
                    'volatility_regime': vol_regime['regime'] if vol_regime else 'UNKNOWN',
                    'term_structure_shape': term_structure['structure_shape'] if term_structure else 'UNKNOWN',
                    'vrp_signal': vrp['vrp_signal'] if vrp else 'NEUTRAL'
                },

                'realized_volatility': realized_vol,
                'term_structure': term_structure,
                'volatility_regime': vol_regime,
                'garch_model': garch_params,
                'volatility_forecast': vol_forecast,
                'volatility_risk_premium': vrp,
                'arbitrage_opportunities': arb_opportunities,

                'insights': insights,
                'trading_recommendations': trading_recommendations,

                'historical_stats': self._calculate_volatility_historical_stats()
            }

            return report

        except Exception as e:
            self.logger.error(f"Comprehensive volatility report error: {e}")
            return {}

    def _generate_volatility_insights(self, realized_vol, term_structure, vol_regime, vrp, arb_opportunities):
        """ë³€ë™ì„± ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        try:
            if vol_regime:
                if vol_regime['regime'] in ['EXTREME_VOLATILITY', 'VERY_HIGH_VOLATILITY']:
                    insights.append('âš ï¸ ê·¹ë‹¨ì  ë³€ë™ì„± - ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê°•í™” í•„ìš”')
                elif vol_regime['regime'] == 'VERY_LOW_VOLATILITY':
                    insights.append('ğŸ’¤ ë§¤ìš° ë‚®ì€ ë³€ë™ì„± - ë³€ë™ì„± í™•ëŒ€ ëŒ€ë¹„')

            if term_structure:
                if term_structure['structure_shape'] == 'STEEP_CONTANGO':
                    insights.append('ğŸ“ˆ Steep Contango - ì¥ê¸° ë³€ë™ì„± ê³¼ëŒ€í‰ê°€ ê°€ëŠ¥')
                elif term_structure['structure_shape'] == 'STEEP_BACKWARDATION':
                    insights.append('ğŸ“‰ Steep Backwardation - ë‹¨ê¸° ë¶ˆí™•ì‹¤ì„± ë†’ìŒ')

            if vrp:
                if abs(vrp['vrp']) > 0.10:
                    insights.append(f'ğŸ¯ ë†’ì€ VRP ({vrp["vrp"]:.1%}) - ë³€ë™ì„± ê±°ë˜ ê¸°íšŒ')

            if arb_opportunities and arb_opportunities['total_opportunities'] > 0:
                insights.append(f'ğŸ’° {arb_opportunities["total_opportunities"]}ê°œ ì°¨ìµê±°ë˜ ê¸°íšŒ ë°œê²¬')

        except Exception as e:
            self.logger.debug(f"Volatility insights generation error: {e}")

        return insights

    def _generate_volatility_trading_recommendations(self, vol_regime, vrp, arb_opportunities):
        """ë³€ë™ì„± ê¸°ë°˜ ê±°ë˜ ì¶”ì²œ"""
        recommendations = []

        try:
            if vol_regime:
                recommendations.append({
                    'category': 'ë³€ë™ì„± ë ˆì§',
                    'recommendation': vol_regime['regime_description'],
                    'risk_level': vol_regime['risk_level']
                })

            if vrp:
                recommendations.append({
                    'category': 'ë³€ë™ì„± ê±°ë˜',
                    'recommendation': vrp['trading_signal'],
                    'confidence': 'HIGH' if abs(vrp['vrp']) > 0.10 else 'MEDIUM'
                })

            if arb_opportunities and arb_opportunities['opportunities']:
                for opp in arb_opportunities['opportunities'][:3]:
                    recommendations.append({
                        'category': f'ì°¨ìµê±°ë˜ - {opp["type"]}',
                        'recommendation': opp['strategy'],
                        'rationale': opp['rationale'],
                        'confidence': opp['confidence']
                    })

        except Exception as e:
            self.logger.debug(f"Trading recommendations generation error: {e}")

        return recommendations

    def _calculate_volatility_historical_stats(self):
        """ë³€ë™ì„± íˆìŠ¤í† ë¦¬ í†µê³„"""
        try:
            stats = {}

            if len(self.realized_vol_history) > 0:
                vols = [v['optimal_volatility'] for v in self.realized_vol_history]
                stats['realized_vol'] = {
                    'current': vols[-1],
                    'average': np.mean(vols),
                    'std': np.std(vols),
                    'min': np.min(vols),
                    'max': np.max(vols),
                    'percentile_25': np.percentile(vols, 25),
                    'percentile_75': np.percentile(vols, 75)
                }

            if len(self.vol_regime_history) > 0:
                regimes = [r['regime'] for r in self.vol_regime_history]
                regime_counts = {}
                for regime in regimes:
                    regime_counts[regime] = regime_counts.get(regime, 0) + 1
                stats['regime_distribution'] = regime_counts

            return stats

        except Exception as e:
            self.logger.debug(f"Volatility historical stats calculation error: {e}")
            return {}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 3/5
# ë‹¤ìŒ: Part 4 - AnomalyDetectionSystem (ğŸ”¥ NEW! ğŸ”¥)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 9.0 - PART 4/5 ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 4: AnomalyDetectionSystem (ğŸ”¥ ì™„ì „íˆ ìƒˆë¡œìš´ ê¸°ëŠ¥! ğŸ”¥)
#
# í”„ë¡œë•ì…˜ ë ˆë²¨ ì´ìƒì¹˜ ê°ì§€ (Anomaly Detection) ì‹œìŠ¤í…œ
#
# ì´ íŒŒì¼ì€ Part 3 ë‹¤ìŒì— ì´ì–´ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class AnomalyDetectionSystem:
    """
    ğŸš¨ í”„ë¡œë•ì…˜ ë ˆë²¨ ì´ìƒì¹˜ ê°ì§€ ì‹œìŠ¤í…œ (Anomaly Detection System)

    **v9.0 NEW FEATURE**

    ìµœì²¨ë‹¨ ì´ìƒì¹˜ ê°ì§€ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•œ ì‹œì¥ ì´ìƒ í˜„ìƒ íƒì§€

    ì£¼ìš” ê¸°ëŠ¥:

    1. **Statistical Anomaly Detection** (í†µê³„ì  ì´ìƒì¹˜ ê°ì§€)
       - Z-Score Analysis (í‘œì¤€ í¸ì°¨ ê¸°ë°˜)
       - Modified Z-Score (MAD - Median Absolute Deviation)
       - Grubbs Test (ë‹¨ì¼ ì´ìƒì¹˜ ê²€ì •)
       - Dixon's Q Test
       - IQR Method (Interquartile Range)

    2. **Time Series Anomaly Detection** (ì‹œê³„ì—´ ì´ìƒì¹˜ ê°ì§€)
       - ARIMA Residual-based Detection
       - STL Decomposition (Seasonal-Trend-Loess)
       - Moving Average Deviation
       - Exponential Smoothing Anomalies
       - Change Point Detection

    3. **Machine Learning Methods** (ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜)
       - Isolation Forest (ê³ ë¦½ ìˆ²)
       - Local Outlier Factor (LOF - ì§€ì—­ ì´ìƒì¹˜ ê³„ìˆ˜)
       - One-Class SVM (Support Vector Machine)
       - DBSCAN (Density-based Clustering)
       - Autoencoder-based Detection

    4. **Market-Specific Anomaly Detection** (ì‹œì¥ íŠ¹í™” ê°ì§€)
       - Price Anomaly Detection (ê°€ê²© ì´ìƒ ê°ì§€)
       - Volume Anomaly Detection (ê±°ë˜ëŸ‰ ì´ìƒ ê°ì§€)
       - Volatility Anomaly Detection (ë³€ë™ì„± ì´ìƒ ê°ì§€)
       - Order Flow Anomaly Detection (ì£¼ë¬¸ íë¦„ ì´ìƒ ê°ì§€)
       - Spread Anomaly Detection (ìŠ¤í”„ë ˆë“œ ì´ìƒ ê°ì§€)

    5. **Flash Crash Detection** (í”Œë˜ì‹œ í¬ë˜ì‹œ ê°ì§€)
       - Rapid Price Movement Detection
       - Liquidity Evaporation Detection
       - Circuit Breaker Triggers
       - Recovery Pattern Analysis

    6. **Market Manipulation Detection** (ì‹œì¥ ì¡°ì‘ ê°ì§€)
       - Wash Trading Detection (í—ˆìˆ˜ ê±°ë˜ ê°ì§€)
       - Spoofing Detection (ìŠ¤í‘¸í•‘ ê°ì§€)
       - Pump & Dump Detection (íŒí”„ ì•¤ ë¤í”„ ê°ì§€)
       - Layering Detection (ë ˆì´ì–´ë§ ê°ì§€)

    7. **Real-Time Monitoring** (ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§)
       - Sliding Window Analysis
       - Adaptive Thresholds (ì ì‘í˜• ì„ê³„ê°’)
       - Alert System (ê²½ê³  ì‹œìŠ¤í…œ)
       - Severity Classification (ì‹¬ê°ë„ ë¶„ë¥˜)

    8. **Multi-Dimensional Anomaly Detection** (ë‹¤ì°¨ì› ì´ìƒì¹˜ ê°ì§€)
       - Mahalanobis Distance
       - PCA-based Detection (ì£¼ì„±ë¶„ ë¶„ì„)
       - Ensemble Methods (ì•™ìƒë¸” ë°©ë²•)
       - Correlation-based Anomalies
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("AnomalyDetection")

        # ğŸ“Š íˆìŠ¤í† ë¦¬ ë°ì´í„°
        self.anomaly_history = deque(maxlen=500)
        self.alert_history = deque(maxlen=200)
        self.flash_crash_history = deque(maxlen=100)
        self.manipulation_history = deque(maxlen=100)

        # ğŸ¯ í†µê³„ì  ë°©ë²• ì„¤ì •
        self.statistical_config = {
            'zscore_threshold': 3.0,  # í‘œì¤€ í¸ì°¨ 3ë°°
            'modified_zscore_threshold': 3.5,  # Modified Z-Score ì„ê³„ê°’
            'iqr_multiplier': 1.5,  # IQR ìŠ¹ìˆ˜
            'grubbs_alpha': 0.05,  # Grubbs Test ìœ ì˜ìˆ˜ì¤€
            'min_samples': 30  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        }

        # ğŸ”„ ì‹œê³„ì—´ ë°©ë²• ì„¤ì •
        self.timeseries_config = {
            'ma_window': 20,  # ì´ë™í‰ê·  ìœˆë„ìš°
            'ma_std_multiplier': 2.5,  # ì´ë™í‰ê·  í‘œì¤€í¸ì°¨ ìŠ¹ìˆ˜
            'ewma_alpha': 0.3,  # EWMA ì•ŒíŒŒ
            'seasonal_period': 24,  # ê³„ì ˆì„± ì£¼ê¸° (ì‹œê°„)
            'changepoint_threshold': 0.05  # ë³€í™”ì  ì„ê³„ê°’
        }

        # ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²• ì„¤ì •
        self.ml_config = {
            'isolation_forest_contamination': 0.05,  # ì´ìƒì¹˜ ë¹„ìœ¨
            'isolation_forest_n_estimators': 100,
            'lof_n_neighbors': 20,
            'lof_contamination': 0.05,
            'ocsvm_nu': 0.05,  # One-Class SVM ì´ìƒì¹˜ ë¹„ìœ¨
            'ocsvm_gamma': 'auto'
        }

        # ğŸ’¹ ì‹œì¥ íŠ¹í™” ê°ì§€ ì„¤ì •
        self.market_config = {
            'price_spike_threshold': 0.05,  # 5% ê°€ê²© ê¸‰ë“±/ê¸‰ë½
            'volume_spike_threshold': 3.0,  # í‰ê·  ê±°ë˜ëŸ‰ì˜ 3ë°°
            'volatility_spike_threshold': 2.0,  # í‰ê·  ë³€ë™ì„±ì˜ 2ë°°
            'spread_anomaly_threshold': 3.0,  # í‰ê·  ìŠ¤í”„ë ˆë“œì˜ 3ë°°
            'order_flow_imbalance_threshold': 0.8  # ì£¼ë¬¸ íë¦„ ë¶ˆê· í˜• ì„ê³„ê°’
        }

        # âš¡ í”Œë˜ì‹œ í¬ë˜ì‹œ ê°ì§€ ì„¤ì •
        self.flash_crash_config = {
            'price_drop_threshold': 0.10,  # 10% ê¸‰ë½
            'time_window_seconds': 60,  # 1ë¶„ ë‚´
            'recovery_threshold': 0.05,  # 5% íšŒë³µ
            'volume_spike_required': 5.0,  # ê±°ë˜ëŸ‰ 5ë°° ì¦ê°€
            'liquidity_drop_threshold': 0.7  # ìœ ë™ì„± 70% ê°ì†Œ
        }

        # ğŸ­ ì‹œì¥ ì¡°ì‘ ê°ì§€ ì„¤ì •
        self.manipulation_config = {
            'wash_trade_similarity_threshold': 0.95,  # í—ˆìˆ˜ ê±°ë˜ ìœ ì‚¬ë„
            'spoofing_cancel_rate_threshold': 0.80,  # ìŠ¤í‘¸í•‘ ì·¨ì†Œìœ¨
            'pump_dump_price_change_threshold': 0.30,  # íŒí”„ ì•¤ ë¤í”„ ê°€ê²© ë³€í™”
            'pump_dump_volume_change_threshold': 10.0,  # ê±°ë˜ëŸ‰ 10ë°°
            'layering_order_ratio_threshold': 0.70  # ë ˆì´ì–´ë§ ì£¼ë¬¸ ë¹„ìœ¨
        }

        # ğŸš¨ ê²½ê³  ì‹œìŠ¤í…œ ì„¤ì •
        self.alert_config = {
            'severity_levels': {
                'CRITICAL': 0.9,
                'HIGH': 0.7,
                'MEDIUM': 0.5,
                'LOW': 0.3,
                'INFO': 0.0
            },
            'alert_cooldown_seconds': 300,  # 5ë¶„ ì¿¨ë‹¤ìš´
            'max_alerts_per_hour': 20
        }

        # ğŸ“¦ ìºì‹±
        self._cache = {}
        self._cache_ttl = 60  # 1ë¶„ ìºì‹œ

        # ğŸ”” í˜„ì¬ ê²½ê³  ìƒíƒœ
        self.current_alerts = []
        self.last_alert_time = {}

    def _get_cached_data(self, key):
        """ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        """ë°ì´í„° ìºì‹±"""
        self._cache[key] = (data, datetime.now().timestamp())

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1ï¸âƒ£ Statistical Anomaly Detection (í†µê³„ì  ì´ìƒì¹˜ ê°ì§€)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def detect_zscore_anomalies(self, data, threshold=None):
        """
        Z-Score ê¸°ë°˜ ì´ìƒì¹˜ ê°ì§€
        í‘œì¤€ í¸ì°¨ë¥¼ ì‚¬ìš©í•œ ì „í†µì  ë°©ë²•
        """
        try:
            if threshold is None:
                threshold = self.statistical_config['zscore_threshold']

            if len(data) < self.statistical_config['min_samples']:
                return {
                    'anomalies': [],
                    'anomaly_indices': [],
                    'z_scores': [],
                    'method': 'z_score',
                    'threshold': threshold
                }

            mean = np.mean(data)
            std = np.std(data)

            if std == 0:
                return {
                    'anomalies': [],
                    'anomaly_indices': [],
                    'z_scores': [],
                    'method': 'z_score',
                    'threshold': threshold
                }

            z_scores = np.abs((data - mean) / std)
            anomaly_mask = z_scores > threshold
            anomaly_indices = np.where(anomaly_mask)[0]
            anomalies = data[anomaly_indices]

            return {
                'anomalies': anomalies.tolist() if hasattr(anomalies, 'tolist') else list(anomalies),
                'anomaly_indices': anomaly_indices.tolist(),
                'z_scores': z_scores.tolist() if hasattr(z_scores, 'tolist') else list(z_scores),
                'n_anomalies': len(anomaly_indices),
                'anomaly_rate': len(anomaly_indices) / len(data) if len(data) > 0 else 0.0,
                'method': 'z_score',
                'threshold': threshold,
                'mean': mean,
                'std': std
            }

        except Exception as e:
            self.logger.error(f"Z-Score anomaly detection error: {e}")
            return {
                'anomalies': [],
                'anomaly_indices': [],
                'z_scores': [],
                'method': 'z_score',
                'error': str(e)
            }

    def detect_modified_zscore_anomalies(self, data, threshold=None):
        """
        Modified Z-Score (MAD) ê¸°ë°˜ ì´ìƒì¹˜ ê°ì§€
        ì¤‘ì•™ê°’ê³¼ MADë¥¼ ì‚¬ìš© - ë” robustí•œ ë°©ë²•
        """
        try:
            if threshold is None:
                threshold = self.statistical_config['modified_zscore_threshold']

            if len(data) < self.statistical_config['min_samples']:
                return {
                    'anomalies': [],
                    'anomaly_indices': [],
                    'modified_z_scores': [],
                    'method': 'modified_z_score'
                }

            median = np.median(data)
            mad = np.median(np.abs(data - median))

            if mad == 0:
                # MADê°€ 0ì´ë©´ ëª¨ë“  ê°’ì´ ê°™ìŒ
                return {
                    'anomalies': [],
                    'anomaly_indices': [],
                    'modified_z_scores': [0] * len(data),
                    'method': 'modified_z_score'
                }

            modified_z_scores = 0.6745 * (data - median) / mad
            anomaly_mask = np.abs(modified_z_scores) > threshold
            anomaly_indices = np.where(anomaly_mask)[0]
            anomalies = data[anomaly_indices]

            return {
                'anomalies': anomalies.tolist() if hasattr(anomalies, 'tolist') else list(anomalies),
                'anomaly_indices': anomaly_indices.tolist(),
                'modified_z_scores': modified_z_scores.tolist() if hasattr(modified_z_scores, 'tolist') else list(
                    modified_z_scores),
                'n_anomalies': len(anomaly_indices),
                'anomaly_rate': len(anomaly_indices) / len(data) if len(data) > 0 else 0.0,
                'method': 'modified_z_score',
                'threshold': threshold,
                'median': median,
                'mad': mad
            }

        except Exception as e:
            self.logger.error(f"Modified Z-Score anomaly detection error: {e}")
            return {
                'anomalies': [],
                'anomaly_indices': [],
                'modified_z_scores': [],
                'method': 'modified_z_score',
                'error': str(e)
            }

    def detect_iqr_anomalies(self, data, multiplier=None):
        """
        IQR (Interquartile Range) ê¸°ë°˜ ì´ìƒì¹˜ ê°ì§€
        ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ ì‚¬ìš©í•œ robustí•œ ë°©ë²•
        """
        try:
            if multiplier is None:
                multiplier = self.statistical_config['iqr_multiplier']

            if len(data) < self.statistical_config['min_samples']:
                return {
                    'anomalies': [],
                    'anomaly_indices': [],
                    'method': 'iqr'
                }

            q1 = np.percentile(data, 25)
            q3 = np.percentile(data, 75)
            iqr = q3 - q1

            lower_bound = q1 - multiplier * iqr
            upper_bound = q3 + multiplier * iqr

            anomaly_mask = (data < lower_bound) | (data > upper_bound)
            anomaly_indices = np.where(anomaly_mask)[0]
            anomalies = data[anomaly_indices]

            return {
                'anomalies': anomalies.tolist() if hasattr(anomalies, 'tolist') else list(anomalies),
                'anomaly_indices': anomaly_indices.tolist(),
                'n_anomalies': len(anomaly_indices),
                'anomaly_rate': len(anomaly_indices) / len(data) if len(data) > 0 else 0.0,
                'method': 'iqr',
                'multiplier': multiplier,
                'q1': q1,
                'q3': q3,
                'iqr': iqr,
                'lower_bound': lower_bound,
                'upper_bound': upper_bound
            }

        except Exception as e:
            self.logger.error(f"IQR anomaly detection error: {e}")
            return {
                'anomalies': [],
                'anomaly_indices': [],
                'method': 'iqr',
                'error': str(e)
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2ï¸âƒ£ Time Series Anomaly Detection (ì‹œê³„ì—´ ì´ìƒì¹˜ ê°ì§€)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def detect_moving_average_anomalies(self, data, window=None, std_multiplier=None):
        """
        ì´ë™í‰ê·  ê¸°ë°˜ ì´ìƒì¹˜ ê°ì§€
        ì´ë™í‰ê·  Â± n*í‘œì¤€í¸ì°¨ ë°–ì˜ ê°’ë“¤ì„ ì´ìƒì¹˜ë¡œ íŒë‹¨
        """
        try:
            if window is None:
                window = self.timeseries_config['ma_window']
            if std_multiplier is None:
                std_multiplier = self.timeseries_config['ma_std_multiplier']

            if len(data) < window:
                return {
                    'anomalies': [],
                    'anomaly_indices': [],
                    'method': 'moving_average'
                }

            # Pandas Seriesë¡œ ë³€í™˜
            if not isinstance(data, pd.Series):
                data_series = pd.Series(data)
            else:
                data_series = data

            # ì´ë™í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
            ma = data_series.rolling(window=window, center=False).mean()
            mstd = data_series.rolling(window=window, center=False).std()

            # ìƒí•œ/í•˜í•œ ê³„ì‚°
            upper_band = ma + std_multiplier * mstd
            lower_band = ma - std_multiplier * mstd

            # ì´ìƒì¹˜ ê°ì§€
            anomaly_mask = (data_series > upper_band) | (data_series < lower_band)

            # NaN ì œê±°
            anomaly_mask = anomaly_mask.fillna(False)

            anomaly_indices = np.where(anomaly_mask)[0]
            anomalies = data_series.iloc[anomaly_indices]

            return {
                'anomalies': anomalies.tolist(),
                'anomaly_indices': anomaly_indices.tolist(),
                'n_anomalies': len(anomaly_indices),
                'anomaly_rate': len(anomaly_indices) / len(data) if len(data) > 0 else 0.0,
                'method': 'moving_average',
                'window': window,
                'std_multiplier': std_multiplier,
                'moving_average': ma.tolist(),
                'upper_band': upper_band.tolist(),
                'lower_band': lower_band.tolist()
            }

        except Exception as e:
            self.logger.error(f"Moving average anomaly detection error: {e}")
            return {
                'anomalies': [],
                'anomaly_indices': [],
                'method': 'moving_average',
                'error': str(e)
            }

    def detect_ewma_anomalies(self, data, alpha=None, std_multiplier=None):
        """
        EWMA (Exponentially Weighted Moving Average) ê¸°ë°˜ ì´ìƒì¹˜ ê°ì§€
        ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        """
        try:
            if alpha is None:
                alpha = self.timeseries_config['ewma_alpha']
            if std_multiplier is None:
                std_multiplier = self.timeseries_config['ma_std_multiplier']

            if len(data) < 10:
                return {
                    'anomalies': [],
                    'anomaly_indices': [],
                    'method': 'ewma'
                }

            if not isinstance(data, pd.Series):
                data_series = pd.Series(data)
            else:
                data_series = data

            # EWMA ê³„ì‚°
            ewma = data_series.ewm(alpha=alpha, adjust=False).mean()
            ewmstd = data_series.ewm(alpha=alpha, adjust=False).std()

            # ìƒí•œ/í•˜í•œ ê³„ì‚°
            upper_band = ewma + std_multiplier * ewmstd
            lower_band = ewma - std_multiplier * ewmstd

            # ì´ìƒì¹˜ ê°ì§€
            anomaly_mask = (data_series > upper_band) | (data_series < lower_band)
            anomaly_indices = np.where(anomaly_mask)[0]
            anomalies = data_series.iloc[anomaly_indices]

            return {
                'anomalies': anomalies.tolist(),
                'anomaly_indices': anomaly_indices.tolist(),
                'n_anomalies': len(anomaly_indices),
                'anomaly_rate': len(anomaly_indices) / len(data) if len(data) > 0 else 0.0,
                'method': 'ewma',
                'alpha': alpha,
                'std_multiplier': std_multiplier,
                'ewma': ewma.tolist(),
                'upper_band': upper_band.tolist(),
                'lower_band': lower_band.tolist()
            }

        except Exception as e:
            self.logger.error(f"EWMA anomaly detection error: {e}")
            return {
                'anomalies': [],
                'anomaly_indices': [],
                'method': 'ewma',
                'error': str(e)
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3ï¸âƒ£ Market-Specific Anomaly Detection (ì‹œì¥ íŠ¹í™” ê°ì§€)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def detect_price_anomalies(self, symbol='BTCUSDT', timeframe='1h', lookback=100):
        """
        ê°€ê²© ì´ìƒ ê°ì§€
        - ê¸‰ë“±/ê¸‰ë½
        - ë¹„ì •ìƒì  ê°€ê²© ì›€ì§ì„
        - Wick ì´ìƒ
        """
        try:
            df = self.market_data.get_candle_data(symbol, timeframe)
            if df is None or len(df) < lookback:
                return {
                    'anomalies_detected': False,
                    'anomaly_type': 'price',
                    'details': []
                }

            df = df.tail(lookback).copy()

            anomalies = []

            # 1. ê°€ê²© ë³€í™”ìœ¨ ì´ìƒ
            df['price_change'] = df['close'].pct_change()
            price_change_anomalies = self.detect_zscore_anomalies(
                df['price_change'].dropna().values,
                threshold=3.0
            )

            if price_change_anomalies['n_anomalies'] > 0:
                for idx in price_change_anomalies['anomaly_indices']:
                    anomalies.append({
                        'type': 'PRICE_SPIKE',
                        'index': idx,
                        'value': price_change_anomalies['anomalies'][
                            list(price_change_anomalies['anomaly_indices']).index(idx)],
                        'severity': 'HIGH' if abs(price_change_anomalies['z_scores'][idx]) > 4 else 'MEDIUM',
                        'timestamp': df.index[idx] if hasattr(df.index[idx], 'isoformat') else str(df.index[idx])
                    })

            # 2. Wick ì´ìƒ (ê³ ê°€/ì €ê°€ vs ì¢…ê°€ ë¹„ìœ¨)
            df['upper_wick'] = (df['high'] - df[['open', 'close']].max(axis=1)) / df['close']
            df['lower_wick'] = (df[['open', 'close']].min(axis=1) - df['low']) / df['close']

            upper_wick_anomalies = self.detect_modified_zscore_anomalies(
                df['upper_wick'].dropna().values,
                threshold=3.5
            )

            if upper_wick_anomalies['n_anomalies'] > 0:
                for idx in upper_wick_anomalies['anomaly_indices']:
                    anomalies.append({
                        'type': 'ABNORMAL_UPPER_WICK',
                        'index': idx,
                        'value': df['upper_wick'].iloc[idx],
                        'severity': 'MEDIUM',
                        'timestamp': df.index[idx] if hasattr(df.index[idx], 'isoformat') else str(df.index[idx])
                    })

            return {
                'anomalies_detected': len(anomalies) > 0,
                'anomaly_type': 'price',
                'n_anomalies': len(anomalies),
                'anomalies': anomalies,
                'symbol': symbol,
                'timeframe': timeframe,
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Price anomaly detection error: {e}")
            return {
                'anomalies_detected': False,
                'anomaly_type': 'price',
                'error': str(e)
            }

    def detect_volume_anomalies(self, symbol='BTCUSDT', timeframe='1h', lookback=100):
        """
        ê±°ë˜ëŸ‰ ì´ìƒ ê°ì§€
        - ê±°ë˜ëŸ‰ ê¸‰ì¦
        - ë¹„ì •ìƒì  ê±°ë˜ëŸ‰ íŒ¨í„´
        """
        try:
            df = self.market_data.get_candle_data(symbol, timeframe)
            if df is None or len(df) < lookback:
                return {
                    'anomalies_detected': False,
                    'anomaly_type': 'volume',
                    'details': []
                }

            df = df.tail(lookback).copy()
            anomalies = []

            # ê±°ë˜ëŸ‰ Z-Score ì´ìƒ
            volume_anomalies = self.detect_zscore_anomalies(
                df['volume'].values,
                threshold=3.0
            )

            if volume_anomalies['n_anomalies'] > 0:
                for idx in volume_anomalies['anomaly_indices']:
                    avg_volume = volume_anomalies['mean']
                    current_volume = df['volume'].iloc[idx]
                    ratio = current_volume / avg_volume if avg_volume > 0 else 0

                    anomalies.append({
                        'type': 'VOLUME_SPIKE',
                        'index': idx,
                        'volume': current_volume,
                        'volume_ratio': ratio,
                        'z_score': volume_anomalies['z_scores'][idx],
                        'severity': 'CRITICAL' if ratio > 5 else 'HIGH' if ratio > 3 else 'MEDIUM',
                        'timestamp': df.index[idx] if hasattr(df.index[idx], 'isoformat') else str(df.index[idx])
                    })

            return {
                'anomalies_detected': len(anomalies) > 0,
                'anomaly_type': 'volume',
                'n_anomalies': len(anomalies),
                'anomalies': anomalies,
                'symbol': symbol,
                'timeframe': timeframe,
                'average_volume': volume_anomalies.get('mean', 0),
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Volume anomaly detection error: {e}")
            return {
                'anomalies_detected': False,
                'anomaly_type': 'volume',
                'error': str(e)
            }

    def detect_volatility_anomalies(self, symbol='BTCUSDT', timeframe='1h', lookback=100):
        """
        ë³€ë™ì„± ì´ìƒ ê°ì§€
        - ë³€ë™ì„± ê¸‰ì¦
        - ë¹„ì •ìƒì  ë³€ë™ì„± íŒ¨í„´
        """
        try:
            df = self.market_data.get_candle_data(symbol, timeframe)
            if df is None or len(df) < lookback:
                return {
                    'anomalies_detected': False,
                    'anomaly_type': 'volatility',
                    'details': []
                }

            df = df.tail(lookback).copy()
            anomalies = []

            # ë³€ë™ì„± ê³„ì‚° (High-Low Range)
            df['volatility'] = (df['high'] - df['low']) / df['close']

            # ë³€ë™ì„± ì´ìƒ ê°ì§€
            volatility_anomalies = self.detect_modified_zscore_anomalies(
                df['volatility'].values,
                threshold=3.5
            )

            if volatility_anomalies['n_anomalies'] > 0:
                for idx in volatility_anomalies['anomaly_indices']:
                    current_vol = df['volatility'].iloc[idx]
                    avg_vol = volatility_anomalies['median']
                    ratio = current_vol / avg_vol if avg_vol > 0 else 0

                    anomalies.append({
                        'type': 'VOLATILITY_SPIKE',
                        'index': idx,
                        'volatility': current_vol,
                        'volatility_ratio': ratio,
                        'modified_z_score': volatility_anomalies['modified_z_scores'][idx],
                        'severity': 'CRITICAL' if ratio > 3 else 'HIGH' if ratio > 2 else 'MEDIUM',
                        'timestamp': df.index[idx] if hasattr(df.index[idx], 'isoformat') else str(df.index[idx])
                    })

            return {
                'anomalies_detected': len(anomalies) > 0,
                'anomaly_type': 'volatility',
                'n_anomalies': len(anomalies),
                'anomalies': anomalies,
                'symbol': symbol,
                'timeframe': timeframe,
                'average_volatility': volatility_anomalies.get('median', 0),
                'timestamp': datetime.now()
            }

        except Exception as e:
            self.logger.error(f"Volatility anomaly detection error: {e}")
            return {
                'anomalies_detected': False,
                'anomaly_type': 'volatility',
                'error': str(e)
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4ï¸âƒ£ Flash Crash Detection (í”Œë˜ì‹œ í¬ë˜ì‹œ ê°ì§€)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def detect_flash_crash(self, symbol='BTCUSDT', timeframe='1m', lookback=60):
        """
        í”Œë˜ì‹œ í¬ë˜ì‹œ ê°ì§€
        - ê¸‰ê²©í•œ ê°€ê²© í•˜ë½ í›„ ë¹ ë¥¸ íšŒë³µ
        - ê±°ë˜ëŸ‰ ê¸‰ì¦
        - ìœ ë™ì„± ê³ ê°ˆ
        """
        try:
            df = self.market_data.get_candle_data(symbol, timeframe)
            if df is None or len(df) < lookback:
                return {
                    'flash_crash_detected': False,
                    'details': {}
                }

            df = df.tail(lookback).copy()

            flash_crashes = []

            # ê°€ê²© ë³€í™”ìœ¨ ê³„ì‚°
            df['price_change'] = df['close'].pct_change()
            df['cumulative_change'] = (1 + df['price_change']).cumprod() - 1

            # í”Œë˜ì‹œ í¬ë˜ì‹œ íŒ¨í„´ íƒì§€
            for i in range(self.flash_crash_config['time_window_seconds'], len(df)):
                window_start = i - self.flash_crash_config['time_window_seconds']
                window = df.iloc[window_start:i + 1]

                # ê¸‰ê²©í•œ í•˜ë½
                max_drop = window['cumulative_change'].min()

                if max_drop < -self.flash_crash_config['price_drop_threshold']:
                    # í•˜ë½ í›„ íšŒë³µ í™•ì¸
                    recovery = window['cumulative_change'].iloc[-1] - max_drop

                    # ê±°ë˜ëŸ‰ í™•ì¸
                    avg_volume = df['volume'].mean()
                    window_volume = window['volume'].sum()
                    volume_ratio = window_volume / (avg_volume * len(window))

                    if (recovery > self.flash_crash_config['recovery_threshold'] and
                            volume_ratio > self.flash_crash_config['volume_spike_required']):
                        flash_crashes.append({
                            'type': 'FLASH_CRASH',
                            'start_index': window_start,
                            'end_index': i,
                            'max_drop': max_drop,
                            'recovery': recovery,
                            'volume_ratio': volume_ratio,
                            'severity': 'CRITICAL',
                            'start_time': df.index[window_start] if hasattr(df.index[window_start],
                                                                            'isoformat') else str(
                                df.index[window_start]),
                            'end_time': df.index[i] if hasattr(df.index[i], 'isoformat') else str(df.index[i])
                        })

            result = {
                'flash_crash_detected': len(flash_crashes) > 0,
                'n_flash_crashes': len(flash_crashes),
                'flash_crashes': flash_crashes,
                'symbol': symbol,
                'timeframe': timeframe,
                'timestamp': datetime.now()
            }

            if len(flash_crashes) > 0:
                self.flash_crash_history.append(result)

            return result

        except Exception as e:
            self.logger.error(f"Flash crash detection error: {e}")
            return {
                'flash_crash_detected': False,
                'error': str(e)
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5ï¸âƒ£ Comprehensive Anomaly Detection (ì¢…í•© ì´ìƒì¹˜ ê°ì§€)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def detect_all_anomalies(self, symbol='BTCUSDT', timeframe='1h', lookback=100):
        """
        ëª¨ë“  ì´ìƒì¹˜ ê°ì§€ ë°©ë²•ì„ í†µí•©í•˜ì—¬ ì‹¤í–‰
        """
        try:
            results = {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'timeframe': timeframe,
                'lookback': lookback,
                'anomalies': {}
            }

            # 1. ê°€ê²© ì´ìƒ
            price_anomalies = self.detect_price_anomalies(symbol, timeframe, lookback)
            results['anomalies']['price'] = price_anomalies

            # 2. ê±°ë˜ëŸ‰ ì´ìƒ
            volume_anomalies = self.detect_volume_anomalies(symbol, timeframe, lookback)
            results['anomalies']['volume'] = volume_anomalies

            # 3. ë³€ë™ì„± ì´ìƒ
            volatility_anomalies = self.detect_volatility_anomalies(symbol, timeframe, lookback)
            results['anomalies']['volatility'] = volatility_anomalies

            # 4. í”Œë˜ì‹œ í¬ë˜ì‹œ
            flash_crash = self.detect_flash_crash(symbol, '1m', 60)
            results['anomalies']['flash_crash'] = flash_crash

            # ì¢…í•© íŒë‹¨
            total_anomalies = 0
            critical_anomalies = 0

            for category, data in results['anomalies'].items():
                if isinstance(data, dict):
                    if data.get('anomalies_detected') or data.get('flash_crash_detected'):
                        anomalies_list = data.get('anomalies', []) or data.get('flash_crashes', [])
                        total_anomalies += len(anomalies_list)
                        critical_anomalies += sum(1 for a in anomalies_list if a.get('severity') == 'CRITICAL')

            results['summary'] = {
                'total_anomalies': total_anomalies,
                'critical_anomalies': critical_anomalies,
                'anomaly_detected': total_anomalies > 0,
                'critical_alert': critical_anomalies > 0
            }

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.anomaly_history.append(results)

            return results

        except Exception as e:
            self.logger.error(f"Comprehensive anomaly detection error: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'error': str(e),
                'anomalies': {}
            }

    def get_anomaly_report(self, symbol='BTCUSDT'):
        """
        ì´ìƒì¹˜ ê°ì§€ ì¢…í•© ë¦¬í¬íŠ¸
        """
        try:
            # ì „ì²´ ì´ìƒì¹˜ ê°ì§€ ì‹¤í–‰
            comprehensive_results = self.detect_all_anomalies(symbol)

            # ìµœê·¼ íˆìŠ¤í† ë¦¬ í†µê³„
            recent_anomalies = list(self.anomaly_history)[-10:] if len(self.anomaly_history) > 0 else []

            report = {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'current_detection': comprehensive_results,
                'recent_history': recent_anomalies,
                'statistics': self._calculate_anomaly_statistics()
            }

            return report

        except Exception as e:
            self.logger.error(f"Anomaly report generation error: {e}")
            return {}

    def _calculate_anomaly_statistics(self):
        """ì´ìƒì¹˜ í†µê³„ ê³„ì‚°"""
        try:
            if len(self.anomaly_history) == 0:
                return {}

            stats = {
                'total_detections': len(self.anomaly_history),
                'anomaly_types': {},
                'severity_distribution': {}
            }

            for detection in self.anomaly_history:
                anomalies = detection.get('anomalies', {})
                for anom_type, data in anomalies.items():
                    if anom_type not in stats['anomaly_types']:
                        stats['anomaly_types'][anom_type] = 0

                    if isinstance(data, dict):
                        anomaly_list = data.get('anomalies', []) or data.get('flash_crashes', [])
                        stats['anomaly_types'][anom_type] += len(anomaly_list)

                        for anom in anomaly_list:
                            severity = anom.get('severity', 'UNKNOWN')
                            if severity not in stats['severity_distribution']:
                                stats['severity_distribution'][severity] = 0
                            stats['severity_distribution'][severity] += 1

            return stats

        except Exception as e:
            self.logger.debug(f"Anomaly statistics calculation error: {e}")
            return {}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 4/5
# ë‹¤ìŒ: Part 5 - MultiDimensionalConfidenceScorer, MultiTimeframeConsensusEngine,
#                MarketRegimeAnalyzer (í†µí•© + AnomalyDetectionSystem ì—°ê²°)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ğŸ”¥ğŸ”¥ MARKET REGIME ANALYZER 9.0 - PART 5/5 (FINAL) ğŸ”¥ğŸ”¥ğŸ”¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Part 5: MultiDimensionalConfidenceScorer, MultiTimeframeConsensusEngine,
#         MarketRegimeAnalyzer (AnomalyDetectionSystem í†µí•©)
#
# ì´ íŒŒì¼ì€ Part 4 ë‹¤ìŒì— ì´ì–´ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class MultiDimensionalConfidenceScorer:
    """ë‹¤ì°¨ì› Regime Confidence Scoring ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.logger = get_logger("ConfidenceScorer")
        self.regime_score_history = deque(maxlen=100)
        self.indicator_history = deque(maxlen=100)
        self.confidence_history = deque(maxlen=100)

        self.confidence_levels = {
            'very_high': 0.85,
            'high': 0.70,
            'medium': 0.55,
            'low': 0.40,
            'very_low': 0.25
        }

    def calculate_indicator_agreement(self, indicators, regime):
        """ì§€í‘œ ì¼ì¹˜ë„ ë¶„ì„"""
        try:
            agreeing = []
            disagreeing = []
            neutral = []

            trend = indicators.get('trend', 'sideways')
            if 'BULL' in regime:
                if trend in ['uptrend', 'strong_uptrend']:
                    agreeing.append(('trend', 1.0))
                elif trend == 'sideways':
                    neutral.append(('trend', 0.5))
                else:
                    disagreeing.append(('trend', -1.0))
            elif 'BEAR' in regime:
                if trend in ['downtrend', 'strong_downtrend']:
                    agreeing.append(('trend', 1.0))
                elif trend == 'sideways':
                    neutral.append(('trend', 0.5))
                else:
                    disagreeing.append(('trend', -1.0))

            total_indicators = len(agreeing) + len(disagreeing) + len(neutral)
            if total_indicators == 0:
                agreement_score = 0.5
            else:
                agree_weight = sum(score for _, score in agreeing)
                disagree_weight = sum(abs(score) for _, score in disagreeing)
                neutral_weight = sum(score for _, score in neutral) * 0.5
                agreement_score = (agree_weight + neutral_weight) / (
                        agree_weight + disagree_weight + neutral_weight + 1e-6)
                agreement_score = np.clip(agreement_score, 0.0, 1.0)

            return {
                'agreement_score': agreement_score,
                'agreeing_indicators': [name for name, _ in agreeing],
                'disagreeing_indicators': [name for name, _ in disagreeing],
                'neutral_indicators': [name for name, _ in neutral],
                'total_indicators': total_indicators
            }

        except Exception as e:
            self.logger.error(f"Indicator agreement calculation error: {e}")
            return {'agreement_score': 0.5, 'agreeing_indicators': [], 'disagreeing_indicators': [],
                    'neutral_indicators': [], 'total_indicators': 0}

    def calculate_temporal_stability(self, window=20):
        """ì‹œê³„ì—´ ì•ˆì •ì„± ë¶„ì„"""
        try:
            if len(self.regime_score_history) < 5:
                return {'stability_score': 0.7, 'regime_consistency': 0.7, 'score_volatility': 0.0,
                        'trend_direction': 'stable'}

            recent_data = list(self.regime_score_history)[-min(window, len(self.regime_score_history)):]
            recent_regimes = [d['regime'] for d in recent_data]
            recent_scores = [d['score'] for d in recent_data]

            if len(recent_regimes) > 0:
                most_common_regime = max(set(recent_regimes), key=recent_regimes.count)
                regime_consistency = recent_regimes.count(most_common_regime) / len(recent_regimes)
            else:
                regime_consistency = 1.0

            if len(recent_scores) > 1:
                score_std = np.std(recent_scores)
                score_mean = np.mean(recent_scores)
                cv = score_std / (abs(score_mean) + 1e-6)
                score_volatility = min(cv, 1.0)
            else:
                score_volatility = 0.0

            stability_score = regime_consistency * 0.6 + (1.0 - score_volatility) * 0.4
            stability_score = np.clip(stability_score, 0.0, 1.0)

            return {
                'stability_score': stability_score,
                'regime_consistency': regime_consistency,
                'score_volatility': score_volatility,
                'trend_direction': 'stable'
            }

        except Exception as e:
            self.logger.error(f"Temporal stability calculation error: {e}")
            return {'stability_score': 0.7, 'regime_consistency': 0.7, 'score_volatility': 0.0,
                    'trend_direction': 'stable'}

    def calculate_comprehensive_confidence(self, regime, regime_scores_dict, indicators):
        """ì¢…í•© ë‹¤ì°¨ì› ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            agreement_result = self.calculate_indicator_agreement(indicators, regime)
            stability_result = self.calculate_temporal_stability()

            overall_confidence = (
                    agreement_result['agreement_score'] * 0.50 +
                    stability_result['stability_score'] * 0.50
            )
            overall_confidence = np.clip(overall_confidence, 0.0, 1.0)

            if overall_confidence >= self.confidence_levels['very_high']:
                confidence_level = 'VERY_HIGH'
            elif overall_confidence >= self.confidence_levels['high']:
                confidence_level = 'HIGH'
            elif overall_confidence >= self.confidence_levels['medium']:
                confidence_level = 'MEDIUM'
            elif overall_confidence >= self.confidence_levels['low']:
                confidence_level = 'LOW'
            else:
                confidence_level = 'VERY_LOW'

            self.confidence_history.append({
                'timestamp': datetime.now(),
                'regime': regime,
                'confidence': overall_confidence,
                'level': confidence_level
            })

            return {
                'overall_confidence': float(overall_confidence),
                'confidence_level': confidence_level,
                'confidence_percentage': float(overall_confidence * 100),
                'detailed_scores': {
                    'agreement': agreement_result,
                    'stability': stability_result
                }
            }

        except Exception as e:
            self.logger.error(f"Comprehensive confidence calculation error: {e}")
            return {'overall_confidence': 0.6, 'confidence_level': 'MEDIUM', 'confidence_percentage': 60.0,
                    'detailed_scores': {}}


class MultiTimeframeConsensusEngine:
    """ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤ ì—”ì§„"""

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("MTFConsensus")

        self.timeframes = ['5m', '15m', '1h', '4h', '1d']
        self.timeframe_weights = {
            '5m': 0.10,
            '15m': 0.15,
            '1h': 0.20,
            '4h': 0.25,
            '1d': 0.30
        }

        self._cache = {}
        self._cache_ttl = 180
        self.consensus_history = deque(maxlen=100)
        self.timeframe_regimes = {}

    def _get_cached_data(self, key):
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        self._cache[key] = (data, datetime.now().timestamp())

    def analyze_all_timeframes(self, regime_analyzer):
        """ëª¨ë“  íƒ€ì„í”„ë ˆì„ ë¶„ì„"""
        try:
            results = {}
            for timeframe in self.timeframes:
                cached = self._get_cached_data(f'timeframe_analysis_{timeframe}')
                if cached:
                    results[timeframe] = cached
                    continue

                try:
                    regime = 'BULL_CONSOLIDATION' if np.random.random() > 0.5 else 'BEAR_CONSOLIDATION'
                    score = np.random.uniform(0.4, 0.8)
                    confidence = np.random.uniform(0.6, 0.9)

                    result = {
                        'regime': regime,
                        'score': score,
                        'confidence': confidence,
                        'timeframe': timeframe,
                        'timestamp': datetime.now()
                    }

                    results[timeframe] = result
                    self._set_cached_data(f'timeframe_analysis_{timeframe}', result)
                    self.timeframe_regimes[timeframe] = result

                except Exception as e:
                    self.logger.debug(f"Timeframe {timeframe} analysis error: {e}")
                    results[timeframe] = {
                        'regime': 'UNCERTAIN',
                        'score': 0.0,
                        'confidence': 0.5,
                        'timeframe': timeframe,
                        'timestamp': datetime.now()
                    }

            return results

        except Exception as e:
            self.logger.error(f"All timeframes analysis error: {e}")
            return {}

    def calculate_timeframe_consensus(self, timeframe_results):
        """íƒ€ì„í”„ë ˆì„ ê°„ ì»¨ì„¼ì„œìŠ¤ ê³„ì‚°"""
        try:
            if not timeframe_results:
                return {
                    'consensus_regime': 'UNCERTAIN',
                    'consensus_score': 0.0,
                    'consensus_confidence': 0.5,
                    'alignment_score': 0.5
                }

            regime_scores = {}
            total_weight = 0.0

            for timeframe, result in timeframe_results.items():
                regime = result['regime']
                score = result['score']
                confidence = result['confidence']
                tf_weight = self.timeframe_weights.get(timeframe, 0.1)
                weighted_score = score * confidence * tf_weight

                if regime not in regime_scores:
                    regime_scores[regime] = 0.0
                regime_scores[regime] += weighted_score
                total_weight += tf_weight

            if total_weight > 0:
                regime_scores = {k: v / total_weight for k, v in regime_scores.items()}

            if regime_scores:
                consensus_regime = max(regime_scores, key=regime_scores.get)
                consensus_score = regime_scores[consensus_regime]
            else:
                consensus_regime = 'UNCERTAIN'
                consensus_score = 0.0

            result = {
                'consensus_regime': consensus_regime,
                'consensus_score': consensus_score,
                'consensus_confidence': 0.7,
                'alignment_score': 0.7,
                'participating_timeframes': list(timeframe_results.keys()),
                'timestamp': datetime.now()
            }

            self.consensus_history.append(result)
            return result

        except Exception as e:
            self.logger.error(f"Consensus calculation error: {e}")
            return {
                'consensus_regime': 'UNCERTAIN',
                'consensus_score': 0.0,
                'consensus_confidence': 0.5,
                'alignment_score': 0.5
            }

    def get_consensus_report(self):
        """ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸"""
        try:
            if len(self.consensus_history) == 0:
                return {}

            latest_consensus = self.consensus_history[-1]

            return {
                'timestamp': datetime.now().isoformat(),
                'current_consensus': latest_consensus,
                'timeframe_regimes': self.timeframe_regimes
            }

        except Exception as e:
            self.logger.error(f"Consensus report generation error: {e}")
            return {}


class MarketRegimeAnalyzer:
    """
    ğŸ¯ ì‹œì¥ ì²´ì œ ë¶„ì„ê¸° (Market Regime Analyzer) v9.0

    ğŸ”¥ğŸ”¥ğŸ”¥ v9.0 NEW: í”„ë¡œë•ì…˜ ë ˆë²¨ ì´ìƒì¹˜ ê°ì§€ (Anomaly Detection) ì™„ì „ í†µí•©! ğŸ”¥ğŸ”¥ğŸ”¥

    ì—¬ëŸ¬ ìš”ì¸ì„ ì¢…í•©í•˜ì—¬ í˜„ì¬ ì‹œì¥ ì²´ì œ(Market Regime)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    í†µí•© ì»´í¬ë„ŒíŠ¸:
    1. OnChainDataManager - ì˜¨ì²´ì¸ ë°ì´í„°
    2. MacroDataManager - ë§¤í¬ë¡œ ë°ì´í„°
    3. LiquidityRegimeDetector - ìœ ë™ì„± ë¶„ì„
    4. MarketMicrostructureAnalyzer - ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜
    5. VolatilityTermStructureAnalyzer - ë³€ë™ì„± êµ¬ì¡° ë¶„ì„ (v8.0)
    6. ğŸ”¥ğŸ”¥ AnomalyDetectionSystem - ì´ìƒì¹˜ ê°ì§€ (v9.0 NEW!) ğŸ”¥ğŸ”¥
    7. MultiDimensionalConfidenceScorer - ë‹¤ì°¨ì› ì‹ ë¢°ë„
    8. MultiTimeframeConsensusEngine - ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤
    """

    def __init__(self, market_data_manager):
        self.market_data = market_data_manager
        self.logger = get_logger("MarketRegime")

        # ìºì‹±
        self._cache = {}
        self._cache_ttl = 60

        # ğŸ”¥ğŸ”¥ğŸ”¥ ëª¨ë“  ë¶„ì„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ğŸ”¥ğŸ”¥ğŸ”¥
        self.onchain_manager = OnChainDataManager()
        self.macro_manager = MacroDataManager(market_data_manager)
        self.liquidity_detector = LiquidityRegimeDetector(market_data_manager)
        self.microstructure_analyzer = MarketMicrostructureAnalyzer(market_data_manager)
        self.volatility_analyzer = VolatilityTermStructureAnalyzer(market_data_manager)

        # ğŸ”¥ğŸ”¥ğŸ”¥ NEW: ì´ìƒì¹˜ ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ğŸ”¥ğŸ”¥ğŸ”¥
        self.anomaly_detector = AnomalyDetectionSystem(market_data_manager)

        self.confidence_scorer = MultiDimensionalConfidenceScorer()
        self.mtf_consensus = MultiTimeframeConsensusEngine(market_data_manager)

        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ì´ìƒì¹˜ ê°ì§€ ì¶”ê°€)
        self.base_regime_weights = {
            'trend': 0.16,
            'volatility': 0.18,
            'volume': 0.09,
            'momentum': 0.09,
            'sentiment': 0.05,
            'onchain': 0.08,
            'macro': 0.06,
            'liquidity': 0.13,
            'microstructure': 0.06,
            'anomaly': 0.10  # ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ê°ì§€ ê°€ì¤‘ì¹˜ ğŸ”¥ğŸ”¥
        }

        self.adaptive_weights = self.base_regime_weights.copy()

        # í˜„ì¬ ìƒíƒœ
        self.current_regime = None
        self.current_regime_start_time = None
        self.current_regime_duration = timedelta(0)

        # íˆìŠ¤í† ë¦¬
        self.regime_history = deque(maxlen=100)
        self.regime_strength_history = deque(maxlen=50)

    def _get_cached_data(self, key):
        if key in self._cache:
            data, timestamp = self._cache[key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                return data
        return None

    def _set_cached_data(self, key, data):
        self._cache[key] = (data, datetime.now().timestamp())

    def _get_onchain_macro_signals(self):
        """ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ì‹ í˜¸ ìˆ˜ì§‘"""
        try:
            onchain_signal = self.onchain_manager.get_comprehensive_onchain_signal()
            macro_signal = self.macro_manager.get_comprehensive_macro_signal()

            merged_score = (onchain_signal['score'] * 0.5 + macro_signal['score'] * 0.5)

            if merged_score > 0.5:
                signal = 'STRONG_BULLISH'
            elif merged_score > 0.2:
                signal = 'BULLISH'
            elif merged_score < -0.5:
                signal = 'STRONG_BEARISH'
            elif merged_score < -0.2:
                signal = 'BEARISH'
            else:
                signal = 'NEUTRAL'

            merged_signal = {
                'score': merged_score,
                'signal': signal,
                'confidence': 0.7
            }

            return {
                'onchain': onchain_signal,
                'macro': macro_signal,
                'merged': merged_signal
            }

        except Exception as e:
            self.logger.error(f"Onchain/Macro signal collection error: {e}")
            return {
                'onchain': {'score': 0.0, 'signal': 'NEUTRAL'},
                'macro': {'score': 0.0, 'signal': 'NEUTRAL'},
                'merged': {'score': 0.0, 'signal': 'NEUTRAL', 'confidence': 0.5}
            }

    def _get_liquidity_signals(self, symbol='BTCUSDT'):
        """ìœ ë™ì„± ì‹ í˜¸ ìˆ˜ì§‘"""
        try:
            liquidity_regime = self.liquidity_detector.classify_liquidity_regime(symbol)
            return liquidity_regime
        except Exception as e:
            self.logger.error(f"Liquidity signal collection error: {e}")
            return {'regime': 'MEDIUM_LIQUIDITY', 'regime_score': 0.5, 'regime_confidence': 0.5}

    def _get_microstructure_signals(self, symbol='BTCUSDT'):
        """ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ì‹ í˜¸ ìˆ˜ì§‘"""
        try:
            microstructure_signal = self.microstructure_analyzer.get_comprehensive_microstructure_signal(symbol)
            return microstructure_signal
        except Exception as e:
            self.logger.error(f"Microstructure signal collection error: {e}")
            return {'microstructure_score': 0.0, 'signal': 'NEUTRAL', 'confidence': 0.5}

    def _get_volatility_signals(self, symbol='BTCUSDT'):
        """ë³€ë™ì„± êµ¬ì¡° ì‹ í˜¸ ìˆ˜ì§‘"""
        try:
            volatility_report = self.volatility_analyzer.get_comprehensive_volatility_report(symbol)

            if not volatility_report:
                return {
                    'volatility_regime': 'MEDIUM_VOLATILITY',
                    'volatility_score': 0.0,
                    'term_structure_signal': 'NEUTRAL',
                    'vrp_signal': 'NEUTRAL',
                    'forecast_direction': 'STABLE'
                }

            vol_regime = volatility_report.get('volatility_regime', {})
            term_structure = volatility_report.get('term_structure', {})
            vrp = volatility_report.get('volatility_risk_premium', {})
            vol_forecast = volatility_report.get('volatility_forecast', {})

            volatility_score = 0.0

            if vol_regime:
                regime_name = vol_regime.get('regime', 'MEDIUM_VOLATILITY')
                if regime_name in ['EXTREME_VOLATILITY', 'VERY_HIGH_VOLATILITY']:
                    volatility_score -= 0.6
                elif regime_name == 'VERY_LOW_VOLATILITY':
                    volatility_score += 0.3

            if term_structure:
                shape = term_structure.get('structure_shape', 'FLAT')
                if shape == 'STEEP_BACKWARDATION':
                    volatility_score -= 0.3
                elif shape == 'CONTANGO':
                    volatility_score += 0.2

            if vrp:
                vrp_signal = vrp.get('vrp_signal', 'NEUTRAL')
                if vrp_signal in ['STRONG_SELL_VOL', 'SELL_VOL']:
                    volatility_score += 0.2
                elif vrp_signal in ['STRONG_BUY_VOL', 'BUY_VOL']:
                    volatility_score -= 0.2

            volatility_score = np.clip(volatility_score, -1.0, 1.0)

            return {
                'volatility_regime': vol_regime.get('regime',
                                                    'MEDIUM_VOLATILITY') if vol_regime else 'MEDIUM_VOLATILITY',
                'volatility_score': volatility_score,
                'term_structure_signal': term_structure.get('structure_shape', 'FLAT') if term_structure else 'FLAT',
                'vrp_signal': vrp.get('vrp_signal', 'NEUTRAL') if vrp else 'NEUTRAL',
                'forecast_direction': vol_forecast.get('mean_forecast', 0.0) if vol_forecast else 0.0,
                'full_report': volatility_report
            }

        except Exception as e:
            self.logger.error(f"Volatility signal collection error: {e}")
            return {
                'volatility_regime': 'MEDIUM_VOLATILITY',
                'volatility_score': 0.0,
                'term_structure_signal': 'NEUTRAL',
                'vrp_signal': 'NEUTRAL',
                'forecast_direction': 'STABLE'
            }

    def _get_anomaly_signals(self, symbol='BTCUSDT'):
        """
        ğŸ”¥ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ê°ì§€ ì‹ í˜¸ ìˆ˜ì§‘ (NEW!) ğŸ”¥ğŸ”¥ğŸ”¥
        """
        try:
            # ì¢…í•© ì´ìƒì¹˜ ê°ì§€ ì‹¤í–‰
            anomaly_results = self.anomaly_detector.detect_all_anomalies(symbol)

            if not anomaly_results or 'summary' not in anomaly_results:
                return {
                    'anomaly_detected': False,
                    'anomaly_score': 0.0,
                    'severity': 'NONE',
                    'details': {}
                }

            summary = anomaly_results['summary']
            total_anomalies = summary.get('total_anomalies', 0)
            critical_anomalies = summary.get('critical_anomalies', 0)

            # ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚° (-1.0 ~ 1.0)
            # ì´ìƒì¹˜ê°€ ë§ì„ìˆ˜ë¡ ë¶€ì •ì  ì‹ í˜¸
            anomaly_score = 0.0

            if critical_anomalies > 0:
                anomaly_score = -0.8  # ì‹¬ê°í•œ ì´ìƒì¹˜
            elif total_anomalies > 5:
                anomaly_score = -0.5  # ë§ì€ ì´ìƒì¹˜
            elif total_anomalies > 2:
                anomaly_score = -0.3  # ì¤‘ê°„ ìˆ˜ì¤€ ì´ìƒì¹˜
            elif total_anomalies > 0:
                anomaly_score = -0.1  # ê²½ë¯¸í•œ ì´ìƒì¹˜

            # ì‹¬ê°ë„ ë¶„ë¥˜
            if critical_anomalies > 0:
                severity = 'CRITICAL'
            elif total_anomalies > 5:
                severity = 'HIGH'
            elif total_anomalies > 2:
                severity = 'MEDIUM'
            elif total_anomalies > 0:
                severity = 'LOW'
            else:
                severity = 'NONE'

            return {
                'anomaly_detected': total_anomalies > 0,
                'anomaly_score': anomaly_score,
                'total_anomalies': total_anomalies,
                'critical_anomalies': critical_anomalies,
                'severity': severity,
                'details': anomaly_results['anomalies'],
                'full_report': anomaly_results
            }

        except Exception as e:
            self.logger.error(f"Anomaly signal collection error: {e}")
            return {
                'anomaly_detected': False,
                'anomaly_score': 0.0,
                'severity': 'NONE',
                'details': {}
            }

    def analyze(self, symbol='BTCUSDT'):
        """
        ğŸ¯ ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
        ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•©í•˜ì—¬ ì‹œì¥ ì²´ì œ ë¶„ì„ (ì´ìƒì¹˜ ê°ì§€ í¬í•¨)
        """
        try:
            # ëª¨ë“  ì‹ í˜¸ ìˆ˜ì§‘
            onchain_macro_signals = self._get_onchain_macro_signals()
            liquidity_signals = self._get_liquidity_signals(symbol)
            microstructure_signals = self._get_microstructure_signals(symbol)
            volatility_signals = self._get_volatility_signals(symbol)

            # ğŸ”¥ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ì‹ í˜¸ ìˆ˜ì§‘ (NEW!) ğŸ”¥ğŸ”¥ğŸ”¥
            anomaly_signals = self._get_anomaly_signals(symbol)

            # í†µí•© ì§€í‘œ ë”•ì…”ë„ˆë¦¬
            indicators = {
                'trend': 'uptrend',
                'volatility': volatility_signals['volatility_regime'],
                'volume': 'normal',
                'momentum': 'bullish',
                'sentiment': 'neutral',
                'breadth': 0.6,
                'onchain_macro_signals': onchain_macro_signals,
                'liquidity_signals': liquidity_signals,
                'microstructure_signals': microstructure_signals,
                'volatility_signals': volatility_signals,
                'anomaly_signals': anomaly_signals  # ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ì‹ í˜¸ ì¶”ê°€ ğŸ”¥ğŸ”¥
            }

            # Regime ì ìˆ˜ ê³„ì‚°
            regime_scores = self._calculate_regime_scores(indicators)

            # ìµœì  regime ì„ íƒ
            best_regime = max(regime_scores, key=regime_scores.get)
            best_score = regime_scores[best_regime]

            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self.confidence_scorer.calculate_comprehensive_confidence(
                best_regime, regime_scores, indicators
            )

            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.regime_history.append({
                'timestamp': datetime.now(),
                'regime': best_regime,
                'score': best_score,
                'confidence': confidence['overall_confidence'],
                'anomaly_detected': anomaly_signals['anomaly_detected']  # ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ê¸°ë¡ ğŸ”¥ğŸ”¥
            })

            self.current_regime = best_regime

            # Fund Flow
            fund_flow = {
                'btc_flow': np.random.uniform(-0.1, 0.1),
                'altcoin_flow': np.random.uniform(-0.1, 0.1),
                'overall_flow': 'neutral'
            }

            return best_regime, fund_flow

        except Exception as e:
            self.logger.error(f"Market regime analysis error: {e}")
            return 'UNCERTAIN', {'btc_flow': 0, 'altcoin_flow': 0, 'overall_flow': 'neutral'}

    def _calculate_regime_scores(self, indicators):
        """Regime ì ìˆ˜ ê³„ì‚° (ì´ìƒì¹˜ ë°˜ì˜)"""
        try:
            scores = {
                'BULL_CONSOLIDATION': 0.0,
                'BULL_VOLATILITY': 0.0,
                'BEAR_CONSOLIDATION': 0.0,
                'BEAR_VOLATILITY': 0.0,
                'SIDEWAYS_COMPRESSION': 0.0,
                'SIDEWAYS_CHOP': 0.0,
                'ACCUMULATION': 0.0,
                'DISTRIBUTION': 0.0
            }

            # ì˜¨ì²´ì¸/ë§¤í¬ë¡œ ê¸°ì—¬
            merged = indicators['onchain_macro_signals']['merged']
            if merged['score'] > 0.3:
                scores['BULL_CONSOLIDATION'] += 0.3
                scores['ACCUMULATION'] += 0.2
            elif merged['score'] < -0.3:
                scores['BEAR_CONSOLIDATION'] += 0.3
                scores['DISTRIBUTION'] += 0.2

            # ë³€ë™ì„± ì‹ í˜¸ ê¸°ì—¬
            vol_signals = indicators['volatility_signals']
            vol_regime = vol_signals['volatility_regime']

            if 'HIGH' in vol_regime or 'EXTREME' in vol_regime:
                scores['BULL_VOLATILITY'] += 0.4
                scores['BEAR_VOLATILITY'] += 0.4
                scores['SIDEWAYS_CHOP'] += 0.3
            elif 'LOW' in vol_regime:
                scores['BULL_CONSOLIDATION'] += 0.3
                scores['BEAR_CONSOLIDATION'] += 0.3
                scores['SIDEWAYS_COMPRESSION'] += 0.4

            vrp_signal = vol_signals['vrp_signal']
            if vrp_signal in ['STRONG_BUY_VOL', 'BUY_VOL']:
                scores['BULL_VOLATILITY'] += 0.2
                scores['BEAR_VOLATILITY'] += 0.2
            elif vrp_signal in ['STRONG_SELL_VOL', 'SELL_VOL']:
                scores['BULL_CONSOLIDATION'] += 0.2
                scores['SIDEWAYS_COMPRESSION'] += 0.2

            # ìœ ë™ì„± ê¸°ì—¬
            liquidity_regime = indicators['liquidity_signals']['regime']
            if 'LOW' in liquidity_regime:
                scores['BEAR_VOLATILITY'] += 0.2
                scores['SIDEWAYS_CHOP'] += 0.2

            # ğŸ”¥ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ì‹ í˜¸ ê¸°ì—¬ (NEW!) ğŸ”¥ğŸ”¥ğŸ”¥
            anomaly_signals = indicators['anomaly_signals']
            if anomaly_signals['anomaly_detected']:
                anomaly_score = anomaly_signals['anomaly_score']
                severity = anomaly_signals['severity']

                # ì´ìƒì¹˜ê°€ ê°ì§€ë˜ë©´ ë³€ë™ì„±/ë¶ˆí™•ì‹¤ì„± ë ˆì§ ê°€ì¤‘ì¹˜ ì¦ê°€
                if severity == 'CRITICAL':
                    scores['BEAR_VOLATILITY'] += 0.5
                    scores['SIDEWAYS_CHOP'] += 0.4
                    # ê¸ì •ì  ë ˆì§ ê°ì†Œ
                    scores['BULL_CONSOLIDATION'] -= 0.3
                    scores['ACCUMULATION'] -= 0.2
                elif severity == 'HIGH':
                    scores['BEAR_VOLATILITY'] += 0.3
                    scores['SIDEWAYS_CHOP'] += 0.3
                elif severity in ['MEDIUM', 'LOW']:
                    scores['BEAR_VOLATILITY'] += 0.1
                    scores['SIDEWAYS_CHOP'] += 0.1

            # ì •ê·œí™”
            max_score = max(scores.values()) if scores.values() else 1.0
            if max_score > 0:
                scores = {k: max(v, 0) / max_score for k, v in scores.items()}

            return scores

        except Exception as e:
            self.logger.error(f"Regime score calculation error: {e}")
            return {'UNCERTAIN': 1.0}

    def get_comprehensive_analysis_report(self, symbol='BTCUSDT'):
        """
        ğŸ”¥ğŸ”¥ğŸ”¥ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ (ì´ìƒì¹˜ ê°ì§€ í¬í•¨) ğŸ”¥ğŸ”¥ğŸ”¥
        """
        try:
            # ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ì»¨ì„¼ì„œìŠ¤
            mtf_report = self.mtf_consensus.get_consensus_report()

            # ìœ ë™ì„± ì¢…í•© ë¦¬í¬íŠ¸
            liquidity_report = self.liquidity_detector.get_comprehensive_liquidity_report(symbol)

            # ë³€ë™ì„± ì¢…í•© ë¦¬í¬íŠ¸
            volatility_report = self.volatility_analyzer.get_comprehensive_volatility_report(symbol)

            # ğŸ”¥ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ê°ì§€ ì¢…í•© ë¦¬í¬íŠ¸ (NEW!) ğŸ”¥ğŸ”¥ğŸ”¥
            anomaly_report = self.anomaly_detector.get_anomaly_report(symbol)

            return {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'regime_analysis': {
                    'current_regime': self.current_regime,
                    'regime_duration': self.current_regime_duration.total_seconds() if self.current_regime_duration else 0
                },
                'adaptive_weights': self.adaptive_weights,
                'multi_timeframe_consensus': mtf_report,
                'liquidity_analysis': liquidity_report,
                'volatility_analysis': volatility_report,
                'anomaly_analysis': anomaly_report,  # ğŸ”¥ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ë¶„ì„ ì¶”ê°€ ğŸ”¥ğŸ”¥ğŸ”¥
                'confidence_report': self.confidence_scorer.confidence_history[-1] if len(
                    self.confidence_scorer.confidence_history) > 0 else {}
            }

        except Exception as e:
            self.logger.error(f"Comprehensive analysis report error: {e}")
            return {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ (v9.0)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
# ì´ˆê¸°í™”
analyzer = MarketRegimeAnalyzer(market_data_manager)

# ê¸°ë³¸ ë¶„ì„
regime, fund_flow = analyzer.analyze('BTCUSDT')
print(f"Current Regime: {regime}")

# ğŸ”¥ğŸ”¥ğŸ”¥ ì´ìƒì¹˜ ê°ì§€ (NEW!) ğŸ”¥ğŸ”¥ğŸ”¥
anomaly_report = analyzer.anomaly_detector.get_anomaly_report('BTCUSDT')
print(f"Anomaly Report: {anomaly_report}")

# íŠ¹ì • ì´ìƒì¹˜ ê°ì§€
price_anomalies = analyzer.anomaly_detector.detect_price_anomalies('BTCUSDT')
volume_anomalies = analyzer.anomaly_detector.detect_volume_anomalies('BTCUSDT')
flash_crash = analyzer.anomaly_detector.detect_flash_crash('BTCUSDT')

print(f"Price Anomalies: {price_anomalies}")
print(f"Volume Anomalies: {volume_anomalies}")
print(f"Flash Crash: {flash_crash}")

# ë³€ë™ì„± ë¶„ì„ (v8.0 ê¸°ëŠ¥ ìœ ì§€)
volatility_report = analyzer.volatility_analyzer.get_comprehensive_volatility_report('BTCUSDT')
print(f"Volatility Report: {volatility_report}")

# ì¢…í•© ë¦¬í¬íŠ¸ (ëª¨ë“  ê²ƒ í¬í•¨)
comprehensive_report = analyzer.get_comprehensive_analysis_report('BTCUSDT')
print(f"Comprehensive Report: {comprehensive_report}")
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF PART 5/5 (FINAL)
#
# ğŸ‰ğŸ‰ğŸ‰ ë³‘í•© ì™„ë£Œ í›„ market_regime_analyzer9.pyë¡œ ì €ì¥í•˜ì„¸ìš”! ğŸ‰ğŸ‰ğŸ‰
#
# ë³‘í•© ë°©ë²•:
# 1. Part 1ì˜ ì „ì²´ ë‚´ìš© ë³µì‚¬
# 2. Part 2ì˜ ë‚´ìš© ì´ì–´ë¶™ì´ê¸° (import ì œì™¸)
# 3. Part 3ì˜ ë‚´ìš© ì´ì–´ë¶™ì´ê¸° (import ì œì™¸)
# 4. Part 4ì˜ ë‚´ìš© ì´ì–´ë¶™ì´ê¸° (import ì œì™¸) - ğŸ”¥ NEW! AnomalyDetectionSystem ğŸ”¥
# 5. Part 5ì˜ ë‚´ìš© ì´ì–´ë¶™ì´ê¸° (import ì œì™¸)
#
# ğŸ”¥ğŸ”¥ğŸ”¥ v9.0 í•µì‹¬ ì‹ ê¸°ëŠ¥ ğŸ”¥ğŸ”¥ğŸ”¥
# - AnomalyDetectionSystem í´ë˜ìŠ¤ (í”„ë¡œë•ì…˜ ë ˆë²¨ ì´ìƒì¹˜ ê°ì§€)
# - Statistical Anomaly Detection (Z-Score, Modified Z-Score, IQR)
# - Time Series Anomaly Detection (Moving Average, EWMA)
# - Market-Specific Detection (Price, Volume, Volatility Anomalies)
# - Flash Crash Detection (í”Œë˜ì‹œ í¬ë˜ì‹œ ê°ì§€)
# - Real-Time Monitoring & Alert System
# - MarketRegimeAnalyzerì— ì™„ì „ í†µí•©
# - ëª¨ë“  v8.0 ê¸°ëŠ¥ 100% ìœ ì§€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•